{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AUDIT library","text":"<p> AUDIT: An open-source Python library for comprehensive evaluation of medical image segmentation models and MRI datasets analysis </p> <p>Documentation: https://caumente.github.io/AUDIT/</p> <p>Source Code: https://github.com/caumente/AUDIT/</p> <p>Welcome to the official documentation for AUDIT (Analysis &amp; evalUation Dashboard of artIficial inTelligence), a a tool designed to provide researchers and developers an interactive way to better analyze and explore MRI datasets and  segmentation models. Given its functionalities to extract the most relevant features and metrics from your several data  sources, it allows for uncovering biases both intra and inter-dataset as well as within the model predictions.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Robust evaluation: Extracts region-specific features and calculates a wide range of performance metrics.</li> <li>Interactive visualizations: Includes a dynamic Streamlit-based web app for intuitive data exploration.</li> <li>Highly customizable: Easily extendable for additional features and metrics tailored to your needs.</li> <li>Friendly integration: Supports plugins and external libraries for advanced analysis (e.g., ITK-SNAP, pymia).</li> <li>Open Source: Fully available on GitHub with comprehensive tutorials and examples.</li> </ul>"},{"location":"#what-youll-find-here","title":"\ud83d\udcda What You'll Find Here","text":"<p>This documentation is structured to help you get the most out of AUDIT:</p> <ul> <li>Getting Started: Learn how to install AUDIT and set up your first project.</li> <li>API Reference: Detailed reference for all library classes and methods.</li> <li>Analysis modes: Explore the dashboard included in the web app.</li> <li>Tutorials: Hands-on examples demonstrating common use cases.</li> <li>About: Check latest AUDIT release and license terms.</li> </ul>"},{"location":"#quick-start","title":"\ud83c\udf1f Quick Start","text":"<p>The best way to get familiar with AUDIT and explore all its capabilities is through our interactive DEMO. You can find it at: https://auditapp.streamlit.app/. </p> <p>Users will find an online wep app with pre-configured data to explore features and compare the accuracy of a set of  medical image segmentation models. However, you could use AUDIT easily in your own computer by following a few little steps. </p> <p>Install AUDIT </p> <p>Directly from PyPI:</p> <pre><code>pip install auditapp\n</code></pre> <p>Alternatively, clone the repository and install it locally:</p> <pre><code>git clone https://github.com/caumente/AUDIT.git\ncd AUDIT\npip install -r requirements.txt\n</code></pre> <p>Launch AUDIT App </p> <p>Start the interactive web app using our default configuration for data visualization and exploration</p> <p>Run the following command if you installed AUDIT directly from PyPI: <pre><code>auditapp run-app\n</code></pre></p> <p>Or alternatively, if you cloned the repository and install it locally, run:</p> <pre><code>python src/audit/app/launcher.py\n</code></pre> <p>That's it! You're ready to explore our default data and evaluate AI segmentations models with AUDIT. Go to  Getting Started to learn more about how to use AUDIT with your own datasets and  configurations.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Please check the contributing guide for details on how to report issues, suggest new features,  or contribute code.</p> <p>Feel free to reach out at Contact Us.</p>"},{"location":"API_reference/features/","title":"Feature extraction","text":"<p>This <code>feature extraction</code> pipeline is designed to process medical imaging datasets, specifically MRI scans, to extract a  wide range of features including spatial, tumor-related, statistical, and texture-based characteristics. The pipeline  is composed of two core components: the feature_extraction.py script, which orchestrates the entire process, and the  underlying feature extraction logic contained in src.features.main.py.</p> <p>The pipeline operates as follows:</p> <ul> <li> <p>Configuration and Logging: The process begins by loading configuration settings from a YAML file, specifying data  paths, features to extract, and output directories. A logging system is set up to monitor and record the progress of  the feature extraction.</p> </li> <li> <p>Dataset Processing: For each dataset defined in the configuration, the pipeline iterates over all subject data,  extracting features from MRI sequences and associated segmentations. This is handled by the extract_features function, which computes various features such as spatial dimensions, tumor characteristics, statistical metrics, and texture  properties of the images.</p> </li> <li> <p>Feature Extraction: The pipeline uses specialized classes for different types of features:</p> <ul> <li>Spatial Features: Related to image dimensions and brain structure.</li> <li>Tumor Features: Derived from segmentations to describe the tumor\u2019s shape, volume, and position.</li> <li>Statistical Features: First-order statistics like mean, variance, etc., extracted from image sequences.</li> <li>Texture Features: Second-order metrics describing the texture patterns in the images.</li> </ul> </li> </ul> <p>If longitudinal data is present, the pipeline also extracts and includes time-point and longitudinal  identifiers for further analysis.</p> <ul> <li>Data Output: Once features are extracted for each subject, they are compiled into a DataFrame, which is saved as  a CSV file. </li> </ul> <p>This pipeline provides an automated and extensible framework for processing large-scale MRI datasets, ensuring that  all relevant features are extracted and saved for downstream analysis, such as predictive modeling or visualization.</p> <p>In the following sections, the users can explore in detail the different methods provided by AUDIT to extract relevant  information from MRIs.</p>"},{"location":"API_reference/features/spatial/","title":"Spatial features","text":"<p>The <code>SpatialFeatures</code> class is designed to compute spatial properties related to 3D medical imaging sequences, such as  brain MRI scans. This class focuses on calculating basic spatial features like the brain's center of mass and the  dimensionality of the scan in various planes.</p>"},{"location":"API_reference/features/spatial/#overview","title":"Overview","text":"<p>This class is intended to provide spatial insights from a 3D sequence of medical images. It helps to extract two key  metrics: The center of mass for the brain, which is calculated based on the sequence. The dimensions of the sequence in the axial, coronal, and sagittal planes. These spatial features are essential in understanding the brain's structure  and alignment in a 3D scan, aiding in medical analysis and further processing of brain images.</p> <p>The following spatial features are available:</p> <ul> <li>Brain Center of Mass: The 3D coordinates of the brain's center, adjusted by voxel spacing.</li> <li>Sequence Dimensions: The dimensions of the sequence in the axial, coronal, and sagittal planes.</li> </ul>"},{"location":"API_reference/features/spatial/#methods","title":"Methods","text":""},{"location":"API_reference/features/spatial/#__init__","title":"<code>__init__()</code>","text":"<p>Description: The constructor method initializes a <code>SpatialFeatures</code> object with a given 3D image sequence and voxel spacing. The  voxel spacing is used for any adjustments to the spatial features (e.g., center of mass). If no spacing is provided,  it defaults to (1, 1, 1).</p> <p>Parameters:</p> <ul> <li><code>sequence</code> (<code>np.ndarray</code>): A 3D MRI in form of NumPy array from which spatial features will be calculated.</li> <li><code>spacing</code> (<code>np.ndarray, optional</code>): A tuple representing the voxel spacing of the medical image. Defaults to  (1, 1, 1) if not provided.</li> </ul>"},{"location":"API_reference/features/spatial/#calculate_brain_center_mass","title":"<code>calculate_brain_center_mass()</code>","text":"<p>Returns (<code>dict</code>): A dictionary containing the 3D coordinates of the brain's center of mass for each plane (axial,  coronal, sagittal), adjusted by the voxel spacing. If the sequence is not found, it returns NaN for each plane.</p>"},{"location":"API_reference/features/spatial/#get_dimensions","title":"<code>get_dimensions()</code>","text":"<p>Returns (<code>dict</code>): A dictionary containing the dimensions of the sequence for each plane (axial_dim, coronal_dim, and  sagittal_dim). If the sequence is not found, the dimensions are returned as NaN.</p>"},{"location":"API_reference/features/spatial/#extract_features","title":"<code>extract_features()</code>","text":"<p>Returns (<code>dict</code>): All spatial features. </p> <ul> <li><code>axial_dim</code>: Number of slices in the axial plane.</li> <li><code>coronal_dim</code>: Number of slices in the coronal plane.</li> <li><code>sagittal_dim</code>: Number of slices in the sagittal plane.</li> <li><code>axial_brain_centre_mass</code>: The center of mass in the axial plane.</li> <li><code>coronal_brain_centre_mass</code>: The center of mass in the coronal plane.</li> <li><code>sagittal_brain_centre_mass</code>: The center of mass in the sagittal plane.</li> </ul>"},{"location":"API_reference/features/statistical/","title":"Statistical features","text":"<p>The <code>StatisticalFeatures</code> class provides a convenient way to compute several common statistical metrics from a given array of data.</p>"},{"location":"API_reference/features/statistical/#overview","title":"Overview","text":"<p>The class utilizes NumPy for efficient numerical operations and SciPy for computing first-order statistical features. By encapsulating these features in a class, users can easily compute various statistical properties of a dataset with minimal boilerplate code.</p> <p>The following statistical features are available:</p> <ul> <li>Maximum intensity: The highest value in the MRI.</li> <li>Minimum intensity: The lowest value in the MRI.</li> <li>Mean intensity: The average value of the MRI.</li> <li>Median intensity: The middle value of the MRI when sorted.</li> <li>Standard deviation intensity: A measure of the amount of variation or dispersion of the values.</li> <li>Range intensity: The difference between the maximum and minimum values.</li> <li>Skewness: A measure of the asymmetry of the distribution of pixel values.</li> </ul>"},{"location":"API_reference/features/statistical/#methods","title":"Methods","text":""},{"location":"API_reference/features/statistical/#__init__","title":"<code>__init__()</code>","text":"<p>Description: The constructor method initializes the <code>StatisticalFeatures</code> object by accepting a 3D magnetic resonance image in the form of a NumPy array. Once initialized, the class provides several methods to compute various statistical features from the MRI. Notice that, given the background of MRI images usually is equal to 0, those values should be removed to not interfere with the computations.</p> <p>Parameters:</p> <ul> <li><code>sequence</code> (<code>np.ndarray</code>): A 3D MRI in form of NumPy array from which statistical features will be calculated.</li> </ul>"},{"location":"API_reference/features/statistical/#get_max_intensity","title":"<code>get_max_intensity()</code>","text":"<p>Returns (<code>float</code>): The maximum intensity value in the MRI.</p>"},{"location":"API_reference/features/statistical/#get_min_intensity","title":"<code>get_min_intensity()</code>","text":"<p>Returns (<code>float</code>): The minimum intensity value in the MRI.</p>"},{"location":"API_reference/features/statistical/#get_mean_intensity","title":"<code>get_mean_intensity()</code>","text":"<p>Returns (<code>float</code>): The mean intensity value in the MRI.  </p>"},{"location":"API_reference/features/statistical/#get_median_intensity","title":"<code>get_median_intensity()</code>","text":"<p>Returns (<code>float</code>): The median intensity value in the MRI.</p>"},{"location":"API_reference/features/statistical/#get_std_intensity","title":"<code>get_std_intensity()</code>","text":"<p>Returns (<code>float</code>): The standard deviation of the of intensity values in the MRI.</p>"},{"location":"API_reference/features/statistical/#get_range_intensity","title":"<code>get_range_intensity()</code>","text":"<p>Returns (<code>float</code>): the range of intensity values in the MRI (i.e., the difference between the maximum and minimum values).</p>"},{"location":"API_reference/features/statistical/#get_skewness","title":"<code>get_skewness()</code>","text":"<p>Returns (<code>float</code>): The skewness of the intensity values in the MRI, a measure of asymmetry in the distribution.</p>"},{"location":"API_reference/features/statistical/#extract_features","title":"<code>extract_features()</code>","text":"<p>Returns (<code>dict</code>): All statistical features. </p> <ul> <li><code>max_intensity</code>: Maximum intensity value in the MRI.</li> <li><code>min_intensity</code>: Minimum intensity value in the MRI.</li> <li><code>mean_intensity</code>: Mean intensity value in the MRI.</li> <li><code>median_intensity</code>: Median intensity value in the MRI.</li> <li><code>std_intensity</code>: Standard deviation of intensity values in the MRI.</li> <li><code>range_intensity</code>: Range of intensity values in the MRI.</li> <li><code>skewness</code>: Skewness of the intensity values in the MRI.</li> </ul>"},{"location":"API_reference/features/texture/","title":"Texture features","text":"<p>The <code>TextureFeatures</code> class provides an efficient mechanism for calculating second-order texture features from a given 3D magnetic resonance image (MRI).</p>"},{"location":"API_reference/features/texture/#overview","title":"Overview","text":"<p>This class utilizes skimage for calculating the gray level co-occurrence matrix (GLCM) and its corresponding texture  features such as contrast, homogeneity, and energy. The texture features extracted from each 2D plane of a 3D MRI  sequence give insights into the structural patterns within the image.</p> <p>By encapsulating these operations in a class, the user can easily compute several texture features with minimal effort. It also supports an option to remove empty planes, improving accuracy when working with brain MRI scans.</p> <p>The following texture features are available:</p> <ul> <li>Contrast: A measure of the intensity contrast between a pixel and its neighbor over the whole image.</li> <li>Dissimilarity: Measures the local intensity variations.</li> <li>Homogeneity: Measures the closeness of the distribution of elements in the GLCM to the GLCM diagonal.</li> <li>ASM (Angular Second Moment): A measure of the texture uniformity.</li> <li>Energy: The square root of ASM, indicating the texture\u2019s level of orderliness.</li> <li>Correlation: A measure of how correlated a pixel is to its neighbor across the whole image.</li> </ul>"},{"location":"API_reference/features/texture/#methods","title":"Methods","text":""},{"location":"API_reference/features/texture/#__init__","title":"<code>__init__()</code>","text":"<p>Description:</p> <p>The constructor initializes a <code>TextureFeatures</code> object by accepting a 3D MRI sequence and an optional parameter to  remove empty planes. The class will compute texture features across 2D planes, making use of GLCM-based calculations.</p> <p>Parameters:</p> <ul> <li>sequence (np.ndarray): A 3D MRI image in the form of a NumPy array from which texture features will be calculated.</li> <li>remove_empty_planes (bool): A flag to indicate whether empty planes (e.g., non-brain areas) should be removed from the MRI sequence. Defaults to False. </li> </ul>"},{"location":"API_reference/features/texture/#compute_texture_values","title":"<code>compute_texture_values()</code>","text":"<p>Description: Computes the specified texture feature for each 2D plane in the 3D image. The GLCM is calculated for each 2D slice, and the texture feature is extracted using graycoprops from the skimage.feature module.</p> <p>Parameters:</p> <ul> <li><code>texture</code> (<code>str</code>): The texture feature to compute (e.g., \"contrast\", \"homogeneity\"). Defaults to \"contrast\".</li> </ul> <p>Returns (<code>np.ndarray</code>): An array of texture values for each 2D plane in the 3D MRI sequence.</p>"},{"location":"API_reference/features/texture/#extract_features","title":"<code>extract_features()</code>","text":"<p>Description:</p> <p>Extracts all specified texture features from the MRI image. This method iterates through the given list of texture features, computing the mean and standard deviation for each one across all 2D planes in the MRI sequence.</p> <p>Parameters:</p> <ul> <li><code>textures</code> (`list[str]): A list of texture features to compute (e.g., 'contrast', 'energy'). If not provided, the      default set includes: 'contrast', 'dissimilarity', 'homogeneity', 'ASM', 'energy', 'correlation'</li> </ul> <p>Returns (<code>dict</code>): A dictionary where the keys represent the texture feature names, and the values represent the mean  and standard deviation for each feature.</p>"},{"location":"API_reference/features/tumor/","title":"Tumor features","text":"<p>The <code>TumorFeatures</code> class computes various tumor-related metrics based on a segmented 3D medical image, such as lesion  size, tumor center of mass, and tumor location relative to the brain's center of mass.</p>"},{"location":"API_reference/features/tumor/#overview","title":"Overview","text":"<p>This class provides methods to compute first-order tumor features, focusing on spatial and volumetric characteristics  derived from medical image segmentation. It is designed to handle common use cases in medical imaging, such as  identifying tumor regions, calculating the tumor's size, and determining its relative position.</p> <p>The class allows customization through optional parameters such as voxel spacing and segmentation label mappings. This  makes it highly adaptable to different medical imaging contexts, including various scan types and segmentation  algorithms.</p> <p>The following tumor features are available:</p> <ul> <li>Tumor Pixel Count: The number of pixels associated with each tumor label in the segmentation.</li> <li>Lesion Size: The volume of the lesion(s) computed based on pixel count and voxel spacing.</li> <li>Tumor Center of Mass: The geometric center of a tumor or lesion in 3D space.</li> <li>Tumor Slices: The image slices in the axial, coronal, and sagittal planes that contain tumor regions.</li> <li>Tumor Position: The location of tumor slices in each plane (e.g., lower and upper bounds).</li> </ul>"},{"location":"API_reference/features/tumor/#methods","title":"Methods","text":""},{"location":"API_reference/features/tumor/#__init__","title":"<code>__init__()</code>","text":"<p>Description: The constructor initializes a <code>TumorFeatures</code> object using a 3D segmented MRI and various optional parameters such as  voxel spacing and label mappings. These attributes are used to calculate tumor features like lesion size and tumor  location across different planes.</p> <p>Parameters:</p> <ul> <li><code>segmentation</code> (<code>np.ndarray</code>): A 3D NumPy array representing the segmentation of the medical image.</li> <li><code>spacing</code> (<code>tuple, optional</code>): The voxel spacing of the medical image (default is (1, 1, 1)).</li> <li><code>mapping_names</code> (<code>dict, optional</code>): A dictionary mapping segmentation values (labels) to human-readable names.</li> <li><code>planes</code> (<code>list[str]</code>, optional`): The planes (axial, coronal, sagittal) for tumor slice analysis. Defaults to  [\"axial\", \"coronal\", \"sagittal\"].</li> </ul>"},{"location":"API_reference/features/tumor/#count_tumor_pixels","title":"<code>count_tumor_pixels()</code>","text":"<p>Returns (<code>dict</code>):  A dictionary where keys represent segmentation labels (or names) and values represent the pixel  count for each label.</p>"},{"location":"API_reference/features/tumor/#count_tumor_pixels_1","title":"<code>count_tumor_pixels()</code>","text":"<p>Returns (<code>dict</code>): A dictionary containing the total lesion size, keyed by \"lesion_size\".</p>"},{"location":"API_reference/features/tumor/#get_tumor_center_masslabelnone","title":"<code>get_tumor_center_mass(label=None)</code>","text":"<p>Parameters:</p> <ul> <li><code>label</code> (<code>int, optional</code>): Specifies the label of the tumor for which the center of mass should be calculated. If not  provided, the calculation is done for all tumor regions.</li> </ul> <p>Return (<code>np.ndarray</code>): The 3D coordinates of the tumor\u2019s center of mass.</p>"},{"location":"API_reference/features/tumor/#get_tumor_slices","title":"<code>get_tumor_slices()</code>","text":"<p>Return (<code>tuple</code>): A tuple containing three lists, each representing the indices of slices with tumors in the axial,  coronal, and sagittal planes.</p>"},{"location":"API_reference/features/tumor/#calculate_tumor_slices","title":"<code>calculate_tumor_slices()</code>","text":"<p>Return (<code>dict</code>): A dictionary where keys represent the plane names (e.g., \"axial_tumor_slice\") and values represent the  number of tumor-containing slices.</p>"},{"location":"API_reference/features/tumor/#calculate_position_tumor_slices","title":"<code>calculate_position_tumor_slices()</code>","text":"<p>Return (<code>dict</code>): A dictionary containing the minimum and maximum slice indices for each plane  (e.g., \"lower_axial_tumor_slice\", \"upper_axial_tumor_slice\").</p>"},{"location":"API_reference/features/tumor/#calculate_tumor_pixel","title":"<code>calculate_tumor_pixel()</code>","text":"<p>Return (<code>dict</code>): A dictionary where keys represent each tumor label (e.g., \"lesion_size_label1\") and values represent  the lesion size in voxels.</p>"},{"location":"API_reference/features/tumor/#calculate_tumor_distance","title":"<code>calculate_tumor_distance()</code>","text":"<p>Parameters:</p> <ul> <li><code>brain_centre_mass</code> (<code>array-like</code>): The center of mass of the brain used as a reference point.</li> </ul> <p>Return (<code>dict</code>): A dictionary where each key represents the tumor label and the value represents the distance between  the tumor and the brain's center of mass.</p>"},{"location":"API_reference/features/tumor/#calculate_tumor_center_mass","title":"<code>calculate_tumor_center_mass()</code>","text":"<p>Parameters:</p> <p>Return (<code>dict</code>):  A dictionary where keys represent each plane and label (e.g., \"axial_tumor_center_mass\") and values  represent the coordinates of the tumor center of mass.</p>"},{"location":"API_reference/features/tumor/#extract_features","title":"<code>extract_features()</code>","text":"<p>Returns (<code>dict</code>): All tumor features. </p> <ul> <li>Center of mass per label and plane (e.g., \"axial_tumor_center_mass\").</li> <li>Tumor location relative to brain center of mass.</li> <li>Lesion size per label.</li> <li>Total lesion size.</li> <li>Number of tumor-containing slices per plane.</li> <li>Lower and upper bounds of tumor slices.</li> </ul>"},{"location":"API_reference/metrics/audit_metrics/","title":"Metrics Computed","text":"<p>The provided code computes a variety of metrics to evaluate the performance of a segmentation model in relation to the  ground truth. These metrics provide insights into the model's accuracy, overlap, and shape conformity with the actual  segmented regions. Below is an overview of each metric computed:</p>"},{"location":"API_reference/metrics/audit_metrics/#1-dice-score-dice","title":"1. Dice Score (DICE)","text":"<p>The Dice score (or Dice coefficient) is a measure of overlap between the ground truth and the predicted segmentation. It ranges from 0 to 1, with 1 indicating perfect overlap.</p> \\[ \\text{Dice} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN} \\] <p>Where <code>TP</code> is true positives, <code>FP</code> is false positives, and <code>FN</code> is false negatives.</p> <p>Interpretation: A higher Dice score indicates better agreement between prediction and ground truth.</p>"},{"location":"API_reference/metrics/audit_metrics/#2-jaccard-index-jacc","title":"2. Jaccard Index (JACC)","text":"<p>Also known as the Intersection over Union (IoU), the Jaccard index is another overlap-based metric. It measures the  size of the intersection divided by the size of the union of the predicted and ground truth regions.</p> \\[ \\text{Jaccard} = \\frac{TP}{TP + FP + FN} \\] <p>Interpretation: A higher Jaccard index indicates a more accurate segmentation. It is always lower than the Dice score  for the same segmentation.</p>"},{"location":"API_reference/metrics/audit_metrics/#3-sensitivity-sens","title":"3. Sensitivity (SENS)","text":"<p>Sensitivity, also known as recall or true positive rate, measures the ability of the model to correctly identify all  the positive regions (i.e., tumor voxels).</p> \\[ \\text{Sensitivity} = \\frac{TP}{TP + FN} \\] <p>Interpretation: A higher sensitivity value indicates the model is good at detecting positive regions (e.g., tumor  regions), but it doesn\u2019t account for false positives.</p>"},{"location":"API_reference/metrics/audit_metrics/#4-specificity-spec","title":"4. Specificity (SPEC)","text":"<p>Specificity measures the model's ability to correctly identify negative regions (i.e., non-tumor voxels).</p> \\[ \\text{Specificity} = \\frac{TN}{TN + FP} \\] <p>Interpretation: A higher specificity value indicates the model is good at ignoring false positives, but doesn\u2019t account for missing true positives.</p>"},{"location":"API_reference/metrics/audit_metrics/#5-precision-prec","title":"5. Precision (PREC)","text":"<p>Precision, or positive predictive value, measures the proportion of predicted positive cases that are actually positive.</p> \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\] <p>Interpretation: A high precision value means that when the model predicts a positive case (e.g., tumor), it is likely  correct.</p>"},{"location":"API_reference/metrics/audit_metrics/#6-accuracy-accu","title":"6. Accuracy (ACCU)","text":"<p>Accuracy provides an overall measure of how often the model makes correct predictions (both true positives and true  negatives) across all regions.</p> \\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\] <p>Interpretation: A high accuracy score reflects the model\u2019s general performance in predicting both positive and negative cases.</p>"},{"location":"API_reference/metrics/audit_metrics/#7-hausdorff-distance-haus","title":"7. Hausdorff Distance (HAUS)","text":"<p>The Hausdorff distance is a shape-based metric that measures the maximum distance between points on the predicted  segmentation and the corresponding points on the ground truth.</p> \\[ \\text{Hausdorff Distance} = \\max_{x \\in A} \\min_{y \\in B} d(x, y) \\] <p>Where <code>A</code> is the set of points on the predicted segmentation, <code>B</code> is the set of points on the ground truth, and  <code>d(x, y)</code> is the Euclidean distance between points.</p> <p>Interpretation: Lower Hausdorff distances indicate that the boundary of the predicted segmentation is closer to the ground truth boundary, implying better shape similarity.</p>"},{"location":"API_reference/metrics/audit_metrics/#8-segmentation-size-size","title":"8. Segmentation Size (SIZE)","text":"<p>This metric calculates the physical size of the predicted segmentation in terms of voxel count, adjusted by the voxel spacing to provide a volume measurement.</p> \\[ \\text{Size} = \\text{Voxel count of predicted region} \\times \\text{Spacing} \\] <p>Interpretation: This helps to quantify the total volume of the segmented region, which can be compared to the expected  size from the ground truth.</p>"},{"location":"API_reference/metrics/metric_extraction/","title":"Metric extraction","text":"<p>This <code>metric extraction</code> pipeline processes MRI segmentation data by comparing ground truth segmentations with model  predictions to compute a variety of metrics. The pipeline supports both custom metrics and Pymia-based metrics, with  the ability to handle multiple models and datasets. The two main components of the pipeline are metric_extraction.py,  which serves as the entry point, and main.py, which contains the core logic for metric computation.</p> <p>The pipeline operates as follows:</p> <ul> <li> <p>Configuration and Logging: The pipeline begins by loading a configuration file, which defines the dataset paths,  models, metrics to be extracted, and output settings. Logging is set up to track the progress and output detailed logs.</p> </li> <li> <p>Metric Extraction: Depending on the configuration, the pipeline can compute either custom metrics (using methods  defined in the project) or Pymia metrics (using the Pymia library for medical image analysis). </p> </li> <li> <p>Custom Metrics: This approach calculates specific metrics like Dice coefficient, sensitivity, or others based on  custom implementations. It involves one-hot encoding the ground truth and predicted segmentations and then computing  the defined metrics for each subject.</p> </li> <li> <p>Pymia Metrics: The pipeline can leverage Pymia's built-in metrics (e.g., Hausdorff distance, Dice coefficient,  Jaccard index) for segmentation evaluation. Pymia's evaluator processes the segmentation files and accumulates the  results across different models and regions.</p> </li> <li> <p>Data Processing: For each dataset and model, metrics are computed for all subjects. The results are collected  into a DataFrame, and if longitudinal data is involved, it can further organize the results by time points.</p> </li> <li> <p>Output and Statistics: The extracted metrics are stored as CSV files, and additional statistical analyses (e.g.,  mean, median, standard deviation) can be computed and saved if required. The output is structured and ready for  further analysis or reporting.</p> </li> </ul> <p>This pipeline provides a flexible and scalable solution for evaluating segmentation models, making it suitable for  multi-model comparisons and performance tracking across different datasets.</p>"},{"location":"API_reference/utils/file_manager/","title":"File manager","text":"<p>The <code>file_operations</code> module provides utilities for managing directories, reading and writing data files, and handling CSV and JSON file formats. These utilities are designed to streamline common file I/O operations.</p>"},{"location":"API_reference/utils/file_manager/#overview","title":"Overview","text":"<p>The <code>file_operations</code> module includes functions for:</p> <ul> <li>Creating directories if they do not exist.</li> <li>Reading and writing JSON and CSV files.</li> <li>Managing files and directories.</li> </ul> <p>These utilities are aimed at simplifying file management tasks using AUDIT library.</p>"},{"location":"API_reference/utils/file_manager/#methods","title":"Methods","text":""},{"location":"API_reference/utils/file_manager/#ensure_dir","title":"<code>ensure_dir()</code>","text":"<p>Description: Ensures that a directory exists, creating it if necessary.</p> <p>Parameters: - <code>path</code> (<code>str</code>): The path to the directory.</p> <p>Returns: None.</p> <p>Notes: Logs the creation of the directory.</p>"},{"location":"API_reference/utils/file_manager/#read_json","title":"<code>read_json()</code>","text":"<p>Description: Reads a JSON file and returns its content.</p> <p>Parameters: - <code>path</code> (<code>str</code>): The path to the JSON file.</p> <p>Returns: - <code>dict</code>: The content of the JSON file.</p> <p>Exceptions: - Raises <code>FileNotFoundError</code> if the file does not exist. - Raises <code>JSONDecodeError</code> for invalid JSON.</p>"},{"location":"API_reference/utils/file_manager/#write_json","title":"<code>write_json()</code>","text":"<p>Description: Writes a dictionary to a JSON file.</p> <p>Parameters: - <code>data</code> (<code>dict</code>): The data to write to the file. - <code>path</code> (<code>str</code>): The path to the output JSON file.</p> <p>Returns: None.</p> <p>Exceptions: - Raises <code>TypeError</code> if the data is not serializable.</p>"},{"location":"API_reference/utils/file_manager/#read_csv","title":"<code>read_csv()</code>","text":"<p>Description: Reads a CSV file and returns its content as a pandas DataFrame.</p> <p>Parameters: - <code>path</code> (<code>str</code>): The path to the CSV file.</p> <p>Returns: - <code>pd.DataFrame</code>: The content of the CSV file.</p> <p>Exceptions: - Raises <code>FileNotFoundError</code> if the file does not exist. - Raises <code>pd.errors.ParserError</code> for malformed CSV.</p>"},{"location":"API_reference/utils/file_manager/#write_csv","title":"<code>write_csv()</code>","text":"<p>Description: Writes a pandas DataFrame to a CSV file.</p> <p>Parameters: - <code>data</code> (<code>pd.DataFrame</code>): The DataFrame to write to the file. - <code>path</code> (<code>str</code>): The path to the output CSV file.</p> <p>Returns: None.</p> <p>Exceptions: - Raises <code>ValueError</code> if the data is not a valid DataFrame.</p>"},{"location":"API_reference/utils/file_manager/#log_operation","title":"<code>log_operation()</code>","text":"<p>Description: Logs details of file operations, such as reads, writes, and errors.</p> <p>Parameters: - <code>message</code> (<code>str</code>): The log message. - <code>log_file</code> (<code>str</code>, optional): Path to the log file. Defaults to <code>\"operations.log\"</code>.</p> <p>Returns: None.</p> <p>Notes: Appends log messages to the specified file.</p>"},{"location":"API_reference/utils/sequences/","title":"Sequences","text":"<p>The <code>utils.sequences</code> module provides a set of utilities for handling 3D medical imaging data in the NIfTI format. This module includes functions for loading NIfTI files, processing segmentation data, replacing labels, and extracting features from medical image sequences.</p>"},{"location":"API_reference/utils/sequences/#overview","title":"Overview","text":"<p>The <code>sequences</code> module focuses on simplifying the handling and processing of 3D medical images. It includes functions for tasks such as:</p> <ul> <li>Loading NIfTI files either as <code>SimpleITK.Image</code> objects or as NumPy arrays.</li> <li>Replacing labels in segmentation data with desired mappings.</li> <li>Iteratively processing segmentation files within a directory.</li> <li>Extracting image dimensions, voxel spacing, and label counts.</li> <li>Fitting boundaries around non-zero regions of medical images.</li> </ul> <p>These utilities are designed for streamlined medical image processing workflows.</p>"},{"location":"API_reference/utils/sequences/#methods","title":"Methods","text":""},{"location":"API_reference/utils/sequences/#load_nii","title":"<code>load_nii()</code>","text":"<p>Description: Loads a NIfTI file from the specified path and returns it as a <code>SimpleITK.Image</code> object or a NumPy array.</p> <p>Parameters:</p> <ul> <li><code>path_folder</code> (str): The file path to the NIfTI file.</li> <li><code>as_array</code> (bool, optional): If <code>True</code>, returns the NIfTI file as a NumPy array. Defaults to <code>False</code>.</li> </ul> <p>Returns: - <code>SimpleITK.Image</code> or <code>np.ndarray</code>: The loaded NIfTI file.</p> <p>Exceptions: - Raises <code>ValueError</code> if the file path is invalid. - Logs warnings for errors during file loading.</p>"},{"location":"API_reference/utils/sequences/#load_nii_by_subject_id","title":"<code>load_nii_by_subject_id()</code>","text":"<p>Description: Loads a specific sequence from a NIfTI file using the subject ID and sequence identifier.</p> <p>Parameters: - <code>root</code> (<code>str</code>): The root directory containing subject data. - <code>subject_id</code> (<code>str</code>): The subject's ID. - <code>seq</code> (<code>str</code>, optional): The sequence identifier (e.g., \"_seg\"). Defaults to \"_seg\". - <code>as_array</code> (<code>bool</code>, optional): If <code>True</code>, returns the NIfTI file as a NumPy array. Defaults to <code>False</code>.</p> <p>Returns: - <code>SimpleITK.Image</code> or <code>np.ndarray</code>: The loaded NIfTI file.</p> <p>Notes: - Logs warnings if the sequence is not found.</p>"},{"location":"API_reference/utils/sequences/#read_sequences_dict","title":"<code>read_sequences_dict()</code>","text":"<p>Description: Reads a dictionary of NIfTI sequences for a subject from the specified directory.</p> <p>Parameters: - <code>root</code> (<code>str</code>): The root directory containing subject data. - <code>subject_id</code> (<code>str</code>): The subject's ID. - <code>sequences</code> (<code>List[str]</code>, optional): A list of sequence identifiers to load (e.g., <code>[\"_t1\", \"_t1ce\", \"_t2\", \"_flair\"]</code>). Defaults to these four sequences.</p> <p>Returns: - <code>dict</code>: A dictionary with sequence names as keys and the corresponding NIfTI files or <code>None</code> if a sequence is not found.</p>"},{"location":"API_reference/utils/sequences/#get_spacing","title":"<code>get_spacing()</code>","text":"<p>Description: Extracts the voxel spacing from a <code>SimpleITK.Image</code>.</p> <p>Parameters: - <code>img</code> (<code>SimpleITK.Image</code>): The input image.</p> <p>Returns: - <code>np.ndarray</code>: The voxel spacing. Defaults to <code>[1, 1, 1]</code> if the image is empty.</p>"},{"location":"API_reference/utils/sequences/#build_nifty_image","title":"<code>build_nifty_image()</code>","text":"<p>Description: Converts a segmentation NumPy array into a <code>SimpleITK.Image</code>.</p> <p>Parameters: - <code>segmentation</code> (<code>np.ndarray</code>): The input segmentation array.</p> <p>Returns: - <code>SimpleITK.Image</code>: The converted image.</p> <p>Exceptions: - Raises <code>ValueError</code> for invalid inputs.</p>"},{"location":"API_reference/utils/sequences/#label_replacement","title":"<code>label_replacement()</code>","text":"<p>Description: Maps original labels in a segmentation array to new labels.</p> <p>Parameters: - <code>segmentation</code> (<code>np.ndarray</code>): The input segmentation array. - <code>original_labels</code> (<code>list</code>): List of original labels. - <code>new_labels</code> (<code>list</code>): List of new labels.</p> <p>Returns: - <code>np.ndarray</code>: The segmentation array with labels replaced.</p> <p>Exceptions: - Raises <code>ValueError</code> if the lengths of the label lists do not match.</p>"},{"location":"API_reference/utils/sequences/#iterative_labels_replacement","title":"<code>iterative_labels_replacement()</code>","text":"<p>Description: Iteratively replaces labels in segmentation files within a directory.</p> <p>Parameters: - <code>root_dir</code> (<code>str</code>): The root directory containing segmentation files. - <code>original_labels</code> (<code>list</code>): List of original labels. - <code>new_labels</code> (<code>list</code>): List of new labels. - <code>ext</code> (<code>str</code>, optional): File extension to identify segmentation files. Defaults to \"_seg\". - <code>verbose</code> (<code>bool</code>, optional): If <code>True</code>, prints progress. Defaults to <code>False</code>.</p> <p>Notes: - Logs the number of files processed and skipped.</p>"},{"location":"API_reference/utils/sequences/#count_labels","title":"<code>count_labels()</code>","text":"<p>Description: Counts the number of pixels for each unique label in a segmentation.</p> <p>Parameters: - <code>segmentation</code> (<code>np.ndarray</code>): The input segmentation array. - <code>mapping_names</code> (<code>dict</code>, optional): A mapping of label values to names.</p> <p>Returns: - <code>dict</code>: A dictionary with label counts.</p>"},{"location":"API_reference/utils/sequences/#fit_brain_boundaries","title":"<code>fit_brain_boundaries()</code>","text":"<p>Description: Fits boundaries around the non-zero regions of a 3D medical image with optional padding.</p> <p>Parameters: - <code>sequence</code> (<code>np.ndarray</code>): The input 3D image. - <code>padding</code> (<code>int</code>, optional): The amount of padding to add around the boundaries. Defaults to <code>1</code>.</p> <p>Returns: - <code>np.ndarray</code>: The cropped 3D image with boundaries fitted around the non-zero regions.</p>"},{"location":"about/LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2024 Carlos Aumente Maestro</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"about/release-notes/","title":"\ud83e\uddfe Release Notes \u2013 AUDIT v0.1.0","text":"<p>\ud83d\udcc5 Release date: July 10, 2025</p> <p>We\u2019re excited to announce the first public release of AUDIT, a lightweight and interactive Python tool for  evaluating medical image segmentation models, especially on MRI datasets. This version focuses on enabling fast exploration and quality control with minimal setup, bridging model developers  and researchers.</p>"},{"location":"about/release-notes/#get-involved","title":"\ud83d\udcec Get Involved","text":"<ul> <li>Try it: auditapp.streamlit.app </li> <li>Read the docs: caumente.github.io </li> <li>Contribute or report bugs: github.com/caumente/audit</li> </ul> <p>Thank you for trying AUDIT \u2014 let\u2019s improve medical segmentation evaluation together!</p>"},{"location":"about/release-notes/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<p>Built on top of an amazing open-source ecosystem:</p> <ul> <li>Streamlit \u2013 rapid UIs  </li> <li>Plotly \u2013 interactive graphs  </li> <li>NiBabel \u2013 neuroimaging I/O  </li> <li>ITK-SNAP \u2013 image inspection</li> </ul>"},{"location":"analysis_modes/home_page/","title":"Home page","text":"<p>Welcome to the AUDIT application homepage!</p> <p>The homepage is the entry point to explore the core functionalities of AUDIT.  Currently, the latest version of AUDIT is preloaded with sample data and deployed using Streamlit.  </p> <p>You can access it here: https://auditapp.streamlit.app</p>"},{"location":"analysis_modes/home_page/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the homepage interface:</p> <p></p>"},{"location":"analysis_modes/home_page/#what-youll-find-on-the-homepage","title":"\ud83e\udded What you\u2019ll find on the homepage","text":"<p>Upon opening the app, you will see:</p> <ul> <li> <p>Project title and credits   Includes information about the creators, affiliations, and contributors.</p> </li> <li> <p>Overview and description   A summary of what AUDIT is, what it does, and its main capabilities.</p> </li> <li> <p>Quick access to analysis modes   Buttons or tabs that guide you to various sections such as:</p> </li> <li> <p>Documentation and repository links   At the bottom of the page, you can find links to this documentation and the GitHub repository.</p> </li> </ul> <p>The homepage is designed to give users a quick orientation before jumping into analysis. It serves as a visual hub where users can read the goals of the project, explore capabilities, and navigate easily across different tools within the app.</p> <p>If you're new to AUDIT, we recommend watching the video above and starting with the Univariate Analysis section to explore your first dataset.</p>"},{"location":"analysis_modes/longitudinal/","title":"Longitudinal analysis","text":"<p>The Longitudinal analysis mode in AUDIT allows users to track how lesion sizes evolve over time in individual  subjects. It compares observed (ground truth) and predicted lesion sizes across multiple timepoints, providing insight  into how well a model captures progression or regression trends.</p> <p>The goals of longitudinal analysis are to:</p> <ul> <li>Assess how well a model predicts lesion sizes over time.</li> <li>Evaluate temporal consistency in predictions across multiple scans.</li> <li>Identify systematic over- or under-estimation of lesion progression.</li> <li>Understand patient-specific modeling behavior.</li> <li>Support clinical and research decisions involving disease monitoring.</li> </ul>"},{"location":"analysis_modes/longitudinal/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the longitudinal analysis mode interface:</p> <p></p>"},{"location":"analysis_modes/longitudinal/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/longitudinal/#1-dataset-selection","title":"1. Dataset selection","text":"<p>Users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the longitudinal analysis is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how a segmentation model performs on  a case from a specific dataset.</p> <p>Make sure that: - The dataset contains multiple timepoints per subject. - The label alignment is correct across timepoints (ground truth and predictions must refer to the same anatomical regions). - The same dataset version and preprocessing pipeline are used for all models being analyzed.</p> <p>It is important that the segmentation labels and the ground truth labels are aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it is explained how to modify the  dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>Ensure that all selected models were evaluated using the same dataset version, and that predictions were computed using consistent preprocessing and alignment protocols. Otherwise, performance comparisons may be biased.</p>"},{"location":"analysis_modes/longitudinal/#2-model-selection","title":"2. Model selection","text":"<p>Users must choose one of the available segmentation models to evaluate in this mode. This model must have associated  prediction files, from which performance metrics were precomputed by the AUDIT backend. These precalculated  metrics are then loaded and visualized in the timeline.</p> <p>AUDIT will automatically:</p> <ul> <li>Load precomputed lesion sizes (in mm\u00b3) from the model's prediction files.</li> <li>Compare these with the ground truth lesion sizes.</li> <li>Display both trajectories (predicted and observed) along a timeline.</li> </ul> <p>Tip</p> <p>If you have multiple models available for the same dataset, switching between them is a powerful way to  compare their behavior under the same data conditions and identify strengths or weaknesses specific to each  architecture.</p>"},{"location":"analysis_modes/longitudinal/#3-select-the-subject-id-to-visualize","title":"3. Select the subject ID to visualize","text":"<p>Users have the option to select which subject(s) they want to analyze.</p> <p>Selecting an individual subject ID will display the longitudinal analysis for that single case. This  allows detailed inspection of how the model performed on that particular subject, helping to identify  subject-specific errors or unique segmentation challenges.</p> <p>This flexibility enables both granular, case-by-case evaluation and broad, cohort-level assessment within the same  interface.</p>"},{"location":"analysis_modes/longitudinal/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The longitudinal analysis panel provides a detailed and interactive visualization of how lesion sizes evolve over time  for a selected subject. The x-axis represents the different timepoints (e.g., baseline and follow-up scans), while  the y-axis shows lesion size in cubic millimeters (mm\u00b3). This enables users to inspect the temporal dynamics of lesion  progression or regression, both in the ground truth and in the model predictions.</p> <p> Figure 1: Longitudinal analysis mode in AUDIT. Comparison of observed (orange solid line) and predicted (yellow solid line) lesion sizes across timepoints. Percentage values over the lines indicate tumor growth or shrinkage between consecutive timepoints. Dotted blue lines and their labels show the relative error between observed and predicted lesion sizes at each timepoint.</p> <p>Two solid lines are displayed: the orange line represents the observed lesion sizes (ground truth), and the yellow  line shows the predicted lesion sizes estimated by the selected segmentation model. Each point on these lines  corresponds to a scan at a specific timepoint. The closer the two lines are, the more accurate the model is at  capturing the lesion size for that particular time.</p> <p>Between each pair of consecutive timepoints, percentage values are shown above the orange and yellow lines. These  indicate the relative change in lesion size compared to the previous timepoint\u2014positive values reflect lesion  growth, while negative values represent shrinkage. This allows for an intuitive comparison of how the model interprets  progression or treatment response compared to the actual clinical evolution.</p> <p>To highlight discrepancies between predictions and ground truth, the panel also includes dotted vertical blue lines  connecting the predicted and observed lesion sizes at each timepoint. Next to each line, a blue percentage indicates  the relative error in volume estimation by the model. Larger percentages point to greater divergence from the ground  truth, making it easy to spot timepoints where the model struggles.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/multi_model/","title":"Multi-model performance comparison","text":"<p>The Multi-model performance comparison analysis mode in AUDIT allows users to evaluate and compare a set of segmentation models on a single dataset, across multiple metrics and tumor regions.</p> <p>It provides both a summary table and an interactive boxplot-based visualization, where each box represents the  performance distribution of a model on a specific region and metric. This allows users to intuitively assess which  models perform better in which contexts, identify strengths and weaknesses, and support informed model selection decisions.</p> <p>The purpose of multi-model performance analysis is to:</p> <ul> <li>Compare several segmentation models on the same dataset under identical conditions.</li> <li>Assess performance across multiple tumor regions and evaluation metrics.</li> <li>Identify strengths and weaknesses of each model depending on the region or metric.</li> <li>Visualize inter-model differences in terms of robustness, variability, and outlier behavior.</li> <li>Support model benchmarking, ensemble design, and model selection over different model versions.</li> </ul>"},{"location":"analysis_modes/multi_model/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the segmentation error matrix mode interface:</p> <p></p>"},{"location":"analysis_modes/multi_model/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/multi_model/#1-dataset-selection","title":"1. Dataset selection","text":"<p>In this mode, users can select only one dataset. This ensures a consistent and fair comparison, as all models are  evaluated on the exact same subjects.</p> <p>In this mode, users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the multi-model performance analysis is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how a segmentation model performs on  a specific dataset, ensuring a consistent and fair comparison, as all models are evaluated on the exact same subjects.</p> <p>It is important that the segmentation labels and the ground truth labels must be aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it is explained how to modify the  dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>Ensure that all selected models were evaluated using the same dataset version, and that predictions were computed using consistent preprocessing and alignment protocols. Otherwise, performance comparisons may be biased.</p>"},{"location":"analysis_modes/multi_model/#2-model-selection","title":"2. Model selection","text":"<p>Users can select two or more segmentation models to include in the comparison. These models must have available  prediction files and precomputed evaluation metrics for the selected dataset by AUDIT's backend. Once selected, the  dashboard will automatically load and visualize their performance across the chosen metrics and regions.</p> <p>Each model is represented by a distinct color in the boxplot grid, ensuring easy visual differentiation and making it  straightforward to spot performance differences across metrics and tumor subregions.</p> <p>This mode is especially useful when:</p> <ul> <li>Comparing successive versions of the same model (e.g., v1, v2, v3) where each version incorporates incremental improvements such as new loss functions, or preprocessing pipelines.</li> <li>Conducting ablation studies, where specific components (e.g., attention modules, auxiliary branches, normalization layers) are removed to evaluate their contribution to overall performance.</li> <li>Evaluating model robustness across regions to determine whether improvements are consistent or limited to certain tumor substructures.</li> <li>Benchmarking against baseline methods such as classical segmentation tools or previously published models.</li> </ul> <p>Tip</p> <p>This mode is ideal for researchers and developers who want to track progress over time, validate architectural changes, or prepare comparative figures for scientific publications. Including both baseline and experimental models can help identify trade-offs and guide future improvements.</p>"},{"location":"analysis_modes/multi_model/#3-region-selection","title":"3. Region selection","text":"<p>By default, model performance is shown aggregated by region. This means that both the summary table and each boxplot  represent a model\u2019s average performance across all tumor subregions. This aggregated view is useful to quickly grasp  overall trends and identify which models perform best on average.</p> <p>However, AUDIT also allows users to disaggregate performance by individual regions, offering a more detailed and  fine-grained analysis. When this option is enabled, the boxplots are split by region, and the performance of each model  is shown separately for each tumor subregion.</p> <p>This disaggregated view is particularly important in tumor segmentation tasks, where subregions may differ  significantly in size, intensity, morphology, or clinical relevance. A model may achieve a high average Dice score by  performing well on large, easy-to-segment regions, while still struggling with smaller or more heterogeneous areas,  a pattern that would only become visible when analyzing regions individually.</p> <p>Tip</p> <p>Use the disaggregated view to detect region-specific weaknesses, such as models that consistently underperform on  small enhancing tumor regions or show high boundary error in the tumor core.</p>"},{"location":"analysis_modes/multi_model/#4-metric-selection","title":"4. Metric selection","text":"<p>Users must select a single evaluation metric that quantifies the segmentation quality of the chosen model. As mentioned,  these metrics are precomputed by the AUDIT backend and loaded from disk when the analysis is launched. The selected  metric is plotted on the Y-axis, allowing users to analyze performance relative to the selected feature (X-axis).</p> <p>AUDIT natively supports the following metrics:</p> Metric Description Interpretation <code>dice</code> Dice coefficient; measures the overlap between predicted and ground truth masks. Higher is better <code>jacc</code> Jaccard Index (Intersection over Union); similar to Dice but penalizes false positives more. Higher is better <code>accu</code> Accuracy; proportion of correctly classified voxels across all classes. Higher is better <code>prec</code> Precision; proportion of predicted positives that are true positives (i.e., few false positives). Higher is better <code>sens</code> Sensitivity (Recall); proportion of true positives detected (i.e., few false negatives). Higher is better <code>spec</code> Specificity; proportion of true negatives correctly identified. Higher is better <code>haus</code> Hausdorff distance (95% percentile); measures the worst-case boundary error. Lower is better <code>lesion_size</code> Total volume of the predicted lesion, in mm\u00b3. Application-dependent <p>Each metric offers insight into different aspects of model performance:</p> <ul> <li>Overlap metrics like Dice and Jaccard assess spatial agreement.</li> <li>Threshold metrics like precision and sensitivity are useful for imbalance analysis.</li> <li>Distance metrics (e.g., Hausdorff) reflect boundary accuracy.</li> <li>Lesion size serves as a complementary indicator to understand under- or over-segmentation.</li> </ul> <p>Warning</p> <p>Some metrics (especially <code>dice</code>, <code>haus</code>, or <code>lesion_size</code>) may behave differently across regions with different sizes. Small structures are more sensitive to minor errors, so always interpret values in context.</p>"},{"location":"analysis_modes/multi_model/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The Multi-model performance comparison mode provides two complementary dashboards:</p>"},{"location":"analysis_modes/multi_model/#dashboard-1-summary-table","title":"Dashboard 1: Summary table","text":"<p>At the top of the interface, a summary table displays the mean \u00b1 standard deviation of each selected metric for  all selected models. These values are computed across all subjects, and by default, aggregated over all tumor regions.</p> <p>This compact overview enables users to quickly identify which models perform best on average, and which ones exhibit  higher variability.</p> <p> Figure 1: Summary table showing model performance aggregated over all tumor regions.</p> <p>The \"Aggregated\" checkbox above the table allows switching between showing average scores across all  regions or displaying each region individually.</p> <p> Figure 2: Summary table with the \"Aggregated\" option disabled. Each region is shown separately for more granular analysis.</p>"},{"location":"analysis_modes/multi_model/#dashboard-2-boxplot-visualization","title":"Dashboard 2: Boxplot visualization","text":"<p>Additionally to the table, an interactive boxplot provides a detailed view of the distribution of metric values. This plot visualizes the variability, median, and outliers for each selected metric, grouped by model.</p> <p>By default, the plot shows aggregated values \u2014 i.e., one box per model and metric, summarizing performance across  all tumor regions.</p> <ul> <li>X-axis: Selected metrics (e.g., Dice, Jaccard, Sensitivity)  </li> <li>Y-axis: Metric value  </li> <li>Color: Each model is assigned a distinct color  </li> <li>Boxes: One per model and metric, showing median, quartiles, and outliers</li> </ul> <p> Figure 3: Boxplots showing the distribution of aggregated model performance across metrics. Each color represents a different model.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/multivariate/","title":"Multivariate feature analysis","text":"<p>The Multivariate analysis mode in AUDIT enables users to investigate how multiple features interact with each other across different datasets. It provides a two-dimensional scatter plot where each data point represents a subject, and its position is determined by two selected features. This visualization is particularly useful for detecting complex patterns, identifying correlations, and revealing dataset-specific behaviors or outliers that may not be visible in univariate analyses.</p> <p>The goals of multivariate analysis are to:</p> <ul> <li>Explore relationships between pairs of features across multiple datasets.</li> <li>Detect hidden clusters or trends in the feature space.</li> <li>Identify dataset shifts or feature interactions that might affect model performance.</li> <li>Spot outliers that deviate from the main population.</li> <li>Visually compare cohorts in a shared 2D space.</li> </ul>"},{"location":"analysis_modes/multivariate/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the multivariate interface:</p> <p></p>"},{"location":"analysis_modes/multivariate/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/multivariate/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Users can load and compare multiple datasets simultaneously in the Multivariate analysis mode. This  enables side-by-side inspection of how a given feature varies across different cohorts \u2014 for example,  across different institutions, scanner protocols, or study years.</p> <p>Each dataset is automatically assigned a distinct color in all plots, ensuring an easy interpretation.</p> <p>Datasets can be:</p> <ul> <li>Selected or deselected using the dedicated selector panel on the left-hand side of the dashboard.</li> <li>Toggled directly from the interactive Plotly legend, which allows temporary hiding/showing of datasets with a single click.</li> </ul> <p>While the Plotly legend interaction is useful for quick comparisons, we recommend using the left-side dataset selector  for a more stable and consistent user experience, especially when exporting plots or applying filters.</p>"},{"location":"analysis_modes/multivariate/#2-feature-selection-x-and-y-axes","title":"2. Feature Selection (X and Y axes)","text":"<p>Users must select two features, one for the X-axis and one for the Y-axis, from the available categories:</p> Feature Description Example Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>The combination of different types of features (e.g., texture vs. spatial) can help uncover intricate data patterns or technical biases.</p> <p>Some features depend heavily on the imaging modality, and comparisons between datasets are only meaningful when  extracted from preprocessed MRIs. Texture features in particular are highly sensitive to variations in acquisition  protocols and preprocessing pipelines. This sensitivity arises because descriptors like entropy, contrast, and energy  are affected by voxel size and image spacing, intensity range and quantization strategy, and other technical aspects  related to the scanners. Such technical differences are especially relevant in multi-center studies, where spatial  resolution and image quality may vary substantially.</p> <p>Other features, such as tumor volume, center of mass, or spatial location, are typically derived from segmentation  masks that take into account the voxel spacing, and are independent of intensity information. These are more robust  to differences in acquisition and preprocessing, making them suitable for comparisons even across heterogeneous cohorts.</p> <p>Info</p> <p>More technical details can be found in our publication and within the API reference.</p>"},{"location":"analysis_modes/multivariate/#3-color-feature","title":"3. Color Feature","text":"<p>In addition to selecting the X and Y axes, users can define a third variable to control the color of each data point in  the scatter plot. This feature enables a richer multivariate exploration by adding a visual encoding of an additional  dimension, effectively transforming the 2D plot into a pseudo-3D representation.</p> <p>The available options include:</p> <ul> <li> <p>Dataset (default): Colors each point based on the dataset it belongs to. This is particularly useful when      comparing cohorts, such as data from different hospitals, protocols, or population groups. Patterns of separation,      overlap, or clustering between datasets can provide insight into dataset shifts or cohort-specific trends.</p> </li> <li> <p>Any other available feature: Users may also choose to color points based on the value of a specific feature, such      as tumor volume, center of mass, texture entropy, or any other scalar variable present in the dataset. This allows      researchers to visually explore how a third variable distributes over the 2D feature space and whether it aligns      with certain regions, clusters, or gradients.</p> </li> </ul> <p>Colorbars are dynamically generated when using feature-based coloring, providing a reference scale for  interpretation. Continuous variables use a sequential or diverging colormap, while categorical features are  represented using distinct colors.</p> <p>Tip</p> <p>When coloring by a continuous feature, watch for visual patterns such as localized extremes. These often indicate a  non-trivial interaction between the color feature and the selected X/Y pair, which may warrant deeper investigation  or stratified modeling approaches.</p>"},{"location":"analysis_modes/multivariate/#4-highlight-subject","title":"4. Highlight Subject","text":"<p>The Highlight subject option allows users to trace the behavior of an individual patient (or subject) across  plots. By entering a specific subject ID and specifying the corresponding dataset, the selected subject will be  visually highlighted in the dashboard, where the subject appears as a clearly marked point (e.g., with a  different color or shape).</p> <p>This feature is particularly valuable in contexts such as detecting whether a subject is an outlier or aligns with cohort  expectations or verifying how individuals from different cohorts are distributed in shared feature space.</p> <p>Tip</p> <p>When analyzing extreme values, this feature can help determine whether a subject truly deviates from the cohort or  lies within natural variability.</p>"},{"location":"analysis_modes/multivariate/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The Multivariate analysis mode displays an interactive 2D scatter plot:</p> <p> Figure 1: A scatter plot showing the relationship between two selected features across datasets. Each point represents a subject and is colored by dataset.</p> <p> Figure 2: A scatter plot showing the relationship between three selected features across datasets. X-axis and Y-axis represent Kurtosis and     Skewness fo FLAIR sequence intensity pixels. Color represents tumor location for enhancing region.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/pairwise_model/","title":"Pairwise model performance comparison","text":""},{"location":"analysis_modes/segmentation_error/","title":"Segmentation error matrix","text":"<p>The Segmentation Error Matrix analysis in AUDIT provides an intuitive way to assess model performance at the pixel level. It visualizes the confusion matrix of predicted vs. ground truth segmentations for each  subject or aggregated across a dataset. This helps identify common misclassifications between tumor  subregions and assess overall segmentation quality.</p> <p>The goals of segmentation error matrix analysis are to:</p> <ul> <li>Visualize systematic errors across a dataset or individual subject.</li> <li>Assess prediction quality for individual tumor subregions, identifying which classes are most accurately segmented and which are often confused.</li> <li>Spot model failure modes not captured by summary metrics like Dice.</li> </ul>"},{"location":"analysis_modes/segmentation_error/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the segmentation error matrix mode interface:</p> <p></p>"},{"location":"analysis_modes/segmentation_error/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/segmentation_error/#1-dataset-selection","title":"1. Dataset selection","text":"<p>In this mode, users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the segmentation error matrix is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how a segmentation model performs on  a specific dataset by analyzing which tumor subregions are well segmented and which are commonly misclassified.</p> <p>It is important that the segmentation labels and the ground truth labels must be aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it is explained how to modify the  dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>This analysis mode compares the predicted segmentations directly against the ground truth labels. Therefore, it can only be used when the original ground truth segmentations are available in the dataset. If the dataset only contains predictions and lacks reference segmentations, the confusion matrix cannot be computed.</p>"},{"location":"analysis_modes/segmentation_error/#2-model-selection","title":"2. Model selection","text":"<p>In this step, users must select the segmentation model whose predicted outputs will be compared against the ground  truth segmentations of the chosen dataset. This is crucial because the confusion matrix is computed by analyzing the  pixel-wise agreement (or disagreement) between the model\u2019s predictions and the ground truth labels.</p> <p>The dropdown menu lists all available models that have generated predictions for the selected dataset. Selecting a  model will load its prediction masks, which are then aligned with the ground truth segmentations to compute the  pseudo-confusion matrix. Users can compare multiple models by changing this selection and observing differences in the  matrices, which helps identify strengths and weaknesses in segmentation performance at the class level.</p> <p>Models might differ in terms of architecture, training data, or preprocessing pipelines, so this step allows for  flexible evaluation of any medical image segmentation model whose outputs are available in AUDIT.</p>"},{"location":"analysis_modes/segmentation_error/#3-select-the-subject-id-to-visualize","title":"3. Select the subject ID to visualize","text":"<p>Users have the option to select which subject(s) they want to analyze through the confusion matrix:</p> <ul> <li> <p>Specific subject: Selecting an individual subject ID will display the confusion matrix for that single case. This      allows detailed inspection of how the model performed on that particular subject, helping to identify      subject-specific errors or unique segmentation challenges.</p> </li> <li> <p>All: Choosing All aggregates the confusion matrices of every subject in the dataset into a single summary     matrix. This aggregated view provides an overview of the model\u2019s overall common error patterns across the entire     cohort, smoothing out subject-level variability.</p> </li> </ul> <p>This flexibility enables both granular, case-by-case evaluation and broad, cohort-level assessment within the same  interface.</p>"},{"location":"analysis_modes/segmentation_error/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The pseudo-confusion matrix shown in this mode is structured as follows:</p> <ul> <li>Rows represent the ground truth (true) labels, indicating the actual pixel classifications.  </li> <li>Columns represent the predicted labels output by the segmentation model, indicating the model\u2019s pixel-wise classification.   </li> <li>Diagonal cells correspond to correctly classified pixels, where the predicted label matches the true label. Not shown, the matrix highlights only errors.</li> <li>Off-diagonal cells highlight misclassifications, where the predicted label differs from the true label.  </li> </ul> <p>Info</p> <p>Darker colors in the matrix represent larger errors or higher misclassification rates by the model, while lighter cells indicate fewer errors or better agreement between prediction and ground truth.  </p> <p>The confusion matrix is normalized row-wise, meaning each row sums to 100%, which helps interpret the prediction  distribution per true label. Since this is a pseudo-confusion matrix, pixels correctly classified are not indicated  within the matrix. It is expected that the model classifies most of the regions correctly, so this analysis mode focuses  on systematic errors instead.</p> <p>Example</p> <p>For example, if the true label is \"edema\", the cells might be distributed as follows: - A value of 70% in the \"bkg\" column means 70% of edema pixels were wrongly predicted as background. - 25% in the \"enh\" column means 25% of edema pixels were wrongly predicted as enhancing tumor. - 5% in the \"nec\" column means 5% of edema pixels were wrongly predicted as necrosis.</p> <p> Figure 1: Single subject confusion matrix for the nnUNet model on the BraTS2024_SSA dataset. The rows represent the true pixel labels. The columns represent the model's predicted labels. Darker cells indicate a higher percentage of misclassified pixels between true and predicted classes.</p>"},{"location":"analysis_modes/segmentation_error/#additional-options","title":"\ud83e\uddf0 Additional options","text":""},{"location":"analysis_modes/segmentation_error/#normalization-and-averaging","title":"Normalization and averaging","text":"<ul> <li> <p>Normalized per ground truth label (enabled by default): normalizes each row so percentages sum to 100%, which     makes it easier to interpret class-wise prediction behavior. When this button is disabled cells show the total     number of misclassified pixels.</p> </li> <li> <p>Averaged per number of subjects (only available when \"All\" subjects are selected): computes the average     pseudo-confusion matrix across all subjects.</p> </li> </ul> <p>Warning</p> <p>Be aware that the absolute pixel count of misclassified regions can be influenced by the voxel spacing of each MRI scan. For this reason, it is highly recommended to register all datasets to a common reference space, so that voxel  sizes and orientations are standardized. Otherwise, cross-dataset comparisons can become difficult to interpret.</p>"},{"location":"analysis_modes/segmentation_error/#itk-snap-integration","title":"ITK-SNAP integration","text":"<p>When a single subject is selected, users can launch ITK-SNAP to directly inspect the segmentation results in 3D. This launches the ground truth and predicted segmentations side by side.</p> <p>Tip</p> <p>This feature is ideal for verifying segmentation quality when unusual confusion patterns appear in the matrix. It  can reveal issues like label swapping, partial segmentations, or preprocessing errors.</p> <p>Warning</p> <p>ITK-SNAP must be installed and correctly configured in your system for this option to work.</p> <p>All confusion matrices in AUDIT are fully interactive and support:</p> <ul> <li>Hover to reveal exact percentages</li> <li>Dynamic updates when selecting different datasets, models, or subjects</li> </ul> <p>Whether you're debugging segmentation behavior or reporting evaluation results, this tool provides a clear and  quantitative view into model errors and class-wise performance.</p>"},{"location":"analysis_modes/single_model/","title":"Single model performance","text":"<p>The Single model performance analysis mode in AUDIT allows users to evaluate how a given segmentation model performs across multiple datasets based on a chosen feature and evaluation metric.</p> <p>It provides a univariate scatter plot where each point represents a subject, placed according to: - X-axis: A selected feature (e.g., maximum intensity in T1, tumor volume, etc.) - Y-axis: A performance metric (e.g., Dice, Hausdorff distance, etc.)</p> <p>This enables intuitive exploration of how specific image characteristics relate to model performance, helping identify  trends, outliers, or systematic failures.</p> <p>The purpose of single model performance analysis is to:</p> <ul> <li>Assess model performance in relation to a single image-derived feature.</li> <li>Compare how the same model behaves across multiple datasets or cohorts.</li> <li>Identify systematic failure patterns linked to specific feature ranges (e.g., intensity, volume).</li> <li>Detect outlier cases where the model performs unexpectedly well or poorly.</li> <li>Understand how specific features relate to performance, to guide model improvement and debugging.</li> </ul>"},{"location":"analysis_modes/single_model/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the segmentation error matrix mode interface:</p> <p></p>"},{"location":"analysis_modes/single_model/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/single_model/#1-dataset-selection","title":"1. Dataset selection","text":"<p>Users can load and compare multiple datasets simultaneously in the Single model performance analysis mode. This  enables side-by-side inspection of how models perform across different cohorts \u2014 for example,  across different institutions, scanner protocols, or study years.</p> <p>Each dataset is automatically assigned a distinct color in all plots, ensuring an easy interpretation.</p> <p>Datasets can be:</p> <ul> <li>Selected or deselected using the dedicated selector panel on the left-hand side of the dashboard.</li> <li>Toggled directly from the interactive Plotly legend, which allows temporary hiding/showing of datasets with a single click.</li> </ul> <p>While the Plotly legend interaction is useful for quick comparisons, we recommend using the left-side dataset selector  for a more stable and consistent user experience, especially when exporting plots or applying filters.</p> <p>Warning</p> <p>To ensure meaningful comparisons, all datasets should ideally be registered to the same reference space and preprocessed in a consistent way. Otherwise, differences in voxel spacing, intensity scaling, or orientation may introduce bias in both features and performance metrics.</p>"},{"location":"analysis_modes/single_model/#2-model-selection","title":"2. Model selection","text":"<p>Users must choose one of the available segmentation models to evaluate in this mode. This model must have associated  prediction files, from which performance metrics were precomputed by the AUDIT backend. These precalculated  metrics are then loaded and visualized in the scatter plot.</p> <p>The selected model determines which performance values are shown on the Y-axis, one point per subject. This allows  users to inspect how the model performs across subjects, in relation to a selected feature on the X-axis.</p> <p>Tip</p> <p>If you have multiple models available for the same dataset, switching between them is a powerful way to  compare their behavior under the same data conditions and identify strengths or weaknesses specific to each  architecture.</p>"},{"location":"analysis_modes/single_model/#3-metric-selection","title":"3. Metric selection","text":"<p>Users must select a single evaluation metric that quantifies the segmentation quality of the chosen model. As mentioned,  these metrics are precomputed by the AUDIT backend and loaded from disk when the analysis is launched. The selected  metric is plotted on the Y-axis, allowing users to analyze performance relative to the selected feature (X-axis).</p> <p>AUDIT natively supports the following metrics:</p> Metric Description Interpretation <code>dice</code> Dice coefficient; measures the overlap between predicted and ground truth masks. Higher is better <code>jacc</code> Jaccard Index (Intersection over Union); similar to Dice but penalizes false positives more. Higher is better <code>accu</code> Accuracy; proportion of correctly classified voxels across all classes. Higher is better <code>prec</code> Precision; proportion of predicted positives that are true positives (i.e., few false positives). Higher is better <code>sens</code> Sensitivity (Recall); proportion of true positives detected (i.e., few false negatives). Higher is better <code>spec</code> Specificity; proportion of true negatives correctly identified. Higher is better <code>haus</code> Hausdorff distance (95% percentile); measures the worst-case boundary error. Lower is better <code>lesion_size</code> Total volume of the predicted lesion, in mm\u00b3. Application-dependent <p>Each metric offers insight into different aspects of model performance:</p> <ul> <li>Overlap metrics like Dice and Jaccard assess spatial agreement.</li> <li>Threshold metrics like precision and sensitivity are useful for imbalance analysis.</li> <li>Distance metrics (e.g., Hausdorff) reflect boundary accuracy.</li> <li>Lesion size serves as a complementary indicator to understand under- or over-segmentation.</li> </ul> <p>Warning</p> <p>Some metrics (especially <code>dice</code>, <code>haus</code>, or <code>lesion_size</code>) may behave differently across regions with different sizes. Small structures are more sensitive to minor errors, so always interpret values in context.</p>"},{"location":"analysis_modes/single_model/#4-feature-selection","title":"4. Feature selection","text":"<p>Users must choose a feature to use for the X-axis. Features are extracted from the images and segmentations and  can belong to various categories:</p> Feature Description Example Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>Some features depend heavily on the imaging modality, and comparisons between datasets are only meaningful when  extracted from preprocessed MRIs. Texture features in particular are highly sensitive to variations in acquisition  protocols and preprocessing pipelines. This sensitivity arises because descriptors like entropy, contrast, and energy  are affected by voxel size and image spacing, intensity range and quantization strategy, and other technical aspects  related to the scanners. Such technical differences are especially relevant in multi-center studies, where spatial  resolution and image quality may vary substantially.</p> <p>Other features, such as tumor volume, center of mass, or spatial location, are typically derived from segmentation  masks that take into account the voxel spacing, and are independent of intensity information. These are more robust  to differences in acquisition and preprocessing, making them suitable for comparisons even across heterogeneous cohorts.</p> <p>Info</p> <p>More technical details can be found in our publication and within the API reference.</p>"},{"location":"analysis_modes/single_model/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The single model performance dashboard displays an interactive 2D scatter plot in which each point represent a subject. Each dot corresponds to a subject, positioned by its feature value (X-axis) and performance  metric (Y-axis), and colored by dataset.</p> <p> Figure 1: A scatter plot showing the relationship between a selected features and metric across datasets. Each point represents a subject and is colored by dataset.</p> <p>Info</p> <p>The \u201cAggregated\u201d checkbox allows toggling between: - Aggregated mode: Each point summarizes the subject across all regions. - Disaggregated mode: The plot is faceted by region, showing one subplot per tumor region.</p>"},{"location":"analysis_modes/single_model/#additional-options","title":"\ud83e\uddf0 Additional options","text":""},{"location":"analysis_modes/single_model/#highlighting-points","title":"Highlighting points","text":"<p>Users can double-click on a point to highlight a specific subject in red. Highlighted points are preserved across views and can be reset by clicking the Reset highlighted cases button.</p> <p>This is useful for inspecting specific subjects with particularly low or high performance. Understanding why some cases  are easier (or harder) to segment can be crucial to improving model generalization capabilities.</p> <p> Figure 2: A scatter plot showing the relationship between a selected features and metric across datasets. Each point represents a subject and is colored by dataset. Notice how there are two specific subjects highlighted in red to monitor their performance.</p>"},{"location":"analysis_modes/single_model/#aggregating","title":"Aggregating","text":"<p>Users can switch between visualizing the subjects in an aggregated or disaggregated view. This is particularly useful to understand which subregions are more challenging to segment.</p> <p>Additionally, it helps understand how different tumor regions contribute to the overall  performance distribution. For example, a model might appear robust in aggregated view but show poor performance on  specific subregions when disaggregated.</p> <p>In aggregated view, performance metrics are computed across the entire segmentation mask (all regions combined). In disaggregated view, the plot is faceted by region, and each subplot shows region-specific performance.</p> <p>Tip</p> <p>Use the highlighting option to mark the patients you are interested in, and then switch to the disaggregated view. This will show how your selected cases perform across different tumor subregions, helping reveal region-specific issues or inconsistencies.</p> <p> Figure 3: A disaggregated view scatter plot showing the relationship between a selected features and metric across datasets. Each point represents a subject and is colored by dataset. Notice how there are two specific subjects highlighted in red to monitor their performance.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/subjects_exploration/","title":"Subjects' exploration","text":""},{"location":"analysis_modes/univariate/","title":"Univariate feature analysis","text":"<p>The Univariate analysis mode in AUDIT provides a comprehensive and intuitive way to explore how a single variable  relates to the behavior of a segmentation model, or to differences across datasets. This mode supports descriptive  analytics through visual exploration, enabling researchers to detect outliers, identify dataset shifts, and understand  variability in imaging or metadata features that may influence downstream model performance.</p> <p>The purpose of univariate analysis is to:</p> <ul> <li>Compare datasets or cohorts based on a single feature of interest.</li> <li>Explore the distribution of quantitative relevant features (statistical, texture, tumor, etc.).</li> <li>Detect outliers or atypical cases that may affect downstream modeling.</li> <li>Identify dataset shifts (e.g., site or scanner variability).</li> <li>Visualize central tendencies and variability in the selected feature.</li> </ul>"},{"location":"analysis_modes/univariate/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the homepage interface:</p> <p></p>"},{"location":"analysis_modes/univariate/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/univariate/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Users can load and compare multiple datasets simultaneously in the Univariate analysis mode. This  enables side-by-side inspection of how a given feature varies across different cohorts \u2014 for example,  across different institutions, scanner protocols, or study years.</p> <p>Each dataset is automatically assigned a distinct color in all plots, ensuring an easy interpretation.</p> <p>Datasets can be:</p> <ul> <li>Selected or deselected using the dedicated selector panel on the left-hand side of the dashboard.</li> <li>Toggled directly from the interactive Plotly legend, which allows temporary hiding/showing of datasets with a single click.</li> </ul> <p>While the Plotly legend interaction is useful for quick comparisons, we recommend using the left-side dataset selector  for a more stable and consistent user experience, especially when exporting plots or applying filters.</p>"},{"location":"analysis_modes/univariate/#2-feature-selection","title":"2. Feature Selection","text":"<p>Users can choose from different feature categories such as:</p> Feature Description Example Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>Some features depend heavily on the imaging modality, and comparisons between datasets are only meaningful when  extracted from preprocessed MRIs. Texture features in particular are highly sensitive to variations in acquisition  protocols and preprocessing pipelines. This sensitivity arises because descriptors like entropy, contrast, and energy  are affected by voxel size and image spacing, intensity range and quantization strategy, and other technical aspects  related to the scanners. Such technical differences are especially relevant in multi-center studies, where spatial  resolution and image quality may vary substantially.</p> <p>Other features, such as tumor volume, center of mass, or spatial location, are typically derived from segmentation  masks that take into account the voxel spacing, and are independent of intensity information. These are more robust  to differences in acquisition and preprocessing, making them suitable for comparisons even across heterogeneous cohorts.</p> <p>Info</p> <p>More technical details can be found in our publication and within the API reference.</p>"},{"location":"analysis_modes/univariate/#3-filtering","title":"3. Filtering","text":"<p>Several filtering strategies are available to clean or constrain the data prior to visualization:</p> <ul> <li>No filter: Show all data as-is.</li> <li>Removing outliers: Exclude points out of a range of values provided by the users.</li> <li>Clipping outliers: Truncate values beyond a user-defined threshold.</li> <li>Standard deviation-based filtering: Retain only values within a specified number of standard deviations from the mean.</li> </ul> <p>This helps in focusing on the core structure of the data, minimizing the impact of extreme values.</p> <p>Warning</p> <p>Some filters or parameter settings may be overly restrictive, reducing the number of visible data points and affecting the interpretability of the visualizations. We encourage users to carefully adjust filtering thresholds depending on the context and data distribution.</p>"},{"location":"analysis_modes/univariate/#4-highlight-subject","title":"4. Highlight subject","text":"<p>The Highlight subject option allows users to trace the behavior of an individual patient (or subject) across  plots. By entering a specific subject ID and specifying the corresponding dataset, the selected subject will be  visually highlighted in Boxplots and Violinplots, where the subject appears as a clearly marked point (e.g., with a  different color or shape).</p> <p>This feature is particularly valuable in contexts such as detecting whether a subject is an outlier or aligns with cohort  expectations or verifying how individuals from different cohorts are distributed in shared feature space.</p> <p>The subject remains highlighted even when toggling between plot types, making it easier to interpret its relative  position under different visual perspectives.</p> <p>Tip</p> <p>When analyzing extreme values, this feature can help determine whether a subject truly deviates from the cohort or lies within natural variability.</p>"},{"location":"analysis_modes/univariate/#visualizations","title":"\ud83d\udcca Visualizations","text":"<p>The Univariate analysis mode provides two dedicated dashboards, each tailored to a specific type of visual  exploration:</p>"},{"location":"analysis_modes/univariate/#dashboard-1-distribution-by-dataset","title":"Dashboard 1: Distribution by dataset","text":"<p>This view allows users to compare feature distributions across datasets or groups using statistical summaries and  individual data points.</p>"},{"location":"analysis_modes/univariate/#box","title":"Box","text":"<p>Displays the median, interquartile range, and potential outliers for each group. Useful for comparing central tendency and variability between datasets.</p> <p> Figure 1: This box plot summarizes the subject-level distribution of a selected feature across multiple datasets, showing median, quartiles, and outliers.</p>"},{"location":"analysis_modes/univariate/#violin-plot","title":"Violin Plot","text":"<p>Extends the boxplot with a mirrored kernel density estimation, offering insight into the shape of the distribution  (e.g., multimodality or skewness). Great for identifying subtle differences between groups.</p> <p> Figure 2: Violin plots illustrate the full shape of feature distributions across datasets, useful for spotting multimodal or skewed distributions.</p>"},{"location":"analysis_modes/univariate/#box-points","title":"Box + Points","text":"<p>Combines a boxplot with individual data points overlaid.  </p> <p> Figure 3: Box + Points visualization combines summary statistics with subject-level markers, making it easier to spot trends or anomalies within groups.</p>"},{"location":"analysis_modes/univariate/#dashboard-2-global-distribution-shape","title":"Dashboard 2: Global Distribution Shape","text":"<p>This view focuses on the overall shape and structure of the feature distribution across datasets.</p>"},{"location":"analysis_modes/univariate/#probability-density-smoothed-distribution","title":"Probability Density (Smoothed Distribution)","text":"<p>Displays a smoothed estimate of the probability density function (PDF) for each dataset, often using kernel density estimation.</p> <p> Figure 4: Probability density functions estimate the underlying shape of feature distributions, revealing subtle differences in central tendency and spread.</p>"},{"location":"analysis_modes/univariate/#histogram","title":"Histogram","text":"<p>Shows the frequency of observations grouped into bins.</p> <p> Figure 5: Histograms display the frequency distribution of a selected feature across datasets, allowing users to tune bin width for detailed or coarse analysis.</p> <p>When using the Histogram plot, users can configure the number of bins used to partition the feature range. This  setting controls the granularity of the histogram. Typically, a smaller number of bins results in broader, coarser  groupings, which can help reveal overall distribution trends. In contrast, a larger number of bins produces a more  detailed view, potentially exposing finer structure or noise in the data.</p> <p>This setting is especially useful when exploring skewed, sparse, or multimodal distributions, where default binning may  obscure relevant patterns.</p> <p>Warning</p> <p>Despite the number defined in the settings, Plotly may automatically adjust the bin size and number of bins to optimize the visualization. As a result, the final number of bins rendered may not exactly match the user\u2019s input.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"getting_started/additional_notes/","title":"Additional Tips &amp; Best Practices","text":"<ul> <li>For ITK-Snap integration, ensure it is installed and configured correctly.</li> <li>Use the <code>logs/</code> directory to monitor execution details and debug issues.</li> <li>Always start from the example configuration files and project structure provided in the repository.</li> <li>Keep your Python environment isolated to avoid dependency conflicts.</li> </ul>"},{"location":"getting_started/additional_notes/#next-steps","title":"\ud83c\udfc1 Next Steps","text":"<p>You're all set to start using AUDIT! Dive into your MRI data, evaluate AI models, and gain deeper insights with AUDIT\u2019s powerful tools.</p> <p>For further details, check out the other sections of the documentation and the AUDIT GitHub repository.</p> <p>Tip</p> <p>Following best practices and recommended workflows ensures reproducible and reliable results.</p>"},{"location":"getting_started/configuration/","title":"Configuration Guide","text":"<p>AUDIT uses configuration files to define paths, settings, and parameters for feature extraction, metric evaluation, and app customization. We recommend starting from the example files included in the repository and adapting them to your needs.</p>"},{"location":"getting_started/configuration/#configuration-file-types","title":"\u2699\ufe0f Configuration File Types","text":"<ul> <li>Feature extraction: <code>feature_extraction.yaml</code></li> <li>Metric evaluation: <code>metric_extraction.yaml</code></li> <li>App settings: <code>app.yaml</code></li> </ul> <p>All configuration files are stored in the <code>config/</code> directory.</p>"},{"location":"getting_started/configuration/#example-feature-extraction-config","title":"\ud83d\udcdd Example: Feature Extraction Config","text":"<p>Defines dataset paths, label mappings, features to extract, and longitudinal study settings.</p> <pre><code># Paths to datasets\ndata_paths:\n  BraTS: '/home/user/AUDIT/datasets/BraTS/BraTS_images'\n  UCSF: '/home/user/AUDIT/datasets/UCSF/UCSF_images'\n\n# Label mapping\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# Features to extract\nfeatures:\n  statistical: true\n  texture: false\n  spatial: true\n  tumor: true\n\n# Longitudinal settings\nlongitudinal:\n  UCSF:\n    pattern: \"_\"\n    longitudinal_id: 1\n    time_point: 2\n\n# Output path\noutput_path: '/home/user/AUDIT/outputs/features'\n</code></pre>"},{"location":"getting_started/configuration/#example-metric-extraction-config","title":"\ud83d\udcdd Example: Metric Extraction Config","text":"<p>Defines dataset and prediction paths, label mappings, metrics to compute, and output settings.</p> <pre><code># Dataset path\ndata_path: '/home/user/AUDIT/datasets/BraTS/BraTS_images'\n\n# Model predictions\nmodel_predictions_paths:\n  nnUnet: '/home/user/AUDIT/datasets/BraTS/BraTS_seg/nnUnet'\n\n# Label mapping\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# Metrics to compute\nmetrics:\n  dice: true\n  jacc: true\n  accu: true\n  prec: true\n  sens: true\n  spec: true\n  haus: true\n  size: true\n\n# Library and output\npackage: custom\ncalculate_stats: false\noutput_path: '/home/user/AUDIT/outputs/metrics'\nfilename: 'BraTS'\n</code></pre>"},{"location":"getting_started/configuration/#example-app-config","title":"\ud83d\udcdd Example: App Config","text":"<p>Defines dataset, feature, and metric paths for the web app.</p> <pre><code>labels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\ndatasets_path: '/home/user/AUDIT/datasets'\nfeatures_path: '/home/user/AUDIT/outputs/features'\nmetrics_path: '/home/user/AUDIT/outputs/metrics'\n\nraw_datasets:\n  BraTS: \"${datasets_path}/BraTS/BraTS_images\"\n  UCSF: \"${datasets_path}/UCSF/UCSF_images\"\n\nfeatures:\n  BraTS: \"${features_path}/extracted_information_BraTS.csv\"\n  UCSF: \"${features_path}/extracted_information_UCSF.csv\"\n\nmetrics:\n  UCSF: \"${metrics_path}/extracted_information_UCSF.csv\"\n\npredictions:\n  UCSF:\n    SegResNet: \"${datasets_path}/UCSF/UCSF_seg/SegResNet\"\n</code></pre>"},{"location":"getting_started/configuration/#getting-started","title":"\ud83c\udfc1 Getting Started","text":"<ul> <li>Start from the example configuration files in the repository.</li> <li>Adjust paths and settings to match your environment and project needs.</li> <li>For more details, see the API reference and other documentation sections.</li> </ul> <p>Tip</p> <p>Consistent configuration ensures reproducible and reliable results.</p>"},{"location":"getting_started/getting_started/","title":"Getting started","text":"<p>Before installing AUDIT on your local computer, we recommend checking out the publicly deployed app on the Streamlit  Cloud server at https://auditapp.streamlit.app/.</p> <p>Users can familiarize themselves with AUDIT through this tool without needing to install anything. When users are ready to make extensive use of it and analyze their own datasets, they will need to follow the steps outlined below.</p>"},{"location":"getting_started/getting_started/#1-installation","title":"1. Installation","text":"<p>The installation of AUDIT can be done in different ways depending on the user's needs. For users who plan to use AUDIT  regularly in their projects, we recommend the \"For Developers\" option, as it allows for easy extension of the library  with new functionalities. For users who do not want to have all the code on their local machine, there is the  \"For Standard Users\" option.</p> <p>The \"For Developers\" option is not necessarily more complex and, in fact, is the one recommended by the authors.</p>"},{"location":"getting_started/getting_started/#11-for-standard-users-using-pip","title":"1.1 For standard users - Using PIP","text":"<p>Install the latest available AUDIT version directly from PyPI (when available) through the following command: </p> <pre><code>pip install auditapp\n</code></pre> <p>This is the simplest method if you just want to use the library without modifying the source code. However,  configuration files and project structure still need to be set up.</p>"},{"location":"getting_started/getting_started/#12-for-developers-using-audit-repository","title":"1.2. For developers - Using AUDIT repository","text":"<p>For development or if you need access to the latest updates, install AUDIT from our repository.  If you do not use Poetry for dependency management, you should choose option 1.2.1. However, if you are  familiar with Poetry, select option 1.2.2.</p>"},{"location":"getting_started/getting_started/#121-without-using-poetry-for-dependency-management","title":"1.2.1. Without using Poetry for dependency management","text":"<ol> <li> <p>Create an isolated environment (recommended for avoiding dependency conflicts):  </p> <pre><code>conda create -n audit_env python=3.10\nconda activate audit_env\n</code></pre> </li> <li> <p>Clone the repository:  </p> <pre><code>git clone git@github.com:caumente/AUDIT.git\ncd AUDIT\n</code></pre> </li> <li> <p>Install the required dependencies: </p> </li> </ol>"},{"location":"getting_started/getting_started/#pip-install-r-requirementstxt","title":"<pre><code>pip install -r requirements.txt\n</code></pre>","text":""},{"location":"getting_started/getting_started/#122-using-poetry-for-dependency-management","title":"1.2.2. Using Poetry for dependency management","text":"<p>Poetry is a dependency manager that simplifies library management and environment creation. Follow these steps:</p> <ol> <li> <p>Ensure Poetry is installed in your environment.</p> </li> <li> <p>Clone AUDIT's repository:  </p> </li> </ol> <pre><code>git clone git@github.com:caumente/AUDIT.git\ncd AUDIT\n</code></pre> <ol> <li>Install the dependencies:  </li> </ol> <pre><code>poetry install\n</code></pre> <ol> <li>Activate the virtual environment:  </li> </ol> <pre><code>poetry shell\n</code></pre>"},{"location":"getting_started/getting_started/#13-conclusion","title":"1.3. Conclusion","text":"<p>The authors recommend following the developer option, as it provides greater flexibility for users once they are  familiar with the use of AUDIT. Additionally, our repository includes example cases, a specific project structure,  and certain outputs. This simplifies interaction with the AUDIT app right from the start of its use.</p>"},{"location":"getting_started/getting_started/#2-project-structure-and-guidelines","title":"2. Project structure and guidelines","text":"<p>AUDIT supports various project structures, but it is highly recommended to adhere to the default structure for clarity,  ease of use, and ensuring correct functionality. This structure is intuitive, modular, and designed to facilitate working with datasets, configurations, outputs, and logs.</p> <p>We will guide you through the recommended structure and explain the purpose of each directory in the context of the  AUDIT Python library.</p>"},{"location":"getting_started/getting_started/#21datasets-directory-datasets","title":"2.1.Datasets directory (<code>/datasets</code>)","text":"<p>The <code>datasets</code> directory is the cornerstone of the project, containing all datasets used for both training and testing  models. Each dataset is organized into subdirectories, making it straightforward to store images, ground truth  segmentations, and predictions generated by different models.</p> <pre><code>your_project/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 dataset_1/\n\u2502   \u2502   \u251c\u2500\u2500 dataset_1_images/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_t1.nii.gz\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_t1ce.nii.gz\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_t2.nii.gz\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_flair.nii.gz\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_seg.nii.gz\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_2/\n\u2502   \u2502   \u2502   ......\n\u2502   \u2502   \u251c\u2500\u2500 dataset_1_seg/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_1/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_pred.nii.gz\n\u2502   \u2502   \u2502   .....\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_2/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_1_case_1_pred.nii.gz\n\u2502   \u2502   \u2502   .....\n\u2502   \u251c\u2500\u2500 dataset_2/\n...\n</code></pre> <p>Explanation of Components</p> <ul> <li><code>dataset_1/</code>: Each dataset is stored in its own directory (e.g., <code>dataset_1</code>, <code>dataset_2</code>, etc.).</li> <li><code>dataset_1_images/</code>: This folder contains all image data for the dataset. Subfolders represent individual cases                             (e.g., <code>dataset_1_case_1</code>), and each case includes its respective sequences, such as T1,                             T1ce, T2, FLAIR, and segmentation (ground truth).</li> <li><code>dataset_1_seg/</code>: This directory stores predictions made on the dataset by different models. Each model has its                          own subdirectory (e.g., <code>model_1</code>, <code>model_2</code>), and within each, predictions for every case are                          organized similarly to the ground truth images. The extension __pred_ is a reserved word in                          AUDIT library for model predictions.</li> </ul> <p>This design supports multi-center and multi-model comparisons by storing predictions from several models alongside the  original data. Please, find an example of this folder in the following link:  dummy dataset folder.</p>"},{"location":"getting_started/getting_started/#22-configuration-directory-config","title":"2.2. Configuration directory (<code>config/</code>)","text":"<p>The <code>config</code> directory houses all configuration files required to run feature and metric extraction as well as the web app. </p> <pre><code>your_project/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 feature_extraction.yaml\n\u2502   \u251c\u2500\u2500 metric_extraction.yaml\n\u2502   \u251c\u2500\u2500 app.yaml\n</code></pre> <p>Each of the config files are carefully described in section 3. Configuration. Please, find an example of each of the  config files in the following link: dummy config files.</p>"},{"location":"getting_started/getting_started/#23-outputs-directory-outputs","title":"2.3. Outputs directory (<code>outputs/</code>)","text":"<p>The <code>outputs</code> directory is where all results generated by the project are stored. This includes extracted features and  metrics calculated during model evaluation.</p> <pre><code>your_project/\n\u251c\u2500\u2500 outputs/\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u251c\u2500\u2500 metrics/\n</code></pre> <p>Explanation of Components - <code>features/</code>: Stores the features extracted from the dataset during processing. These could include case-specific features or summary statistics. - <code>metrics/</code>: Contains evaluation results, such as segmentation metrics (e.g., Dice scores) or model performance comparisons.</p>"},{"location":"getting_started/getting_started/#24-logs-directory-logs","title":"2.4. Logs Directory (<code>logs/</code>)","text":"<p>The <code>logs</code> directory contains logs generated during project execution. These logs are critical for debugging, monitoring progress, and keeping a historical record of runs.</p> <pre><code>your_project/\n\u251c\u2500\u2500 logs/\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u251c\u2500\u2500 metric/\n</code></pre> <p>Explanation of Components - <code>features/</code>: Logs related to feature extraction, such as execution times, errors encountered during processing, or debugging information. - <code>metric/</code>: Logs generated during metric evaluation, including potential issues with predictions.</p>"},{"location":"getting_started/getting_started/#25-conclusion","title":"2.5. Conclusion","text":"<p>By following this recommended structure, you can ensure seamless integration with the AUDIT Python library, enabling  efficient data management, model evaluation, and reproducibility. This structure is both modular and extensible, making  it adaptable to projects of varying complexity.</p> <p>In addition, to configure this project structure, especially the <code>datasets</code> directory, AUDIT provides methods for the  organization and standardization of datasets.</p> <p>The <code>outputs/</code> and <code>logs/</code> directories will be automatically created the first time AUDIT is executed, so there is no  need to create them explicitly.</p> <p>Users should note that, as mentioned in Section 1, using the developer option does not require creating the project  structure manually, as it is already implemented by default.</p>"},{"location":"getting_started/getting_started/#3-configuration","title":"3. Configuration","text":"<p>AUDIT uses configuration files to define paths, settings, and parameters. These files are located in  the <code>audit/configs/</code> directory. However, if the user is not following the developer configuration both files must be created.</p> <ul> <li>Feature extraction: Configure MRI features and datasets in <code>feature_extraction.yml</code>.</li> <li>Metric extraction: Define evaluation metrics and paths in <code>metric_extraction.yml</code>.</li> <li>App settings: Customize web app options in <code>app.yml</code>.</li> </ul> <p>Make sure to adjust other paths and settings according to your environment. Please, find an example of each of the  config files in the following link: dummy config files.</p>"},{"location":"getting_started/getting_started/#31-example-of-feature-extraction-config-file","title":"3.1. Example of feature extraction config file","text":"<p>This configuration file is used to define the settings for feature extraction in the AUDIT library. Each key and its usage is explained below:</p> <ul> <li><code>data_paths</code>: Specifies the paths to the directories containing MRI datasets. It is a dictionary where each key                  represents a dataset name (e.g., BraTS, UCSF), and the value is the file path to the dataset folder.</li> <li><code>labels</code>: Maps region names (e.g., tumor labels) to their numeric values. This mapping is used to identify regions in              segmentation maps. <ul> <li>BKG: Background (non-tumor regions). </li> <li>EDE: Edema. </li> <li>ENH: Enhancing tumor. </li> <li>NEC: Necrotic tumor tissue.</li> </ul> </li> <li><code>features</code>: Lists the types of features to be extracted from the MRI datasets. Each key is a feature type, and its                value (true or false) enables or disables that feature. Key Options:<ul> <li>statistical: Extract basic statistical properties (e.g., mean, variance) of the MRI intensity values.</li> <li>texture: Compute texture-based features (e.g., entropy, contrast).</li> <li>spatial: Analyze spatial properties (e.g., brain location, spatial resolution).</li> <li>tumor: Extract tumor-specific features (e.g., tumor volume, tumor location).</li> </ul> </li> <li><code>longitudinal</code>: Configures settings for longitudinal studies, allowing analysis of changes in subjects over time. Each                  dataset can have a unique configuration.<ul> <li>pattern: A delimiter (e.g., _, -, or /) used to split filenames.</li> <li>longitudinal_id: The position (0-based index) in the split filename where the subject ID is located.</li> <li>time_point: The position (0-based index) in the split filename indicating the time point (e.g., pre-treatment, post-treatment).</li> </ul> </li> <li><code>output_path</code>: Specifies the directory where extracted features will be saved.</li> </ul> <p>Below is a complete configuration file, demonstrating how these keys are used together:</p> <pre><code># Paths to all your datasets\ndata_paths:\n  BraTS: '/home/user/AUDIT/datasets/BraTS/BraTS_images'\n  UCSF: '/home/user/AUDIT/datasets/UCSF/UCSF_images'\n\n# Mapping of labels to their numeric values\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# List of features to extract\nfeatures:\n  statistical: true\n  texture: false\n  spatial: true\n  tumor: true\n\n# Longitudinal settings (if longitudinal data is available)\nlongitudinal:\n  UCSF:\n    pattern: \"_\"            # Pattern used for splitting filename\n    longitudinal_id: 1      # Index position for the subject ID after splitting the filename\n    time_point: 2           # Index position for the time point after splitting the filename\n\n\n# Path where extracted features will be saved\noutput_path: '/home/user/AUDIT/outputs/features'\n</code></pre>"},{"location":"getting_started/getting_started/#32-example-of-metric-extraction-config-file","title":"3.2. Example of metric extraction config file","text":"<p>This configuration file is used to define the settings for feature extraction in the AUDIT library. Each key and its  usage is explained below:</p> <ul> <li><code>data_paths</code>: Specifies the paths to the directories containing MRI datasets. It is a dictionary where each key                  represents a dataset name (e.g., BraTS, UCSF), and the value is the file path to the dataset folder.</li> <li><code>model_predictions_paths</code>: Specifies the paths to the directories containing model predictions. It is a dictionary where each key                  represents a model name (e.g., nn-UNet, SegResNet), and the value is the corresponding path.</li> <li><code>labels</code>: Maps region names (e.g., tumor labels) to their numeric values. This mapping is used to identify regions in              segmentation maps. <ul> <li>BKG: Background (non-tumor regions). </li> <li>EDE: Edema. </li> <li>ENH: Enhancing tumor. </li> <li>NEC: Necrotic tumor tissue.</li> </ul> </li> <li><code>metrics</code>: Lists the metrics to be computed to evaluate the model predictions. Each key represents a metric, and its               value (true or false) enables or disables the computation of that metric. Available options include: dice,              jacc, accu, prec, sens, spec, haus, size.</li> <li><code>package</code>: Specifies the library used to compute the metrics. AUDIT will be used by default.</li> <li><code>calculate_stats</code>: A flag that determines whether additional statistical information (e.g., mean, variance) is                       computed for the evaluation. Only available is using pymia library.</li> <li><code>output_path</code>: Specifies the directory where the computed metrics will be saved after evaluation.</li> <li><code>filename</code>: Defines the filename prefix for saving the output metrics.</li> </ul> <p>Below is a complete configuration file, demonstrating how these keys are used together:</p> <pre><code># Path to the raw dataset\ndata_path: '/home/user/AUDIT/datasets/BraTS/BraTS_images'\n\n# Paths to model predictions\nmodel_predictions_paths:\n  nnUnet: '/home/user/AUDIT/datasets/BraTS/BraTS_seg/nnUnet'\n\n# Mapping of labels to their numeric values\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# List of metrics to compute\nmetrics:\n  dice: true\n  jacc: true\n  accu: true\n  prec: true\n  sens: true\n  spec: true\n  haus: true\n  size: true\n\n# Library used for computing all the metrics\npackage: custom\ncalculate_stats: false\n\n# Path where output metrics will be saved\noutput_path: '/home/user/AUDIT/outputs/metrics'\n\n# Filename for the extracted information\nfilename: 'BraTS'\n</code></pre>"},{"location":"getting_started/getting_started/#33-example-of-app-web-config-file","title":"3.3. Example of APP web config file","text":"<p>This configuration file is used to define the settings for organizing datasets, feature extractions, and model  predictions in the AUDIT library. Each key is explained below:</p> <ul> <li><code>labels</code>: Maps region names (e.g., tumor labels) to their numeric values. This mapping is used to identify regions in              segmentation maps. <ul> <li>BKG: Background (non-tumor regions). </li> <li>EDE: Edema. </li> <li>ENH: Enhancing tumor. </li> <li>NEC: Necrotic tumor tissue.</li> </ul> </li> <li><code>features_path</code>: Defines the root path where the feature extraction results are saved.</li> <li><code>metrics_path</code>: Defines the root path where the metric extraction results are saved.</li> <li><code>raw_datasets</code>: Specifies paths to directories containing the raw MRI datasets. Each key represents a dataset name                    (e.g., BraTS, UCSF), and the value is the file path to the respective dataset folder.</li> <li><code>features</code>: Specifies paths to CSV files where the extracted feature information is saved for each dataset. Each key                represents a dataset name, and the value is the file path to the corresponding feature extraction CSV file.</li> <li><code>metrics</code>: Specifies paths to CSV files where metric extraction information is saved for each dataset. Similar to the              features section, each key represents a dataset name, and the value is the path to the corresponding metrics CSV file.</li> <li><code>predictions</code>: Specifies the paths for model predictions for different datasets. Each dataset name                   (e.g., BraTS_SSA) maps to a dictionary containing model names (e.g., nnUnet, SegResNet) as keys,                   and the file paths to their respective segmentation predictions as values.</li> </ul> <pre><code># Mapping of labels to their numeric values\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# Root path for datasets, features extracted, and metrics extracted\ndatasets_path: '/home/user/AUDIT/datasets'\nfeatures_path: '/home/user/AUDIT/outputs/features'\nmetrics_path: '/home/user/AUDIT/outputs/metrics'\n\n# Paths for raw datasets\nraw_datasets:\n  BraTS: \"${datasets_path}/BraTS/BraTS_images\"\n  BraTS_SSA: \"${datasets_path}/BraTS_SSA/BraTS_SSA_images\"\n  UCSF: \"${datasets_path}/UCSF/UCSF_images\"\n\n# Paths for feature extraction CSV files\nfeatures:\n  BraTS: \"${features_path}/extracted_information_BraTS.csv\"\n  BraTS_SSA: \"${features_path}/extracted_information_BraTS_SSA.csv\"\n  UCSF: \"${features_path}/extracted_information_UCSF.csv\"\n\n# Paths for metric extraction CSV files\nmetrics:\n  BraTS_SSA: \"${metrics_path}/extracted_information_BraTS.csv\"\n  UCSF: \"${metrics_path}/extracted_information_UCSF.csv\"\n\n# Paths for models predictions\npredictions:\n  BraTS_SSA:\n    nnUnet: \"${datasets_path}/BraTS_SSA/BraTS_SSA_seg/nnUnet\"\n    SegResNet: \"${datasets_path}/BraTS_SSA/BraTS_SSA_seg/SegResNet\"\n  UCSF:\n    SegResNet: \"${datasets_path}/UCSF/UCSF_seg/SegResNet\"\n</code></pre>"},{"location":"getting_started/getting_started/#4-run-audit-backend","title":"4. Run AUDIT Backend","text":"<p>The backend of AUDIT is responsible for calculating the metrics specified in configuration files and extracting features  from magnetic resonance imaging (MRI) data. Depending on the installation method\u2014either via the AUDIT repository  (for developers) or through pip (for standard usage)\u2014the library is designed to execute these processes via  command-line commands.</p>"},{"location":"getting_started/getting_started/#41-for-standard-users","title":"4.1. For standard users","text":"<pre><code>auditapp feature-extraction --config path/to/your/feature_extraction/config/file.yaml\nauditapp metric-extraction --config path/to/your/metric_extraction/config/file.yaml\n</code></pre>"},{"location":"getting_started/getting_started/#42-for-developers","title":"4.2. For developers","text":"<p>If you are using the developer mode of AUDIT (installed via the repository), you can directly run the feature extraction  and metric extraction modules as follows:</p> <pre><code>python src/audit/feature_extraction.py --config path/to/your/feature_extraction/config/file.yml\npython src/audit/metric_extraction.py --config path/to/your/metric_extraction/config/file.yml\n</code></pre> <p>In developer mode, specifying the --config parameter is optional. Instead, you can edit the default configuration files provided by the library to suit your needs. These files are located at AUDIT/src/audit/configs folder. Simply modify the  configuration files (feature_extraction.yml and metric_extraction.yml) to match your requirements before running the commands. Additionally, AUDIT provides config files that allows users to run de APP by default. Some of the functionalies are then limited.</p> <p>Logs and output files will be saved in the directories specified in the configuration files (default are the <code>logs/</code> <code>outputs/</code> folders).</p>"},{"location":"getting_started/getting_started/#5-run-audit-app","title":"5. Run AUDIT App","text":"<p>The AUDIT web app provides an interactive interface for exploring your data and visualizing metrics. Start the app with:</p>"},{"location":"getting_started/getting_started/#51-for-standard-users","title":"5.1. For standard users","text":"<pre><code>auditapp run-app --config path/to/your/app/config/file.yml\n</code></pre>"},{"location":"getting_started/getting_started/#52-for-developers","title":"5.2. For developers","text":"<pre><code>python src/audit/app/launcher.py --config path/to/your/app/config/file.yml\n</code></pre> <p>In developer mode, specifying the --config parameter is optional. Instead, you can edit the default configuration file provided by the library to suit your needs. These file is located at AUDIT/src/audit/configs folder. Simply modify the  configuration file (app.yml) to match your requirements before running the commands.</p> <p>This will open the app in your default web browser  at http://localhost:8501/. Use the dashboards to:</p> <ul> <li>Explore univariate and multivariate data distributions.</li> <li>Compare model performance across datasets.</li> <li>Analyze trends in longitudinal data.</li> </ul>"},{"location":"getting_started/getting_started/#6-additional-tips","title":"6. Additional Tips","text":"<p>For ITK-Snap integration, ensure it is installed and configured correctly. Use the logs folder to monitor execution details for debugging.</p> <p>You're all set to start using AUDIT! Dive into your MRI data, evaluate AI models, and gain deeper insights with the help of AUDIT\u2019s powerful tools. For further details, check out the other sections of the documentation.</p>"},{"location":"getting_started/installation/","title":"Installation Guide","text":"<p>Welcome to AUDIT! Before installing on your local computer, we recommend exploring the publicly deployed app on Streamlit Cloud:</p> <p>https://auditapp.streamlit.app/</p> <p>This allows you to try AUDIT without any setup. When you're ready to analyze your own datasets or use advanced features, follow the steps below to install AUDIT locally.</p>"},{"location":"getting_started/installation/#installation-via-repository-recommended","title":"\ud83c\udfc6 Installation via Repository (Recommended)","text":"<p>For full flexibility, access to the latest updates, and example files, we recommend installing AUDIT by cloning the official repository. This method is suitable for most users, including those who want to customize or contribute to the project.</p> <p>Choose one of the following options:</p>"},{"location":"getting_started/installation/#option-1-using-conda","title":"Option 1: Using Conda","text":"<ol> <li> <p>Create an isolated environment (recommended to avoid dependency conflicts):</p> <pre><code>conda create -n audit_env python=3.10\nconda activate audit_env\n</code></pre> </li> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/caumente/AUDIT.git\ncd AUDIT\n</code></pre> </li> <li> <p>Install dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> </ol>"},{"location":"getting_started/installation/#option-2-using-poetry","title":"Option 2: Using Poetry","text":"<p>Poetry is a modern dependency manager that simplifies library management and environment creation.</p> <ol> <li> <p>Install Poetry (if not already installed):</p> <p>Poetry installation guide</p> </li> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/caumente/AUDIT.git\ncd AUDIT\n</code></pre> </li> <li> <p>Install dependencies:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Activate the virtual environment:</p> <pre><code>poetry shell\n</code></pre> </li> </ol>"},{"location":"getting_started/installation/#installation-via-pypi","title":"\ud83d\udce6 Installation via PyPI","text":"<p>If you want a quick way to use AUDIT for data analysis and evaluation, you can install the latest stable version from PyPI:</p> <pre><code>pip install auditapp\n</code></pre> <p>This method is ideal for users who do not need to modify the source code. You will still need to set up configuration files and the recommended project structure.</p> <p>Tip</p> <p>If you encounter issues with permissions, try running <code>pip install --user auditapp</code>.</p>"},{"location":"getting_started/installation/#whats-next","title":"\u2705 What\u2019s Next?","text":"<ul> <li>The repository installation provides greater flexibility and access to example cases, project structure templates, and outputs.</li> <li>Example configuration files and datasets are included in the repository to help you get started quickly.</li> <li>For details on project structure, configuration, and running AUDIT, see the other sections in the documentation.</li> </ul> <p>Info</p> <p>The recommended workflow is to install AUDIT via the repository for full functionality and easier customization.</p>"},{"location":"getting_started/installation/#troubleshooting-tips","title":"\ud83e\udde9 Troubleshooting &amp; Tips","text":"<ul> <li>If you have issues with Python versions, ensure you are using Python 3.10 or higher.</li> <li>For help with dependencies, check the requirements.txt or pyproject.toml files.</li> <li>For more information, visit the AUDIT GitHub repository.</li> </ul> <p>You're ready to start using AUDIT! For further guidance, explore the rest of the documentation.</p>"},{"location":"getting_started/project_structure/","title":"Project Structure Guide","text":"<p>AUDIT supports flexible project organization, but we recommend following the default structure for clarity, reproducibility, and ease of use. This structure is modular and designed to help you manage datasets, configurations, outputs, and logs efficiently.</p>"},{"location":"getting_started/project_structure/#recommended-directory-layout","title":"\ud83d\udcc1 Recommended Directory Layout","text":"<pre><code>your_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u251c\u2500\u2500 logs/\n</code></pre>"},{"location":"getting_started/project_structure/#datasets-directory-datasets","title":"Datasets Directory (<code>datasets/</code>)","text":"<p>Central location for all datasets used in training and testing. Organize each dataset in its own subdirectory, with images, ground truth segmentations, and model predictions clearly separated.</p> <p>Info</p> <p>The reserved word <code>__pred_</code> is used in AUDIT for model predictions.</p> <p>Example: <pre><code>datasets/\n\u251c\u2500\u2500 dataset_1/\n\u2502   \u251c\u2500\u2500 dataset_1_images/\n\u2502   \u251c\u2500\u2500 dataset_1_seg/\n\u2502   \u2502   \u251c\u2500\u2500 model_1/\n\u2502   \u2502   \u251c\u2500\u2500 model_2/\n\u251c\u2500\u2500 dataset_2/\n</code></pre></p>"},{"location":"getting_started/project_structure/#configuration-directory-config","title":"Configuration Directory (<code>config/</code>)","text":"<p>Store all configuration files needed for feature extraction, metric evaluation, and app settings.</p> <p>Example: <pre><code>config/\n\u251c\u2500\u2500 feature_extraction.yaml\n\u251c\u2500\u2500 metric_extraction.yaml\n\u251c\u2500\u2500 app.yaml\n</code></pre></p>"},{"location":"getting_started/project_structure/#outputs-directory-outputs","title":"Outputs Directory (<code>outputs/</code>)","text":"<p>All results generated by AUDIT are saved here, including extracted features and evaluation metrics.</p> <p>Example: <pre><code>outputs/\n\u251c\u2500\u2500 features/\n\u251c\u2500\u2500 metrics/\n</code></pre></p>"},{"location":"getting_started/project_structure/#logs-directory-logs","title":"Logs Directory (<code>logs/</code>)","text":"<p>Contains logs for debugging, monitoring, and record-keeping.</p> <p>Example: <pre><code>logs/\n\u251c\u2500\u2500 features/\n\u251c\u2500\u2500 metric/\n</code></pre></p>"},{"location":"getting_started/project_structure/#getting-started","title":"\ud83c\udfc1 Getting Started","text":"<ul> <li>The recommended structure is automatically created when you run AUDIT for the first time.</li> <li>Example datasets and configuration files are available in the repository.</li> <li>For details on configuration and running AUDIT, see the other sections in the documentation.</li> </ul> <p>Tip</p> <p>Using the default structure ensures seamless integration and reproducibility.</p>"},{"location":"getting_started/run_app/","title":"Running AUDIT Web App","text":"<p>AUDIT provides an interactive web app for data exploration and visualization. We recommend running the app from the cloned repository for full access to features and customization.</p>"},{"location":"getting_started/run_app/#run-from-repository-recommended","title":"\u25b6\ufe0f Run from Repository (Recommended)","text":"<p>Start the app using:</p> <pre><code>python src/audit/app/launcher.py --config path/to/your/app.yaml\n</code></pre> <p>Info</p> <p>The <code>--config</code> parameter is optional if you edit the default configuration file in <code>src/audit/configs/app.yaml</code>.</p>"},{"location":"getting_started/run_app/#run-from-pypi-installation","title":"\u25b6\ufe0f Run from PyPI Installation","text":"<p>If installed via PyPI, use:</p> <pre><code>auditapp run-app --config path/to/your/app.yaml\n</code></pre>"},{"location":"getting_started/run_app/#accessing-the-app","title":"\ud83c\udf10 Accessing the App","text":"<ul> <li>The app will open in your default web browser at http://localhost:8501/.</li> <li>Use the dashboards to explore data distributions, compare model performance, and analyze trends.</li> </ul>"},{"location":"getting_started/run_app/#getting-started","title":"\ud83c\udfc1 Getting Started","text":"<ul> <li>Example configuration files are available in the repository.</li> <li>For more details, see the configuration and project structure guides.</li> </ul> <p>Tip</p> <p>For full functionality, use the repository installation and default configuration files.</p>"},{"location":"getting_started/run_backend/","title":"Running AUDIT Backend","text":"<p>AUDIT's backend is responsible for extracting features from medical images and evaluating segmentation model performance using metrics. These steps are essential for understanding your data, benchmarking models, and generating results for further analysis and visualization.</p> <p>Feature extraction computes quantitative descriptors (statistical, texture, spatial, tumor features) from your datasets, enabling detailed cohort analysis and model comparison. Metric extraction evaluates segmentation predictions against ground truth, providing objective measures of model accuracy and reliability.</p> <p>All extraction and evaluation methods are documented in the API reference.</p>"},{"location":"getting_started/run_backend/#run-from-repository-recommended","title":"\u25b6\ufe0f Run from Repository (Recommended)","text":"<p>Use the following commands to run feature extraction and metric evaluation modules:</p> <pre><code>python src/audit/feature_extraction.py --config path/to/your/feature_extraction.yaml\npython src/audit/metric_extraction.py --config path/to/your/metric_extraction.yaml\n</code></pre> <p>Info</p> <p>The <code>--config</code> parameter is optional if you edit the default configuration files in <code>src/audit/configs/</code>.</p>"},{"location":"getting_started/run_backend/#run-from-pypi-installation","title":"\u25b6\ufe0f Run from PyPI Installation","text":"<p>If installed via PyPI, use the command-line interface:</p> <pre><code>auditapp feature-extraction --config path/to/your/feature_extraction.yaml\nauditapp metric-extraction --config path/to/your/metric_extraction.yaml\n</code></pre>"},{"location":"getting_started/run_backend/#getting-started","title":"\ud83c\udfc1 Getting Started","text":"<ul> <li>Output and log files are saved in the directories specified in your configuration files.</li> <li>Example configuration files are available in the repository.</li> <li>For more details, see the configuration and project structure guides.</li> <li>For technical details, see the API reference documentation.</li> </ul> <p>Tip</p> <p>For best results, use the repository installation and default configuration files.</p>"},{"location":"tutorials/postprocessing_segmentations/","title":"Post-processing model predictions with AUDIT","text":"<p>In the <code>Preprocessing dataset</code> tutorial, we prepared and adapted the LUMIERE dataset to the file structure required to  work with AUDIT. In this tutorial, we will see how, in addition to standardizing the organization of your project's  files, it will be beneficial for your datasets to be standardized and follow the same naming conventions. This way,  it'll be much easier to compare them with each other, evaluate models, and more.</p> <p>Let's assume we've selected a pre-trained model from an open-source repository to generate predictions on your  dataset. It is likely that the model wasn\u2019t trained to predict the same labels that our segmentations use. In such  cases, we\u2019ll need to apply some post-processing to properly evaluate these predictions using AUDIT.</p> <p>Fortunately, AUDIT provides users with tools to perform this post-processing and adapt the predictions as needed. </p> <p>By the end of this tutorial, you will: - Organize predictions into a structured directory. - Rename files and labels to align with AUDIT's requirements. - Standardize labels for both ground truth and predictions, ensuring compatibility.</p> <p>Let\u2019s get started!</p>"},{"location":"tutorials/postprocessing_segmentations/#1-load-functions","title":"1. Load functions","text":"<p>This tutorial uses some utility functions from the <code>audit.utils.commons.file_manager</code> and <code>audit.utils.sequences.sequences</code>  modules to manipulate files and sequences.</p> <pre><code>from audit.utils.sequences.sequences import(\n    load_nii_by_subject_id,\n    iterative_labels_replacement,\n    count_labels\n)\n\nfrom audit.utils.commons.file_manager import (\n    list_dirs,\n    list_files,\n    organize_files_into_folders,\n    add_suffix_to_files\n)\n</code></pre>"},{"location":"tutorials/postprocessing_segmentations/#2-understanding-the-data","title":"2. Understanding the data","text":"<p>Now, let's suppose we have a model that was trained on a brain MRI dataset that we don't know in advance  (in this case, it was BraTS2020). We use this model to generate a series of predictions on our own dataset. In our  scenario, we want to run inference on LUMIERE.</p> <p>After running the inference, we store the predictions in the following directory:</p> <pre><code>root_path_sequences = \"./datasets/LUMIERE/LUMIERE_images/\"\nroot_path_predictions = \"./datasets/LUMIERE/LUMIERE_seg/nnUnet\"\n</code></pre> <p>Let\u2019s check the contents of this directory:</p> <pre><code>print(list_dirs(root_path_predictions))\nprint(list_files(root_path_predictions)[:6])\n</code></pre> <pre><code>[]\n['Patient-001-week-044.nii.gz', 'Patient-001-week-056.nii.gz', 'Patient-002-week-000.nii.gz', 'Patient-002-week-003.nii.gz', 'Patient-002-week-021.nii.gz', 'Patient-002-week-037.nii.gz']\n</code></pre>"},{"location":"tutorials/postprocessing_segmentations/#3-organize-folder","title":"3. Organize folder","text":"<p>AUDIT is designed to work with multiple models and datasets, but it requires a specific organization of the data for  processing. Currently, we have a single directory \".datasets/LUMIERE/LUMIERE_seg/\" </p> <p>This directory contains the predictions generated by our model for the LUMIERE dataset. However, AUDIT requires that  each segmentation be contained within a folder named after the subject ID. Since users may not structure their code to  produce output in this specific format\u2014like the pre-trained model we are using\u2014AUDIT provides functions to facilitate  the organization of files.</p> <p>In this case, we can use the organize_files_into_folders function to create the necessary data structure without  needing to perform complex manipulations.</p> <pre><code>organize_files_into_folders(\n    root_dir=root_path_predictions,\n    extension='.nii.gz',\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would move: ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-058-week-002.nii.gz -&gt; ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-058-week-002/Patient-058-week-002.nii.gz\n[SAFE MODE] Would move: ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-018-week-063.nii.gz -&gt; ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-018-week-063/Patient-018-week-063.nii.gz\n...\n</code></pre> <p>Let's turn safe_mode into False to apply the changes.</p> <pre><code>organize_files_into_folders(\n    root_dir=root_path_predictions,\n    extension='.nii.gz',\n    safe_mode=False\n)\n</code></pre> <p>The structure of the directory containing the predictions has changed. Instead of having all the files  scattered within the directory, a separate folder has been created for each subject, containing their respective  predictions. </p> <p>This organized structure makes it much easier to manage and process the predictions for each subject in the dataset.  Let\u2019s take a moment to verify this new organization:</p> <pre><code>|--Patient-000-week-000\n|---- Patient-000-week-000.nii.gz\n|--Patient-000-week-001\n|---- Patient-000-week-001.nii.gz\n...\n</code></pre>"},{"location":"tutorials/postprocessing_segmentations/#4-add-extension-name","title":"4. Add extension name","text":"<p>Another important aspect of AUDIT is that it uses file extensions to distinguish between sequences (e.g., _t1, _t2,  _t1ce, _flair), segmentations (_seg), and predictions (_pred`). However, if we check the names of the files  generated after inference, they did not contain any specific nomenclature.</p> <p>To simplify the task for users and eliminate the need to modify their pipelines to ensure compatibility with AUDIT,  the library provides methods to quickly and easily adapt the file names. </p> <p>Let\u2019s explore how to rename the files so they align with AUDIT\u2019s requirements. Taking advantage of the function  add_suffix_to_files, we can add the extension required by AUDIT.</p> <pre><code>add_suffix_to_files(\n    root_dir=root_path_predictions,\n    suffix='_pred', \n    ext='.nii.gz', \n    safe_mode=False\n)\n</code></pre> <p>Now, all the files within the root_path contain the _pred extension.</p> <pre><code>list_files(os.path.join(root_path_preditions, 'Patient-012-week-016'))\n</code></pre> <pre><code>[Patient-012-week-016_pred.nii.gz]\n</code></pre>"},{"location":"tutorials/postprocessing_segmentations/#5-label-replacement-ground-truth","title":"5. Label replacement (Ground Truth)","text":"<p>We need to verify which labels were generated after the inference. As mentioned, the model used  was pre-trained on the BraTS2020 dataset to predict the labels ENH, NEC, and EDE. However, these may not align with the  labels used in the LUMIERE dataset. In fact, they are different and will need to be adjusted accordingly.</p> <pre><code>subject = \"Patient-006-week-000\"\n\nseg = load_nii_by_subject_id(\n    root_dir=root_path_sequences,\n    subject_id=subject,\n    seq=\"_seg\",\n    as_array=True\n)\n\ncount_labels(seg)\n</code></pre> <pre><code>{0: 6968420, 1: 31427, 2: 44342, 3: 176843}\n</code></pre> <p>The BraTS and UCSF datasets provided by AUDIT use the following mapping (after preprocessing that we performed earlier).</p> <ul> <li>BKG: 0</li> <li>EDE: 3</li> <li>ENH: 1</li> <li>NEC: 2</li> </ul> <p>In contrast, LUMIERE uses:</p> <ul> <li>BKG: 0</li> <li>EDE: 3</li> <li>ENH: 2</li> <li>NEC: 1</li> </ul> <p>To resolve this mismatch, we will use the iterative_labels_replacement function. This function takes the old mapping  and the new mapping as parameters and replaces the labels accordingly.</p> <pre><code>original_labels = [0, 1, 2, 3]  # current mapping used by LUMIERE (BKG: 0 NEC: 1 ENH: 2 EDE: 3)\nnew_labels = [0, 2, 1, 3]  # new mapping we want LUMIERE to use (BKG: 0 ENH: 1 NEC: 2 EDE: 3)\n\niterative_labels_replacement(\n    root_dir=root_path_sequences,\n    original_labels=original_labels,\n    new_labels=new_labels,\n    ext=\"_seg\"  # only files whose extension is '_seg' will be relabeled\n)\n</code></pre> <p>Once we apply this function, we can confirm that the labels are now correct.</p> <pre><code>2025-01-20 13:52:47.302 | INFO     | src.audit.utils.sequences.sequences:iterative_labels_replacement:216 - Iterative label replacement completed: 513 files processed, 2052 files skipped.\n</code></pre>"},{"location":"tutorials/postprocessing_segmentations/#6-label-replacement-predictions","title":"6. Label replacement (Predictions)","text":"<p>The model we used to generate the predictions was trained with the intention that tumor regions be labeled as follows:</p> <ul> <li>BKG: 0</li> <li>EDE: 2</li> <li>ENH: 4</li> <li>NEC: 1</li> </ul> <p>As can be observed in the prediction generated for the same subject we have been working with.</p> <pre><code>pred = load_nii_by_subject_id(\n    root_dir=root_path_predictions,\n    subject_id=subject,\n    seq=\"_pred\",\n    as_array=True\n)\n\ncount_labels(pred)\n</code></pre> <pre><code>{0.0: 6959023, 1.0: 33811, 2.0: 192371, 4.0: 35827}\n</code></pre> <p>Therefore, we will once again need to rename the labels to match the ground truth.</p> <pre><code>original_labels = [0, 1, 2, 4]  # current mapping used in our predictions  (BKG: 0 NEC: 1 EDE: 2 ENH: 4)\nnew_labels = [0, 2, 3, 1]  # new mapping we want LUMIERE to use (BKG: 0 ENH: 1 NEC: 2 EDE: 3)\n\n\niterative_labels_replacement(\n    root_dir=root_path_predictions,\n    original_labels=original_labels,\n    new_labels=new_labels,\n    ext=\"_pred\"  # only files whose extension is '_seg' will be relabeled\n)\n</code></pre> <pre><code>2025-01-20 14:34:52.473 | INFO     | src.audit.utils.sequences.sequences:iterative_labels_replacement:216 - Iterative label replacement completed: 507 files processed, 0 files skipped.\n</code></pre> <p>Finally, we have prepared the dataset to run the metric_extraction.py module and start using AUDIT effectively.</p> <pre><code>seg = load_nii_by_subject_id(\n    root_dir=root_path_sequences,\n    subject_id=subject,\n    seq=\"_seg\",\n    as_array=True\n)\npred = load_nii_by_subject_id(\n    root_dir=root_path_predictions,\n    subject_id=subject,\n    seq=\"_pred\",\n    as_array=True\n)\n\n# now they use the same labeling system\nprint(count_labels(seg))\nprint(count_labels(pred))\n</code></pre> <pre><code>{0: 6968420, 1: 31427, 2: 44342, 3: 176843}\n{0.0: 6959023, 1.0: 35827, 2.0: 33811, 3.0: 192371}\n</code></pre>"},{"location":"tutorials/preprocessing/","title":"Dataset preprocessing with AUDIT","text":"<p>In this tutorial, we will walk you through the preprocessing steps of the LUMIERE dataset using the AUDIT  library. Our goal is to demonstrate how to clean, organize, and standardize the dataset to prepare it for further  analysis. We'll cover key operations such as file and folder cleaning, reorganization, and renaming, all while  explaining each step in detail.</p> <p>Let's get started!</p>"},{"location":"tutorials/preprocessing/#0-import-librarys-functions","title":"0. Import Library's Functions","text":"<p>Before we dive into the dataset preprocessing, we first need to import the necessary functions from the AUDIT  library. These utility functions will help us clean and reorganize the dataset. Importing these functions at the  start will give us all the tools we need for the upcoming steps.</p> <p>If you have already installed AUDIT a library, then you'll be able to import the main functions.</p> <pre><code>import os\nfrom audit.utils.commons.file_manager import (\n    list_dirs,\n    list_files,\n    delete_files_by_extension,\n    delete_folders_by_pattern,\n    move_files_to_parent,\n    organize_subfolders_into_named_folders,\n    rename_files,\n    add_string_filenames,\n    rename_directories\n)\n</code></pre>"},{"location":"tutorials/preprocessing/#1-data-understanding","title":"1. Data understanding","text":"<p>Before diving into the preprocessing tasks, it is essential to gain an understanding of the dataset's structure. This  helps us identify the key elements we will be working with, such as the available sequences and segmentation data.  The LUMIERE dataset contains images captured over multiple time points, so we'll need to identify and remove the unwanted data, keeping only what\u2019s relevant for analysis.</p> <p>In this section, we will explore the overall structure of the dataset, focusing on the folders and files that need our attention.</p> <p>To follow along, download the LUMIERE dataset using this link:  DOWNLOAD LUMIERE</p> <pre><code>root_data_path = \"./datasets/LUMIERE/\"\n</code></pre>"},{"location":"tutorials/preprocessing/#exploring-the-dataset","title":"Exploring the Dataset","text":"<p>We begin by checking the main directory of the dataset to understand its organization. We observe that there are 91  directories, each corresponding to a patient.</p> <pre><code>print(list_dirs(root_data_path)[:5])\nprint(list_dirs(root_data_path)[-5:])\n</code></pre> <pre><code>['Patient-001', 'Patient-002', 'Patient-003', 'Patient-004', 'Patient-005']\n['Patient-087', 'Patient-088', 'Patient-089', 'Patient-090', 'Patient-091']\n</code></pre> <p>Each patient folder contains subdirectories for different timepoints, typically named in the format \"week-XXX\", where  XXX corresponds to the week the image was taken. We may also encounter directories with additional suffixes like \"-N\"  for specific timepoints.</p> <p>To streamline our tutorial, we will focus only on the core timepoints and exclude those with the \"-N\" suffix.</p> <pre><code>print(list_dirs(f\"{root_data_path}Patient-001\"))\nprint(list_dirs(f\"{root_data_path}Patient-091\"))\n</code></pre> <pre><code>['week-000-1', 'week-000-2', 'week-044', 'week-056']\n['week-000', 'week-001', 'week-014', 'week-026', 'week-036', 'week-043']\n</code></pre>"},{"location":"tutorials/preprocessing/#reviewing-the-data-at-each-timepoint","title":"Reviewing the data at each timepoint","text":"<p>Each timepoint contains a variety of sequences along with segmentation data. For example, in the case of patient 091, we examine the week-0000 folder to find different sequence data along with segmentation predictions generated by two  models: \"DeepBraTumIA-segmentation\" and \"HD-GLIO-AUTO-segmentation\".</p> <p>For this tutorial, we will consider the segmentation provided by \"DeepBraTumIA-segmentation\" as the ground truth.</p> <pre><code>print(list_dirs(f\"{root_data_path}Patient-091/week-000\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000\"))\n</code></pre> <pre><code>['DeepBraTumIA-segmentation', 'HD-GLIO-AUTO-segmentation']\n['CT1.nii.gz', 'FLAIR.nii.gz', 'T1.nii.gz', 'T2.nii.gz']\n</code></pre> <p>The sequences that are of primary interest to us are stored in the \"atlas/skull_strip\" directory, and the corresponding segmentation is found in the \"atlas/segmentation\" directory.</p> <pre><code>print(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/skull_strip\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/segmentation\"))\n</code></pre> <pre><code>['brain_mask.nii.gz', 'ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n['measured_volumes_in_mm3.json', 'seg_mask.nii.gz']\n</code></pre>"},{"location":"tutorials/preprocessing/#2-files-cleaning","title":"2. Files cleaning","text":"<p>Now that we understand the structure, the next step is to clean the dataset by removing unnecessary files. This  involves eliminating sequences and files that we won\u2019t use in our analysis. The files we want to remove are mainly  the raw image sequences ('CT1.nii.gz', 'FLAIR.nii.gz', 'T1.nii.gz', 'T2.nii.gz'), as well as other irrelevant files  like 'brain_mask.nii.gz' and 'measured_volumes_in_mm3.json'.</p> <p>In this section, we will walk you through the process of cleaning these files using AUDIT functions.</p> <pre><code>delete_files_by_extension(\n    root_dir=root_data_path,\n    ext='CT1.nii.gz',\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-001/week-000-1/CT1.nii.gz\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-001/week-000-2/CT1.nii.gz\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-001/week-044/CT1.nii.gz\n....\n....\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-091/week-036/CT1.nii.gz\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-091/week-043/CT1.nii.gz\n</code></pre> <p>First, we use the <code>delete_files_by_extension</code> function in safe mode to preview the files that will be deleted. After  confirming that these are indeed the unnecessary files, we proceed to remove them.</p> <pre><code>sequences_to_delete = ['CT1.nii.gz', 'FLAIR.nii.gz', 'T1.nii.gz', 'T2.nii.gz']\nfor seq in sequences_to_delete:\n    delete_files_by_extension(\n        root_dir=root_data_path,\n        ext=seq,\n        safe_mode=False\n    )\n</code></pre> <p>After deletion, we can verify that the unnecessary files have been removed from the timepoint folders.</p> <pre><code>print(list_dirs(f\"{root_data_path}Patient-091/week-000\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000\"))\n</code></pre> <p>Additionally, we clean up other unnecessary files like 'brain_mask.nii.gz' and 'measured_volumes_in_mm3.json'.</p> <pre><code>files_to_delete = ['.json', 'brain_mask.nii.gz']\nfor file in files_to_delete:\n    delete_files_by_extension(\n        root_dir=root_data_path,\n        ext=file,\n        safe_mode=False\n    )\n</code></pre>"},{"location":"tutorials/preprocessing/#3-folders-cleaning","title":"3. Folders cleaning","text":"<p>In this step, we remove unnecessary folders that contain irrelevant data, such as \"HD-GLIO-AUTO-segmentation\" and  \"DeepBraTumIA-segmentation/atlas/\". This helps us further simplify the structure, keeping only the essential files  and folders for our analysis.</p> <pre><code>delete_folders_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"HD-GLIO\",\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-088/week-000-2/HD-GLIO-AUTO-segmentation\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-088/week-000-1/HD-GLIO-AUTO-segmentation\n....\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-078/week-119/HD-GLIO-AUTO-segmentation\n</code></pre> <pre><code>delete_folders_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"HD-GLIO\",\n    safe_mode=False\n)\n\ndelete_folders_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"native\",\n    safe_mode=False\n)\n</code></pre> <p>After cleaning the folders, we verify that the unnecessary directories have been removed.</p> <pre><code># No unnecessary files in the folders.\nprint(list_files(f\"{root_data_path}Patient-091/week-000/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas\"))\n\n# No unnecessary directories in the folders.\nprint(list_dirs(f\"{root_data_path}Patient-091/week-000/\"))\nprint(list_dirs(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/\"))\nprint(list_dirs(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas\"))\n</code></pre> <pre><code>[]\n[]\n[]\n['DeepBraTumIA-segmentation']\n['atlas']\n['segmentation', 'skull_strip']\n</code></pre>"},{"location":"tutorials/preprocessing/#4-files-organization","title":"4. Files organization","text":"<p>Now that we have cleaned up the unnecessary files and folders, it\u2019s time to organize the remaining files. Currently,  they are nested in deep directory structures, and we need to move them to the parent folders for easier access.</p> <p>In this section, we will use the <code>move_files_to_parent</code> function to simplify the file structure by moving the  necessary files to their corresponding parent directories.</p> <pre><code>move_files_to_parent(\n    root_dir=root_data_path,\n    levels_up=3,\n    ext=None,\n    safe_mode=False\n)\n</code></pre> <p>This will move the files up from their current deep folder structure to the appropriate parent folders. Let's verify  that everything has been moved correctly.</p> <pre><code>print(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/skull_strip\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/segmentation\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/\"))\nprint(list_files(f\"{root_data_path}Patient-002/week-047/\"))\n</code></pre> <pre><code>[]\n[]\n[]\n[]\n['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n</code></pre> <p>We also remove any remaining directories we no longer need.</p> <pre><code>delete_folders_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"DeepBraTumIA-segmentation\",\n    safe_mode=False\n)\n</code></pre> <pre><code>[]\n['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n</code></pre>"},{"location":"tutorials/preprocessing/#5-folders-organization","title":"5. Folders organization","text":"<p>At this point, we need to organize the folders further. To align with the expected structure for AUDIT, we will  move the timepoint folders to the root level. This way, each subject and their respective timepoints will be placed  in a well-organized directory.</p> <p>The function <code>organize_subfolders_into_named_folders</code> will help us organize subdirectories by joining their parent and child folder names. The \"join\" argument defines the string used to concatenate the parent folder and the child folder.  Check the documentation for more detailed information.</p> <pre><code>organize_subfolders_into_named_folders(\n    root_dir=root_data_path,\n    join_char=\"-\",\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would move: ./datasets/LUMIERE/Patient-001/week-000-2/t2_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-001-week-000-2/t2_skull_strip.nii.gz\n[SAFE MODE] Would move: ./datasets/LUMIERE/Patient-001/week-000-2/ct1_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-001-week-000-2/ct1_skull_strip.nii.gz\n....\n[SAFE MODE] Would move: ./datasets/LUMIERE/Patient-091/week-014/flair_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-091-week-014/flair_skull_strip.nii.gz\n</code></pre> <p>Now, we will move the subfolders into the root directory and rename them as necessary.</p> <pre><code>organize_subfolders_into_named_folders(\n    root_dir=root_data_path,\n    join_char=\"-\",\n    safe_mode=False\n)\n</code></pre> <pre><code>print(list_dirs(root_data_path)[:6])\n</code></pre> <pre><code>['Patient-001-week-000-1', 'Patient-001-week-000-2', 'Patient-001-week-044', 'Patient-001-week-056', 'Patient-002-week-000', 'Patient-002-week-003']\n</code></pre> <p>As mentioned earlier, for simplicity, we will only keep the timepoints that do not have a \"-N\" suffix at the end of  the corresponding week.</p> <p><pre><code>pattern_to_delete = r\"^Patient-\\d{3}-week-\\d{3}-\\d\"\n\ndelete_folders_by_pattern(\n    root_dir=root_data_path,\n    pattern=pattern_to_delete,\n    safe_mode=False\n)\n\nprint(list_dirs(root_data_path)[:6])\n</code></pre> <pre><code>['Patient-001-week-044', 'Patient-001-week-056', 'Patient-002-week-000', 'Patient-002-week-003', 'Patient-002-week-021', 'Patient-002-week-037']\n</code></pre></p>"},{"location":"tutorials/preprocessing/#6-sequences-name-standardization","title":"6. Sequences name standardization","text":"<p>Finally, to follow a more standardized naming convention, such as the one used in the BraTS dataset, we will rename  the sequences and the segmentation to follow a similar pattern. Typically, MRI sequences are named <code>t1</code>, <code>t2</code>, <code>t1ce</code>,  and <code>flair</code>, and the segmentation is named <code>seg</code>. However, the names we currently have do not follow this convention.  Let's use <code>rename_files</code> to modify them.</p> <pre><code>print(list_files(f\"{root_data_path}Patient-001-week-044\"))\n</code></pre> <pre><code>['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n</code></pre> <pre><code>old_names = ['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\nnew_names = ['t1ce.nii.gz', 'flair.nii.gz', 'seg.nii.gz', 't1.nii.gz', 't2.nii.gz']\n\nfor o, n in zip(old_names, new_names):\n    rename_files(\n        root_dir=root_data_path,\n        old_name=o,\n        new_name=n,\n        safe_mode=True\n    )\n</code></pre> <pre><code>[SAFE MODE] Would rename: ./datasets/LUMIERE/Patient-012-week-016/ct1_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-012-week-016/t1ce.nii.gz\n[SAFE MODE] Would rename: ./datasets/LUMIERE/Patient-023-week-001/ct1_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-023-week-001/t1ce.nii.gz\n....\n[SAFE MODE] Would rename: ./datasets/LUMIERE/Patient-077-week-083/t2_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-077-week-083/t2.nii.gz\n</code></pre> <pre><code>for o, n in zip(old_names, new_names):\n    rename_files(\n        root_dir=root_data_path,\n        old_name=o,\n        new_name=n,\n        safe_mode=False\n    )\n</code></pre> <p>Additionally, to allow AUDIT to locate each image simply by the subject ID, we will name each image with the  corresponding subject identifier along with the sequence name. To do this, we will use the <code>add_string_filenames</code>  function, which allows us to add both suffixes and prefixes to specific files.</p> <pre><code>for subject in list_dirs(root_data_path):\n    add_string_filenames(\n        root_dir=os.path.join(root_data_path, subject),\n        prefix=f\"{subject}_\",\n        ext=None,\n        safe_mode=False\n    )\n</code></pre> <p>With this, we would have organized the project as required to work with AUDIT. Additionally, we recommend that the  images (sequences and segmentations provided by the medical experts) be placed in a directory called <code>DATASET_images</code>,  so that the segmentations from each model are contained in the <code>DATASET_seg</code> directory. Therefore, to conclude, we'll  rename the LUMIERE directory to <code>LUMIERE_images</code>.</p> <pre><code>rename_directories(\n    root_dir=\"./datasets/\",\n    old_name=\"LUMIERE\",\n    new_name=\"LUMIERE_images\",\n    safe_mode=False\n)\n</code></pre>"},{"location":"tutorials/preprocessing/#conclusion","title":"Conclusion","text":"<p>By following these steps, we have successfully cleaned, organized, and standardized the LUMIERE dataset, making it  ready for further analysis. The AUDIT library has provided a powerful toolkit for efficiently preprocessing and  structuring the data.</p>"}]}