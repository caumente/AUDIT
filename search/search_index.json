{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AUDIT library","text":"<p> AUDIT: An open-source Python library for comprehensive evaluation of medical image segmentation models and MRI datasets analysis </p> <p>Documentation: https://caumente.github.io/AUDIT/</p> <p>Source Code: https://github.com/caumente/AUDIT/</p> <p>Welcome to the official documentation for AUDIT (Analysis &amp; evalUation Dashboard of artIficial inTelligence), a a tool designed to provide researchers and developers an interactive way to better analyze and explore MRI datasets and  segmentation models. Given its functionalities to extract the most relevant features and metrics from your several data  sources, it allows for uncovering biases both intra and inter-dataset as well as within the model predictions.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Robust evaluation: Extracts region-specific features and calculates a wide range of performance metrics.</li> <li>Interactive visualizations: Includes a dynamic Streamlit-based web app for intuitive data exploration.</li> <li>Highly customizable: Easily extendable for additional features and metrics tailored to your needs.</li> <li>Friendly integration: Supports plugins and external libraries for advanced analysis (e.g., ITK-SNAP, pymia).</li> <li>Open Source: Fully available on GitHub with comprehensive tutorials and examples.</li> </ul>"},{"location":"#what-youll-find-here","title":"\ud83d\udcda What You'll Find Here","text":"<p>This documentation is structured to help you get the most out of AUDIT:</p> <ul> <li>Getting Started: Learn how to install AUDIT and set up your first project.</li> <li>API Reference: Detailed reference for all library classes and methods.</li> <li>Analysis modes: Explore the dashboard included in the web app.</li> <li>Tutorials: Hands-on examples demonstrating common use cases.</li> <li>About: Check latest AUDIT release and license terms.</li> </ul>"},{"location":"#quick-start","title":"\ud83c\udf1f Quick Start","text":"<p>The best way to get familiar with AUDIT and explore all its capabilities is through our interactive DEMO. You can find it at: https://auditapp.streamlit.app/. </p> <p>Users will find an online wep app with pre-configured data to explore features and compare the accuracy of a set of  medical image segmentation models. However, you could use AUDIT easily in your own computer by following a few little steps. </p> <p>Install AUDIT </p> <p>Directly from PyPI:</p> <pre><code>pip install auditapp\n</code></pre> <p>Alternatively, clone the repository and install it locally:</p> <pre><code>git clone https://github.com/caumente/AUDIT.git\ncd AUDIT\npip install -r requirements.txt\n</code></pre> <p>Launch AUDIT App </p> <p>Start the interactive web app using our default configuration for data visualization and exploration</p> <p>Run the following command if you installed AUDIT directly from PyPI: <pre><code>auditapp run-app\n</code></pre></p> <p>Or alternatively, if you cloned the repository and install it locally, run:</p> <pre><code>python src/audit/app/launcher.py\n</code></pre> <p>That's it! You're ready to explore our default data and evaluate AI segmentations models with AUDIT. Go to  Getting Started to learn more about how to use AUDIT with your own datasets and  configurations.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Please check the contributing guide for details on how to report issues, suggest new features,  or contribute code.</p> <p>Feel free to reach out at Contact Us.</p>"},{"location":"API_reference/features/","title":"Feature extraction","text":"<p>This <code>feature extraction</code> pipeline is designed to process medical imaging datasets, specifically MRI scans, to extract a  wide range of features including spatial, tumor-related, statistical, and texture-based characteristics. The pipeline  is composed of two core components: the feature_extraction.py script, which orchestrates the entire process, and the  underlying feature extraction logic contained in src.features.main.py.</p> <p>The pipeline operates as follows:</p> <ul> <li> <p>Configuration and Logging: The process begins by loading configuration settings from a YAML file, specifying data  paths, features to extract, and output directories. A logging system is set up to monitor and record the progress of  the feature extraction.</p> </li> <li> <p>Dataset Processing: For each dataset defined in the configuration, the pipeline iterates over all subject data,  extracting features from MRI sequences and associated segmentations. This is handled by the extract_features function, which computes various features such as spatial dimensions, tumor characteristics, statistical metrics, and texture  properties of the images.</p> </li> <li> <p>Feature Extraction: The pipeline uses specialized classes for different types of features:</p> <ul> <li>Spatial Features: Related to image dimensions and brain structure.</li> <li>Tumor Features: Derived from segmentations to describe the tumor\u2019s shape, volume, and position.</li> <li>Statistical Features: First-order statistics like mean, variance, etc., extracted from image sequences.</li> <li>Texture Features: Second-order metrics describing the texture patterns in the images.</li> </ul> </li> </ul> <p>If longitudinal data is present, the pipeline also extracts and includes time-point and longitudinal  identifiers for further analysis.</p> <ul> <li>Data Output: Once features are extracted for each subject, they are compiled into a DataFrame, which is saved as  a CSV file. </li> </ul> <p>This pipeline provides an automated and extensible framework for processing large-scale MRI datasets, ensuring that  all relevant features are extracted and saved for downstream analysis, such as predictive modeling or visualization.</p> <p>In the following sections, the users can explore in detail the different methods provided by AUDIT to extract relevant  information from MRIs.</p>"},{"location":"API_reference/features/spatial/","title":"Spatial features","text":"<p>The <code>SpatialFeatures</code> class is designed to compute spatial properties related to 3D medical imaging sequences, such as  brain MRI scans. This class focuses on calculating basic spatial features like the brain's center of mass and the  dimensionality of the scan in various planes.</p>"},{"location":"API_reference/features/spatial/#overview","title":"Overview","text":"<p>This class is intended to provide spatial insights from a 3D sequence of medical images. It helps to extract two key  metrics: The center of mass for the brain, which is calculated based on the sequence, and the dimensions of the sequence in the axial, coronal, and sagittal planes. These spatial features are essential in understanding the brain's structure  and alignment in a 3D scan, aiding in medical analysis and further processing of brain images.</p> <p>The following spatial features are available:</p> <ul> <li>Brain Center of Mass: The 3D coordinates of the brain's center, adjusted by voxel spacing.  </li> <li>Sequence Dimensions: The dimensions of the sequence in the axial, coronal, and sagittal planes.</li> </ul>"},{"location":"API_reference/features/spatial/#methods","title":"Methods","text":""},{"location":"API_reference/features/spatial/#__init__","title":"<code>__init__</code>","text":"<p>Constructs all the necessary attributes for the SpatialFeatures object.</p> <p>Parameters </p> <ul> <li>sequence (<code>np.ndarray</code>): A 3D NumPy array representing the medical image sequence.  </li> <li>spacing (<code>np.ndarray</code>, optional): A tuple representing the voxel spacing of the medical image. Defaults to <code>(1, 1, 1)</code>.</li> </ul>"},{"location":"API_reference/features/spatial/#calculate_anatomical_center_mass","title":"<code>calculate_anatomical_center_mass</code>","text":"<p>Calculates the center of mass for the anatomical image.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary containing the 3D coordinates of the brain's center of mass for each plane (axial, coronal, sagittal), adjusted by the voxel spacing.</li> </ul>"},{"location":"API_reference/features/spatial/#get_shape","title":"<code>get_shape</code>","text":"<p>Gets the dimensions of the sequence in axial, coronal, and sagittal planes.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary containing the dimensions of the sequence:  </li> <li><code>axial_plane_resolution</code>: Axial plane resolution.  </li> <li><code>coronal_plane_resolution</code>: Coronal plane resolution.  </li> <li><code>sagittal_plane_resolution</code>: Sagittal plane resolution.</li> </ul>"},{"location":"API_reference/features/spatial/#extract_features","title":"<code>extract_features</code>","text":"<p>Extracts all spatial features from the sequence.</p> <p>Returns </p> <p>A dictionary containing all spatial features, including dimensions and center of mass for each plane.</p>"},{"location":"API_reference/features/statistical/","title":"Statistical features","text":"<p>The <code>StatisticalFeatures</code> class provides a convenient way to compute several common statistical metrics from a given array of data.</p>"},{"location":"API_reference/features/statistical/#overview","title":"Overview","text":"<p>This class utilizes NumPy for efficient numerical operations and SciPy for computing first-order statistical features. By encapsulating these features in a class, users can easily compute various statistical properties of a dataset with minimal boilerplate code.</p> <p>The following statistical features are available:</p> <ul> <li>Maximum intensity: The highest value in the MRI.  </li> <li>Minimum intensity: The lowest value in the MRI.  </li> <li>Mean intensity: The average value of the MRI.  </li> <li>Median intensity: The middle value of the MRI when sorted.  </li> <li>Standard deviation intensity: A measure of the amount of variation or dispersion of the values.  </li> <li>Range intensity: The difference between the maximum and minimum values.  </li> <li>Skewness: A measure of the asymmetry of the distribution of pixel values.  </li> <li>Kurtosis: A measure of the \"tailedness\" of the intensity distribution.</li> </ul>"},{"location":"API_reference/features/statistical/#methods","title":"Methods","text":""},{"location":"API_reference/features/statistical/#__init__","title":"<code>__init__</code>","text":"<p>Constructs all the necessary attributes for the StatisticalFeatures object.</p> <p>Parameters </p> <ul> <li>sequence (<code>np.ndarray</code>): A 3D NumPy array representing the MRI sequence from which statistical features are to be computed.</li> </ul>"},{"location":"API_reference/features/statistical/#get_max_intensity","title":"<code>get_max_intensity</code>","text":"<p>Computes the maximum intensity value in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The maximum intensity value in the sequence.</li> </ul>"},{"location":"API_reference/features/statistical/#get_min_intensity","title":"<code>get_min_intensity</code>","text":"<p>Computes the minimum intensity value in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The minimum intensity value in the sequence.</li> </ul>"},{"location":"API_reference/features/statistical/#get_mean_intensity","title":"<code>get_mean_intensity</code>","text":"<p>Computes the mean intensity value in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The mean intensity value in the sequence.</li> </ul>"},{"location":"API_reference/features/statistical/#get_median_intensity","title":"<code>get_median_intensity</code>","text":"<p>Computes the median intensity value in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The median intensity value in the sequence.</li> </ul>"},{"location":"API_reference/features/statistical/#get_percentile_n","title":"<code>get_percentile_n</code>","text":"<p>Computes the n-th percentile of the intensity values in the sequence.</p> <p>Parameters </p> <ul> <li>n (<code>float</code>): The percentile value to compute (e.g., <code>10</code> or <code>90</code>).</li> </ul> <p>Returns </p> <ul> <li><code>float</code>: The computed n-th percentile intensity.</li> </ul>"},{"location":"API_reference/features/statistical/#get_std_intensity","title":"<code>get_std_intensity</code>","text":"<p>Computes the standard deviation of intensity values in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The standard deviation of intensity values.</li> </ul>"},{"location":"API_reference/features/statistical/#get_range_intensity","title":"<code>get_range_intensity</code>","text":"<p>Computes the range of intensity values in the sequence (max - min).</p> <p>Returns </p> <ul> <li><code>float</code>: The range of intensity values in the sequence.</li> </ul>"},{"location":"API_reference/features/statistical/#get_skewness","title":"<code>get_skewness</code>","text":"<p>Computes the skewness of the intensity values in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The skewness of the intensity distribution.</li> </ul>"},{"location":"API_reference/features/statistical/#get_kurtosis","title":"<code>get_kurtosis</code>","text":"<p>Computes the kurtosis of the intensity values in the sequence.</p> <p>Returns </p> <ul> <li><code>float</code>: The kurtosis of the intensity distribution.</li> </ul>"},{"location":"API_reference/features/statistical/#extract_features","title":"<code>extract_features</code>","text":"<p>Computes and returns all statistical metrics as a dictionary.</p> <p>Returns </p> <p>A dictionary containing the following statistical metrics:</p> <ul> <li><code>max_intensity</code>: Maximum intensity value in the MRI.  </li> <li><code>min_intensity</code>: Minimum intensity value in the MRI.  </li> <li><code>mean_intensity</code>: Mean intensity value in the MRI.  </li> <li><code>median_intensity</code>: Median intensity value in the MRI.  </li> <li><code>10th_percentile_intensity</code>: 10th percentile intensity.  </li> <li><code>90th_percentile_intensity</code>: 90th percentile intensity.  </li> <li><code>std_intensity</code>: Standard deviation of intensity values.  </li> <li><code>range_intensity</code>: Range of intensity values.  </li> <li><code>skewness</code>: Skewness of the intensity values.  </li> <li><code>kurtosis</code>: Kurtosis of the intensity values.</li> </ul>"},{"location":"API_reference/features/texture/","title":"Texture features","text":"<p>The <code>TextureFeatures</code> class provides an efficient mechanism for calculating second-order texture features from a given 3D magnetic resonance image (MRI).</p>"},{"location":"API_reference/features/texture/#overview","title":"Overview","text":"<p>This class utilizes skimage for calculating the gray level co-occurrence matrix (GLCM) and its corresponding texture  features such as contrast, homogeneity, and energy. The texture features extracted from each 2D plane of a 3D MRI  sequence give insights into the structural patterns within the image.</p> <p>By encapsulating these operations in a class, the user can easily compute several texture features with minimal effort. It also supports an option to remove empty planes, improving accuracy when working with brain MRI scans.</p> <p>The following texture features are available:</p> <ul> <li>Contrast: A measure of the intensity contrast between a pixel and its neighbor over the whole image.</li> <li>Dissimilarity: Measures the local intensity variations.</li> <li>Homogeneity: Measures the closeness of the distribution of elements in the GLCM to the GLCM diagonal.</li> <li>ASM (Angular Second Moment): A measure of the texture uniformity.</li> <li>Energy: The square root of ASM, indicating the texture\u2019s level of orderliness.</li> <li>Correlation: A measure of how correlated a pixel is to its neighbor across the whole image.</li> </ul>"},{"location":"API_reference/features/texture/#methods","title":"Methods","text":""},{"location":"API_reference/features/texture/#__init__","title":"<code>__init__</code>","text":"<p>Constructs all the necessary attributes for the TextureFeatures object.</p> <p>Parameters </p> <ul> <li>sequence (<code>np.ndarray</code>): A 3D NumPy array representing the MRI image.  </li> <li>remove_empty_planes (<code>bool</code>, default: <code>False</code>): Whether to remove empty (non-brain) planes before processing.</li> </ul>"},{"location":"API_reference/features/texture/#compute_texture_values","title":"<code>compute_texture_values</code>","text":"<p>Computes texture values for each 2D plane in the 3D image array.</p> <p>Parameters </p> <ul> <li>texture (<code>str</code>): The texture feature to compute (default is <code>\"contrast\"</code>).</li> </ul> <p>Returns </p> <ul> <li><code>np.ndarray</code>: An array of texture values for each 2D plane in the image.</li> </ul>"},{"location":"API_reference/features/texture/#extract_features","title":"<code>extract_features</code>","text":"<p>Extracts texture features from the MRI image by calculating statistical summaries for multiple texture metrics.</p> <p>Parameters </p> <ul> <li>textures (<code>list[str]</code>, optional): A list of texture features to compute (e.g., <code>'contrast'</code>, <code>'energy'</code>).   Defaults to <code>['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'energy', 'correlation']</code>.</li> </ul> <p>Returns </p> <p>A dictionary where keys represent texture feature names, and values represent the mean and standard deviation for each feature.</p>"},{"location":"API_reference/features/tumor/","title":"Tumor features","text":"<p>The <code>TumorFeatures</code> class computes various tumor-related metrics based on a segmented 3D medical image, such as lesion  size, tumor center of mass, and tumor location relative to the brain's center of mass.</p>"},{"location":"API_reference/features/tumor/#overview","title":"Overview","text":"<p>This class provides methods to compute first-order tumor features, focusing on spatial and volumetric characteristics  derived from medical image segmentation. It is designed to handle common use cases in medical imaging, such as  identifying tumor regions, calculating the tumor's size, and determining its relative position.</p> <p>The class allows customization through optional parameters such as voxel spacing and segmentation label mappings. This  makes it highly adaptable to different medical imaging contexts, including various scan types and segmentation  algorithms.</p> <p>The following tumor features are available:</p> <ul> <li>Tumor Pixel Count: The number of pixels associated with each tumor label in the segmentation.</li> <li>Lesion Size: The volume of the lesion(s) computed based on pixel count and voxel spacing.</li> <li>Tumor Center of Mass: The geometric center of a tumor or lesion in 3D space.</li> <li>Tumor Slices: The image slices in the axial, coronal, and sagittal planes that contain tumor regions.</li> <li>Tumor Position: The location of tumor slices in each plane (e.g., lower and upper bounds).</li> </ul>"},{"location":"API_reference/features/tumor/#methods","title":"Methods","text":""},{"location":"API_reference/features/tumor/#__init__","title":"<code>__init__</code>","text":"<p>Constructs all the necessary attributes for the TumorFeatures object.</p> <p>Parameters </p> <ul> <li>segmentation (<code>np.ndarray</code>): A numpy array representing the segmentation of the medical image.  </li> <li>spacing (<code>tuple</code>, optional): The voxel spacing of the image (default is <code>(1, 1, 1)</code>).  </li> <li>mapping_names (<code>dict</code>, optional): A dictionary mapping segmentation values to names.  </li> <li>planes (<code>list[str]</code>, optional): The planes (axial, coronal, sagittal) for tumor slice analysis. Defaults to <code>[\"axial\", \"coronal\", \"sagittal\"]</code>.</li> </ul>"},{"location":"API_reference/features/tumor/#count_tumor_pixels","title":"<code>count_tumor_pixels</code>","text":"<p>Counts the number of pixels for each unique value in the segmentation.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary with the counts of each unique value in the segmentation.</li> </ul>"},{"location":"API_reference/features/tumor/#calculate_whole_lesion_size","title":"<code>calculate_whole_lesion_size</code>","text":"<p>Calculates the total lesion size in the segmentation.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary containing the lesion size in cubic millimeters.</li> </ul>"},{"location":"API_reference/features/tumor/#get_tumor_center_mass","title":"<code>get_tumor_center_mass</code>","text":"<p>Calculates the center of mass for the tumor in the image.</p> <p>Parameters </p> <ul> <li>label (<code>int</code>, optional): The label value of the tumor (default is <code>None</code>).</li> </ul> <p>Returns </p> <ul> <li><code>np.ndarray</code>: The center of mass coordinates adjusted by the voxel spacing.</li> </ul>"},{"location":"API_reference/features/tumor/#get_tumor_slices","title":"<code>get_tumor_slices</code>","text":"<p>Obtains the slices that contain tumor regions in the axial, coronal, and sagittal planes.</p> <p>Returns </p> <ul> <li><code>tuple</code>: A tuple containing three lists, each representing the indices of slices with tumor presence in each plane.</li> </ul>"},{"location":"API_reference/features/tumor/#calculate_tumor_slices","title":"<code>calculate_tumor_slices</code>","text":"<p>Calculates the number of tumor-containing slices per plane.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary where keys represent each plane (e.g., <code>\"axial_tumor_slices\"</code>) and values represent the number of slices with tumor.</li> </ul>"},{"location":"API_reference/features/tumor/#calculate_position_tumor_slices","title":"<code>calculate_position_tumor_slices</code>","text":"<p>Determines the lower and upper tumor slice indices for each plane.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary containing the minimum and maximum slice indices for each plane (e.g., <code>\"lower_axial_tumor_slice\"</code>, <code>\"upper_axial_tumor_slice\"</code>).</li> </ul>"},{"location":"API_reference/features/tumor/#calculate_tumor_pixel","title":"<code>calculate_tumor_pixel</code>","text":"<p>Computes the number of pixels per tumor label and converts them into volume using voxel spacing.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary where keys represent each tumor label (e.g., <code>\"lesion_size_label1\"</code>) and values represent the lesion size in voxels.</li> </ul>"},{"location":"API_reference/features/tumor/#calculate_tumor_distance","title":"<code>calculate_tumor_distance</code>","text":"<p>Calculates the Euclidean distance between the tumor center of mass and the brain's center of mass.</p> <p>Parameters </p> <ul> <li>brain_centre_mass (<code>array-like</code>): The center of mass of the brain used as a reference point.</li> </ul> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary where each key represents the tumor label and the value represents the distance between the tumor and the brain's center of mass.</li> </ul>"},{"location":"API_reference/features/tumor/#calculate_tumor_center_mass","title":"<code>calculate_tumor_center_mass</code>","text":"<p>Calculates the tumor center of mass for each label and plane.</p> <p>Returns </p> <ul> <li><code>dict</code>: A dictionary where keys represent each plane and label (e.g., <code>\"axial_tumor_center_mass\"</code>) and values represent the coordinates of the tumor center of mass.</li> </ul>"},{"location":"API_reference/features/tumor/#extract_features","title":"<code>extract_features</code>","text":"<p>Extracts all tumor-related features, combining lesion size, center of mass, position, and slice information.</p> <p>Returns </p> <p>A dictionary containing all computed tumor features, including:</p> <ul> <li>Center of mass per label and plane.  </li> <li>Tumor location relative to brain center of mass.  </li> <li>Lesion size per label.  </li> <li>Total lesion size.  </li> <li>Number of tumor-containing slices per plane.  </li> <li>Lower and upper bounds of tumor slices.</li> </ul>"},{"location":"API_reference/metrics/audit_metrics/","title":"Metrics Computed","text":"<p>The provided code computes a variety of metrics to evaluate the performance of a segmentation model in relation to the  ground truth. These metrics provide insights into the model's accuracy, overlap, and shape conformity with the actual  segmented regions. Below is an overview of each metric computed:</p>"},{"location":"API_reference/metrics/audit_metrics/#1-dice-score-dice","title":"1. Dice Score (DICE)","text":"<p>The Dice score (or Dice coefficient) is a measure of overlap between the ground truth and the predicted segmentation. It ranges from 0 to 1, with 1 indicating perfect overlap.</p> \\[ \\text{Dice} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN} \\] <p>Where TP is true positives, FP is false positives, and FN is false negatives.</p> <p>Interpretation: A higher Dice score indicates better agreement between prediction and ground truth.</p>"},{"location":"API_reference/metrics/audit_metrics/#2-jaccard-index-jacc","title":"2. Jaccard Index (JACC)","text":"<p>Also known as the Intersection over Union (IoU), the Jaccard index is another overlap-based metric. It measures the  size of the intersection divided by the size of the union of the predicted and ground truth regions.</p> \\[ \\text{Jaccard} = \\frac{TP}{TP + FP + FN} \\] <p>Interpretation: A higher Jaccard index indicates a more accurate segmentation. It is always lower than the Dice score  for the same segmentation.</p>"},{"location":"API_reference/metrics/audit_metrics/#3-sensitivity-sens","title":"3. Sensitivity (SENS)","text":"<p>Sensitivity, also known as recall or true positive rate, measures the ability of the model to correctly identify all  the positive regions (i.e., tumor voxels).</p> \\[ \\text{Sensitivity} = \\frac{TP}{TP + FN} \\] <p>Interpretation: A higher sensitivity value indicates the model is good at detecting positive regions (e.g., tumor  regions), but it doesn\u2019t account for false positives.</p>"},{"location":"API_reference/metrics/audit_metrics/#4-specificity-spec","title":"4. Specificity (SPEC)","text":"<p>Specificity measures the model's ability to correctly identify negative regions (i.e., non-tumor voxels).</p> \\[ \\text{Specificity} = \\frac{TN}{TN + FP} \\] <p>Interpretation: A higher specificity value indicates the model is good at ignoring false positives, but doesn\u2019t account for missing true positives.</p>"},{"location":"API_reference/metrics/audit_metrics/#5-precision-prec","title":"5. Precision (PREC)","text":"<p>Precision, or positive predictive value, measures the proportion of predicted positive cases that are actually positive.</p> \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\] <p>Interpretation: A high precision value means that when the model predicts a positive case (e.g., tumor), it is likely  correct.</p>"},{"location":"API_reference/metrics/audit_metrics/#6-accuracy-accu","title":"6. Accuracy (ACCU)","text":"<p>Accuracy provides an overall measure of how often the model makes correct predictions (both true positives and true  negatives) across all regions.</p> \\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\] <p>Interpretation: A high accuracy score reflects the model\u2019s general performance in predicting both positive and negative cases.</p>"},{"location":"API_reference/metrics/audit_metrics/#7-hausdorff-distance-haus","title":"7. Hausdorff Distance (HAUS)","text":"<p>The Hausdorff distance is a shape-based metric that measures the maximum distance between points on the predicted  segmentation and the corresponding points on the ground truth.</p> \\[ \\text{Hausdorff Distance} = \\max_{x \\in A} \\min_{y \\in B} d(x, y) \\] <p>Where A is the set of points on the predicted segmentation, B is the set of points on the ground truth, and  d(x, y) is the Euclidean distance between points.</p> <p>Interpretation: Lower Hausdorff distances indicate that the boundary of the predicted segmentation is closer to the ground truth boundary, implying better shape similarity.</p>"},{"location":"API_reference/metrics/audit_metrics/#8-segmentation-size-size","title":"8. Segmentation Size (SIZE)","text":"<p>This metric calculates the physical size of the predicted segmentation in terms of voxel count, adjusted by the voxel spacing to provide a volume measurement.</p> \\[ \\text{Size} = \\text{Voxel count of predicted region} \\times \\text{Spacing} \\] <p>Interpretation: This helps to quantify the total volume of the segmented region, which can be compared to the expected  size from the ground truth.</p>"},{"location":"API_reference/metrics/metric_extraction/","title":"Metric extraction","text":"<p>This <code>metric extraction</code> pipeline processes MRI segmentation data by comparing ground truth segmentations with model  predictions to compute a variety of metrics. The pipeline supports both custom metrics and Pymia-based metrics, with  the ability to handle multiple models and datasets. The two main components of the pipeline are metric_extraction.py,  which serves as the entry point, and main.py, which contains the core logic for metric computation.</p> <p>The pipeline operates as follows:</p> <ul> <li> <p>Configuration and Logging: The pipeline begins by loading a configuration file, which defines the dataset paths,  models, metrics to be extracted, and output settings. Logging is set up to track the progress and output detailed logs.</p> </li> <li> <p>Metric Extraction: Depending on the configuration, the pipeline can compute either custom metrics (using methods  defined in the project) or Pymia metrics (using the Pymia library for medical image analysis). </p> </li> <li> <p>Custom Metrics: This approach calculates specific metrics like Dice coefficient, sensitivity, or others based on  custom implementations. It involves one-hot encoding the ground truth and predicted segmentations and then computing  the defined metrics for each subject.</p> </li> <li> <p>Pymia Metrics: The pipeline can leverage Pymia's built-in metrics (e.g., Hausdorff distance, Dice coefficient,  Jaccard index) for segmentation evaluation. Pymia's evaluator processes the segmentation files and accumulates the  results across different models and regions.</p> </li> <li> <p>Data Processing: For each dataset and model, metrics are computed for all subjects. The results are collected  into a DataFrame, and if longitudinal data is involved, it can further organize the results by time points.</p> </li> <li> <p>Output and Statistics: The extracted metrics are stored as CSV files, and additional statistical analyses (e.g.,  mean, median, standard deviation) can be computed and saved if required. The output is structured and ready for  further analysis or reporting.</p> </li> </ul> <p>This pipeline provides a flexible and scalable solution for evaluating segmentation models, making it suitable for  multi-model comparisons and performance tracking across different datasets.</p>"},{"location":"API_reference/utils/file_manager/","title":"File manager","text":"<p>The <code>file_manager</code> module provides a comprehensive set of utilities for managing files and directories. It supports operations for listing, renaming, copying, moving, deleting, and organizing files, with pattern matching, recursion, and safe simulation modes.</p>"},{"location":"API_reference/utils/file_manager/#project-initialization","title":"Project Initialization","text":""},{"location":"API_reference/utils/file_manager/#create_project_structure","title":"<code>create_project_structure</code>","text":"<p>Creates the project directory structure and copies default config files from the installed AUDIT package into the project's configs folder.</p> <pre><code>your_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 app.yaml\n\u2502   \u251c\u2500\u2500 feature_extraction.yaml\n\u2502   \u2514\u2500\u2500 metric_extraction.yaml\n\u251c\u2500\u2500 outputs/\n\u2514\u2500\u2500 logs/\n</code></pre> <p>Parameters </p> <ul> <li>base_path (<code>str</code>, default: <code>\"./\"</code>): Root directory name where the project structure will be created.</li> </ul>"},{"location":"API_reference/utils/file_manager/#listing-operations","title":"Listing Operations","text":""},{"location":"API_reference/utils/file_manager/#list_dirs","title":"<code>list_dirs</code>","text":"<p>List directories in a given path.</p> <p>Parameters </p> <ul> <li>path (<code>str</code> or <code>Path</code>): The root directory where to look for subdirectories.  </li> <li>recursive (<code>bool</code>, default: <code>False</code>): If True, search subdirectories recursively.  </li> <li>full_path (<code>bool</code>, default: <code>False</code>): If True, return absolute paths instead of just directory names.  </li> <li>pattern (<code>str</code>, optional): Optional regex pattern to filter directory names.</li> </ul> <p>Returns </p> <ul> <li><code>list[str]</code>: A sorted list of directory names or paths.</li> </ul>"},{"location":"API_reference/utils/file_manager/#list_files","title":"<code>list_files</code>","text":"<p>List files in a given directory.</p> <p>Parameters </p> <ul> <li>path (<code>str</code> or <code>Path</code>): Root directory to search.  </li> <li>recursive (<code>bool</code>, default: <code>False</code>): If True, search subdirectories recursively.  </li> <li>full_path (<code>bool</code>, default: <code>False</code>): If True, return absolute paths instead of just filenames.  </li> <li>pattern (<code>str</code>, optional): Regex pattern to filter file names.  </li> <li>extensions (<code>list[str]</code> or <code>None</code>, optional): List of file extensions to filter by (e.g., <code>['.csv', '.yml']</code>).</li> </ul> <p>Returns </p> <ul> <li><code>list[str]</code>: A sorted list of file names or paths.</li> </ul>"},{"location":"API_reference/utils/file_manager/#directory-operations","title":"Directory Operations","text":""},{"location":"API_reference/utils/file_manager/#rename_dirs","title":"<code>rename_dirs</code>","text":"<p>Rename directories recursively by replacing a substring in their names.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code> or <code>Path</code>): Path to the directory where renaming will be performed.  </li> <li>old_name (<code>str</code>): The string to be replaced in the directory names.  </li> <li>new_name (<code>str</code>): The new string that will replace old_name.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print information about each rename operation.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, only simulate renaming without making changes.</li> </ul>"},{"location":"API_reference/utils/file_manager/#add_string_dirs","title":"<code>add_string_dirs</code>","text":"<p>Add a prefix and/or suffix to all directories and subdirectories.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code> or <code>Path</code>): Root directory to start renaming.  </li> <li>prefix (<code>str</code>, default: <code>\"\"</code>): Prefix to add to directory names.  </li> <li>suffix (<code>str</code>, default: <code>\"\"</code>): Suffix to add to directory names.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print information about renamed directories (only when safe_mode=False).  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate renaming without changing directories.</li> </ul>"},{"location":"API_reference/utils/file_manager/#file-operations","title":"File Operations","text":""},{"location":"API_reference/utils/file_manager/#rename_files","title":"<code>rename_files</code>","text":"<p>Recursively rename files by replacing a substring in their filenames.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code> or <code>Path</code>): Root directory to start renaming files.  </li> <li>old_name (<code>str</code>, default: <code>\"\"</code>): Substring in filenames to replace.  </li> <li>new_name (<code>str</code>, default: <code>\"\"</code>): Substring to replace old_name with.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print information about renamed files (only when safe_mode=False).  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate renaming without changing files.</li> </ul>"},{"location":"API_reference/utils/file_manager/#add_suffix_to_files","title":"<code>add_suffix_to_files</code>","text":"<p>Adds a suffix to all files with a specific extension in a folder and its subdirectories.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): The folder where the files are located.  </li> <li>suffix (<code>str</code>, default: <code>\"_pred\"</code>): The suffix to add to the filenames before the extension.  </li> <li>ext (<code>str</code>, default: <code>\".nii.gz\"</code>): The file extension to search for and rename.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed information about each file being renamed.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate the renaming operation without changing any files.</li> </ul>"},{"location":"API_reference/utils/file_manager/#add_string_files","title":"<code>add_string_files</code>","text":"<p>Add a prefix and/or suffix to all files in a folder and its subfolders.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code> or <code>Path</code>): Directory containing files to rename.  </li> <li>prefix (<code>str</code>, default: <code>\"\"</code>): Prefix to add to the file name (before the stem).  </li> <li>suffix (<code>str</code>, default: <code>\"\"</code>): Suffix to add to the file name (after the stem, before extension).  </li> <li>ext (<code>str</code> or <code>None</code>, optional): If provided, treat this exact string as the file extension (supports multi-part extensions like <code>'.nii.gz'</code>).  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print information about actual renames (only when safe_mode=False).  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate renames and print planned operations (no filesystem changes).</li> </ul>"},{"location":"API_reference/utils/file_manager/#copying-and-moving-operations","title":"Copying and Moving Operations","text":""},{"location":"API_reference/utils/file_manager/#copy_files_by_extension","title":"<code>copy_files_by_extension</code>","text":"<p>Copy all files with a specific extension from one directory to another.</p> <p>Parameters </p> <ul> <li>src_dir (<code>str</code>): The source directory from which to copy files.  </li> <li>dst_dir (<code>str</code>): The destination directory where files will be copied.  </li> <li>ext (<code>str</code>): The file extension to search for and copy (e.g., <code>\".txt\"</code>, <code>\".yaml\"</code>).  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate the operation without making changes.  </li> <li>overwrite (<code>bool</code>, default: <code>False</code>): If True, allow overwriting existing files in the destination directory.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed logs for each file operation.</li> </ul>"},{"location":"API_reference/utils/file_manager/#move_files_to_parent","title":"<code>move_files_to_parent</code>","text":"<p>Move files (optionally filtered by extension) from subdirectories to a specified parent level above their current location.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Root directory where the search will start.  </li> <li>levels_up (<code>int</code>, default: <code>1</code>): Number of parent levels up to move the files.  </li> <li>ext (<code>str</code> or <code>None</code>, optional): File extension to filter by (e.g., <code>\".txt\"</code>).  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed logs for each file move operation.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate the move without actually moving the files.</li> </ul>"},{"location":"API_reference/utils/file_manager/#deletion-operations","title":"Deletion Operations","text":""},{"location":"API_reference/utils/file_manager/#delete_files_by_extension","title":"<code>delete_files_by_extension</code>","text":"<p>Deletes all files with a specific extension in a path and its subdirectories.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): The root directory where the search will start.  </li> <li>ext (<code>str</code>): The file extension of the files to be deleted (e.g., <code>'.nii.gz'</code>).  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed logs for each file deletion operation.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate the deletion without actually removing the files.</li> </ul>"},{"location":"API_reference/utils/file_manager/#delete_dirs_by_pattern","title":"<code>delete_dirs_by_pattern</code>","text":"<p>Deletes folders matching a pattern in a path and its subdirectories.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Directory where the search will start.  </li> <li>pattern (<code>str</code>): Pattern to match folder names.  </li> <li>match_type (<code>str</code>, default: <code>'contains'</code>): Type of matching: <code>'contains'</code>, <code>'starts'</code>, <code>'ends'</code>, or <code>'exact'</code>.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed logs for each folder deletion operation.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate deletion without actually removing folders.</li> </ul>"},{"location":"API_reference/utils/file_manager/#organization-operations","title":"Organization Operations","text":""},{"location":"API_reference/utils/file_manager/#organize_files_into_dirs","title":"<code>organize_files_into_dirs</code>","text":"<p>Organizes files into folders based on their filenames. Each file will be moved into a folder named after the file (excluding the extension).</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Directory containing the files to organize.  </li> <li>extension (<code>str</code>, default: <code>'.nii.gz'</code>): The file extension to look for.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed logs about each file being organized.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate the file organization without moving the files.</li> </ul>"},{"location":"API_reference/utils/file_manager/#organize_subdirs_into_named_dirs","title":"<code>organize_subdirs_into_named_dirs</code>","text":"<p>Organizes subfolders into combined named folders. Combines parent folder names and their subfolder names into a single folder per subfolder.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Directory containing the parent folders.  </li> <li>join_char (<code>str</code>, default: <code>\"-\"</code>): Character to join parent and subfolder names.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, print detailed logs about each operation.  </li> <li>safe_mode (<code>bool</code>, default: <code>True</code>): If True, simulate the folder organization without making changes.</li> </ul> <p>Returns </p> <ul> <li><code>dict[str, list[str]]</code>: Summary of operations performed or simulated.   Keys: <code>\"created_folders\"</code>, <code>\"moved_items\"</code>, <code>\"removed_folders\"</code>.</li> </ul>"},{"location":"API_reference/utils/sequences/","title":"Sequences","text":"<p>The <code>sequences</code> module provides helpers for loading, manipulating, and analyzing NIfTI images and segmentations.</p> <p>Important</p> <p>Some of the functions presented in this module are used internally by AUDIT and are not intended to be used by users.</p>"},{"location":"API_reference/utils/sequences/#io-operations","title":"I/O Operations","text":""},{"location":"API_reference/utils/sequences/#load_nii","title":"<code>load_nii</code>","text":"<p>Load a NIfTI image from disk.</p> <p>This function reads a NIfTI file using SimpleITK. If <code>as_array</code> is True, the image is returned as a NumPy array; otherwise a <code>SimpleITK.Image</code> is returned. If an error occurs while reading, <code>None</code> is returned and a warning is logged.</p> <p>Parameters </p> <ul> <li>path (<code>str</code>): Path to the NIfTI file on disk (e.g., <code>/path/to/scan.nii.gz</code>).  </li> <li>as_array (<code>bool</code>, default: <code>False</code>): If True, return the image as a NumPy array; otherwise return a SimpleITK image.</li> </ul> <p>Returns </p> <ul> <li><code>SimpleITK.Image | np.ndarray | None</code>: The loaded image (<code>SimpleITK.Image</code> or <code>np.ndarray</code>) if successful; otherwise <code>None</code>.</li> </ul>"},{"location":"API_reference/utils/sequences/#load_nii_by_subject_id","title":"<code>load_nii_by_subject_id</code>","text":"<p>Load a specific NIfTI sequence for a subject ID from a dataset tree.</p> <p>This helper builds the expected path <code>{root_dir}/{subject_id}/{subject_id}{seq}.nii.gz</code>,  verifies its existence, and loads it via <code>load_nii</code> optionally as a NumPy array.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Root folder containing all subject subfolders.  </li> <li>subject_id (<code>str</code>): Identifier of the subject (e.g., <code>Patient-001</code>).  </li> <li>seq (<code>str</code>, default: <code>\"_seg\"</code>): Sequence suffix to append to the subject id (e.g., <code>_t1</code>, <code>_flair</code>).  </li> <li>as_array (<code>bool</code>, default: <code>False</code>): If True, return the image as a NumPy array; otherwise return a SimpleITK image.</li> </ul> <p>Returns </p> <ul> <li><code>SimpleITK.Image | np.ndarray | None</code>: The loaded image if found and readable; otherwise <code>None</code>.</li> </ul>"},{"location":"API_reference/utils/sequences/#reading-multiple-sequences","title":"Reading Multiple Sequences","text":""},{"location":"API_reference/utils/sequences/#read_sequences_dict","title":"<code>read_sequences_dict</code>","text":"<p>Read multiple NIfTI sequences for a subject and return them as a dictionary.</p> <p>For each sequence in <code>sequences</code> (defaults to <code>[\"_t1\", \"_t1ce\", \"_t2\", \"_flair\"]</code>), attempts to load <code>{subject_id}{seq}.nii.gz</code> and returns a map from the sequence name without underscore (e.g., <code>\"t1\"</code>) to a NumPy array. Missing or unreadable sequences are returned as <code>None</code>.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Root directory where subject data is stored.  </li> <li>subject_id (<code>str</code>): Subject identifier used to locate the NIfTI files.  </li> <li>sequences (<code>list[str]</code>, optional): Sequence suffixes to load. Defaults to <code>[\"_t1\", \"_t1ce\", \"_t2\", \"_flair\"]</code>.</li> </ul> <p>Returns </p> <ul> <li><code>dict[str, np.ndarray | None]</code>: Mapping from sequence key (without leading underscore) to the loaded array, or <code>None</code> if missing/unreadable.</li> </ul>"},{"location":"API_reference/utils/sequences/#spacing-conversion","title":"Spacing &amp; Conversion","text":""},{"location":"API_reference/utils/sequences/#get_spacing","title":"<code>get_spacing</code>","text":"<p>Get voxel spacing of a SimpleITK image as a NumPy array.</p> <p>If <code>img</code> is <code>None</code>, returns isotropic spacing <code>[1, 1, 1]</code> and logs a warning.</p> <p>Parameters </p> <ul> <li>img (<code>SimpleITK.Image | None</code>): Input image from which to read spacing.</li> </ul> <p>Returns </p> <ul> <li><code>np.ndarray</code>: The spacing vector as <code>(z, y, x)</code>.</li> </ul>"},{"location":"API_reference/utils/sequences/#build_nifty_image","title":"<code>build_nifty_image</code>","text":"<p>Convert a segmentation array into a SimpleITK Image.</p> <p>Parameters </p> <ul> <li>segmentation (<code>np.ndarray | list</code>): Input segmentation array.</li> </ul> <p>Returns </p> <ul> <li><code>SimpleITK.Image</code>: The created SimpleITK image.</li> </ul>"},{"location":"API_reference/utils/sequences/#label-operations","title":"Label Operations","text":""},{"location":"API_reference/utils/sequences/#label_replacement","title":"<code>label_replacement</code>","text":"<p>Map label values in a segmentation from original labels to new labels.</p> <p>Parameters </p> <ul> <li>segmentation (<code>np.ndarray</code>): Segmentation array containing the original label values.  </li> <li>original_labels (<code>list[int]</code>): Original labels present in the segmentation array.  </li> <li>new_labels (<code>list[int]</code>): New labels to replace the original labels.</li> </ul> <p>Returns </p> <ul> <li><code>np.ndarray</code>: A new segmentation array with the remapped labels.</li> </ul>"},{"location":"API_reference/utils/sequences/#iterative_labels_replacement","title":"<code>iterative_labels_replacement</code>","text":"<p>Iteratively replace labels in segmentation files across a dataset tree.</p> <p>Walks the directory tree <code>root_dir</code>, finds files whose names contain <code>ext</code> (e.g., <code>_seg</code> or <code>_pred</code>), loads each file as a 3D array, replaces labels using <code>label_replacement</code>, and writes the modified segmentation back in place.</p> <p>Parameters </p> <ul> <li>root_dir (<code>str</code>): Root directory containing segmentation files.  </li> <li>original_labels (<code>list[int]</code>): Original label values present in the segmentation arrays.  </li> <li>new_labels (<code>list[int]</code>): New labels that will replace the original labels.  </li> <li>ext (<code>str</code>, default: <code>\"_seg\"</code>): File-name pattern used to identify segmentation files.  </li> <li>verbose (<code>bool</code>, default: <code>False</code>): If True, log per-file processing details.</li> </ul>"},{"location":"API_reference/utils/sequences/#analysis-utilities","title":"Analysis Utilities","text":""},{"location":"API_reference/utils/sequences/#count_labels","title":"<code>count_labels</code>","text":"<p>Count the number of pixels/voxels for each unique value in a segmentation.</p> <p>If <code>segmentation</code> is <code>None</code>, return an empty dict, or a dict with NaN values for keys provided by <code>mapping_names</code>.</p> <p>Parameters </p> <ul> <li>segmentation (<code>np.ndarray | None</code>): Segmentation array to count values from.  </li> <li>mapping_names (<code>dict[int, str]</code>, optional): Mapping to rename label IDs (keys) to friendly names (values).</li> </ul> <p>Returns </p> <ul> <li><code>dict[int | str, float]</code>: Counts per unique label (renamed if <code>mapping_names</code> is provided).</li> </ul>"},{"location":"API_reference/utils/sequences/#fit_brain_boundaries","title":"<code>fit_brain_boundaries</code>","text":"<p>Crop a 3D sequence tightly around the non-zero brain region with optional padding.</p> <p>The function computes the bounding box around non-zero voxels and returns the cropped subvolume. If the input is all zeros, the input is returned unchanged.</p> <p>Parameters </p> <ul> <li>sequence (<code>np.ndarray</code>): Input 3D array to crop.  </li> <li>padding (<code>int</code>, default: <code>1</code>): Number of voxels to pad the bounding box on each side.</li> </ul> <p>Returns </p> <ul> <li><code>np.ndarray</code>: Cropped subvolume of <code>sequence</code>.</li> </ul>"},{"location":"about/LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2024 Carlos Aumente Maestro</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"about/release-notes/","title":"\ud83e\uddfe Release Notes \u2013 AUDIT v0.1.0","text":"<p>\ud83d\udcc5 Release date: July 10, 2025</p> <p>We\u2019re excited to announce the first public release of AUDIT, a lightweight and interactive Python tool for  evaluating medical image segmentation models, especially on MRI datasets. This version focuses on enabling fast exploration and quality control with minimal setup, bridging model developers  and researchers.</p>"},{"location":"about/release-notes/#get-involved","title":"\ud83d\udcec Get Involved","text":"<ul> <li>Try it: auditapp.streamlit.app </li> <li>Read the docs: caumente.github.io </li> <li>Contribute or report bugs: github.com/caumente/audit</li> </ul> <p>Thank you for trying AUDIT \u2014 let\u2019s improve medical segmentation evaluation together!</p>"},{"location":"about/release-notes/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<p>Built on top of an amazing open-source ecosystem:</p> <ul> <li>Streamlit \u2013 rapid UIs  </li> <li>Plotly \u2013 interactive graphs  </li> <li>NiBabel \u2013 neuroimaging I/O  </li> <li>ITK-SNAP \u2013 image inspection</li> </ul>"},{"location":"analysis_modes/home_page/","title":"Home page","text":"<p>Welcome to the AUDIT application homepage!</p> <p>The homepage is the entry point to explore the core functionalities of AUDIT.  Currently, the latest version of AUDIT is preloaded with sample data and deployed using Streamlit.  </p> <p>You can access it here: https://auditapp.streamlit.app</p>"},{"location":"analysis_modes/home_page/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the homepage interface:</p> <p></p>"},{"location":"analysis_modes/home_page/#what-youll-find-on-the-homepage","title":"\ud83e\udded What you\u2019ll find on the homepage","text":"<p>Upon opening the app, you will see:</p> <ul> <li> <p>Project title and credits   Includes information about the creators, affiliations, and contributors.</p> </li> <li> <p>Overview and description   A summary of what AUDIT is, what it does, and its main capabilities.</p> </li> <li> <p>Quick access to analysis modes   Buttons or tabs that guide you to various sections such as:</p> </li> <li> <p>Documentation and repository links   At the bottom of the page, you can find links to this documentation and the GitHub repository.</p> </li> </ul> <p>The homepage is designed to give users a quick orientation before jumping into analysis. It serves as a visual hub where users can read the goals of the project, explore capabilities, and navigate easily across different tools within the app.</p> <p>If you're new to AUDIT, we recommend watching the video above and starting with the Univariate Analysis section to explore your first dataset.</p>"},{"location":"analysis_modes/longitudinal/","title":"Longitudinal analysis","text":"<p>The Longitudinal analysis mode in AUDIT allows users to track how lesion sizes evolve over time in individual  subjects. It compares observed (ground truth) and predicted lesion sizes across multiple timepoints, providing insight  into how well a model captures progression or regression trends.</p> <p>The goals of longitudinal analysis are to:</p> <ul> <li>Assess how well a model predicts lesion sizes over time.</li> <li>Evaluate temporal consistency in predictions across multiple scans.</li> <li>Identify systematic over- or under-estimation of lesion progression.</li> <li>Understand patient-specific modeling behavior.</li> <li>Support clinical and research decisions involving disease monitoring.</li> </ul>"},{"location":"analysis_modes/longitudinal/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the longitudinal analysis mode interface:</p> <p></p>"},{"location":"analysis_modes/longitudinal/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/longitudinal/#1-dataset-selection","title":"1. Dataset selection","text":"<p>Users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the longitudinal analysis is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how a segmentation model performs on  a case from a specific dataset.</p> <p>Make sure that: - The dataset contains multiple timepoints per subject. - The label alignment is correct across timepoints (ground truth and predictions must refer to the same anatomical regions). - The same dataset version and preprocessing pipeline are used for all models being analyzed.</p> <p>It is important that the segmentation labels and the ground truth labels are aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it is explained how to modify the  dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>Ensure that all selected models were evaluated using the same dataset version, and that predictions were computed using consistent preprocessing and alignment protocols. Otherwise, performance comparisons may be biased.</p>"},{"location":"analysis_modes/longitudinal/#2-model-selection","title":"2. Model selection","text":"<p>Users must choose one of the available segmentation models to evaluate in this mode. This model must have associated  prediction files, from which performance metrics were precomputed by the AUDIT backend. These precalculated  metrics are then loaded and visualized in the timeline.</p> <p>AUDIT will automatically:</p> <ul> <li>Load precomputed lesion sizes (in mm\u00b3) from the model's prediction files.</li> <li>Compare these with the ground truth lesion sizes.</li> <li>Display both trajectories (predicted and observed) along a timeline.</li> </ul> <p>Tip</p> <p>If you have multiple models available for the same dataset, switching between them is a powerful way to  compare their behavior under the same data conditions and identify strengths or weaknesses specific to each  architecture.</p>"},{"location":"analysis_modes/longitudinal/#3-select-the-subject-id-to-visualize","title":"3. Select the subject ID to visualize","text":"<p>Users have the option to select which subject(s) they want to analyze.</p> <p>Selecting an individual subject ID will display the longitudinal analysis for that single case. This  allows detailed inspection of how the model performed on that particular subject, helping to identify  subject-specific errors or unique segmentation challenges.</p> <p>This flexibility enables both granular, case-by-case evaluation and broad, cohort-level assessment within the same  interface.</p>"},{"location":"analysis_modes/longitudinal/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The longitudinal analysis panel provides a detailed and interactive visualization of how lesion sizes evolve over time  for a selected subject. The x-axis represents the different timepoints (e.g., baseline and follow-up scans), while  the y-axis shows lesion size in cubic millimeters (mm\u00b3). This enables users to inspect the temporal dynamics of lesion  progression or regression, both in the ground truth and in the model predictions.</p> <p> Figure 1: Longitudinal analysis mode in AUDIT. Comparison of observed (orange solid line) and predicted (yellow solid line) lesion sizes across timepoints. Percentage values over the lines indicate tumor growth or shrinkage between consecutive timepoints. Dotted blue lines and their labels show the relative error between observed and predicted lesion sizes at each timepoint.</p> <p>Two solid lines are displayed: the orange line represents the observed lesion sizes (ground truth), and the yellow  line shows the predicted lesion sizes estimated by the selected segmentation model. Each point on these lines  corresponds to a scan at a specific timepoint. The closer the two lines are, the more accurate the model is at  capturing the lesion size for that particular time.</p> <p>Between each pair of consecutive timepoints, percentage values are shown above the orange and yellow lines. These  indicate the relative change in lesion size compared to the previous timepoint\u2014positive values reflect lesion  growth, while negative values represent shrinkage. This allows for an intuitive comparison of how the model interprets  progression or treatment response compared to the actual clinical evolution.</p> <p>To highlight discrepancies between predictions and ground truth, the panel also includes dotted vertical blue lines  connecting the predicted and observed lesion sizes at each timepoint. Next to each line, a blue percentage indicates  the relative error in volume estimation by the model. Larger percentages point to greater divergence from the ground  truth, making it easy to spot timepoints where the model struggles.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/multi_model/","title":"Multi-model performance comparison","text":"<p>The Multi-model performance comparison analysis mode in AUDIT allows users to evaluate and compare a set of segmentation models on a single dataset, across multiple metrics and tumor regions.</p> <p>It provides both a summary table and an interactive boxplot-based visualization, where each box represents the  performance distribution of a model on a specific region and metric. This allows users to intuitively assess which  models perform better in which contexts, identify strengths and weaknesses, and support informed model selection decisions.</p> <p>The purpose of multi-model performance analysis is to:</p> <ul> <li>Compare several segmentation models on the same dataset under identical conditions.</li> <li>Assess performance across multiple tumor regions and evaluation metrics.</li> <li>Identify strengths and weaknesses of each model depending on the region or metric.</li> <li>Visualize inter-model differences in terms of robustness, variability, and outlier behavior.</li> <li>Support model benchmarking, ensemble design, and model selection over different model versions.</li> </ul>"},{"location":"analysis_modes/multi_model/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the segmentation error matrix mode interface:</p> <p></p>"},{"location":"analysis_modes/multi_model/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/multi_model/#1-dataset-selection","title":"1. Dataset selection","text":"<p>In this mode, users can select only one dataset. This ensures a consistent and fair comparison, as all models are  evaluated on the exact same subjects.</p> <p>In this mode, users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the multi-model performance analysis is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how a segmentation model performs on  a specific dataset, ensuring a consistent and fair comparison, as all models are evaluated on the exact same subjects.</p> <p>It is important that the segmentation labels and the ground truth labels must be aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it is explained how to modify the  dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>Ensure that all selected models were evaluated using the same dataset version, and that predictions were computed using consistent preprocessing and alignment protocols. Otherwise, performance comparisons may be biased.</p>"},{"location":"analysis_modes/multi_model/#2-model-selection","title":"2. Model selection","text":"<p>Users can select two or more segmentation models to include in the comparison. These models must have available  prediction files and precomputed evaluation metrics for the selected dataset by AUDIT's backend. Once selected, the  dashboard will automatically load and visualize their performance across the chosen metrics and regions.</p> <p>Each model is represented by a distinct color in the boxplot grid, ensuring easy visual differentiation and making it  straightforward to spot performance differences across metrics and tumor subregions.</p> <p>This mode is especially useful when:</p> <ul> <li>Comparing successive versions of the same model (e.g., v1, v2, v3) where each version incorporates incremental improvements such as new loss functions, or preprocessing pipelines.</li> <li>Conducting ablation studies, where specific components (e.g., attention modules, auxiliary branches, normalization layers) are removed to evaluate their contribution to overall performance.</li> <li>Evaluating model robustness across regions to determine whether improvements are consistent or limited to certain tumor substructures.</li> <li>Benchmarking against baseline methods such as classical segmentation tools or previously published models.</li> </ul> <p>Tip</p> <p>This mode is ideal for researchers and developers who want to track progress over time, validate architectural changes, or prepare comparative figures for scientific publications. Including both baseline and experimental models can help identify trade-offs and guide future improvements.</p>"},{"location":"analysis_modes/multi_model/#3-region-selection","title":"3. Region selection","text":"<p>By default, model performance is shown aggregated by region. This means that both the summary table and each boxplot  represent a model\u2019s average performance across all tumor subregions. This aggregated view is useful to quickly grasp  overall trends and identify which models perform best on average.</p> <p>However, AUDIT also allows users to disaggregate performance by individual regions, offering a more detailed and  fine-grained analysis. When this option is enabled, the boxplots are split by region, and the performance of each model  is shown separately for each tumor subregion.</p> <p>This disaggregated view is particularly important in tumor segmentation tasks, where subregions may differ  significantly in size, intensity, morphology, or clinical relevance. A model may achieve a high average Dice score by  performing well on large, easy-to-segment regions, while still struggling with smaller or more heterogeneous areas,  a pattern that would only become visible when analyzing regions individually.</p> <p>Tip</p> <p>Use the disaggregated view to detect region-specific weaknesses, such as models that consistently underperform on  small enhancing tumor regions or show high boundary error in the tumor core.</p>"},{"location":"analysis_modes/multi_model/#4-metric-selection","title":"4. Metric selection","text":"<p>Users must select a single evaluation metric that quantifies the segmentation quality of the chosen model. As mentioned,  these metrics are precomputed by the AUDIT backend and loaded from disk when the analysis is launched. The selected  metric is plotted on the Y-axis, allowing users to analyze performance relative to the selected feature (X-axis).</p> <p>AUDIT natively supports the following metrics:</p> Metric Description Interpretation <code>dice</code> Dice coefficient; measures the overlap between predicted and ground truth masks. Higher is better <code>jacc</code> Jaccard Index (Intersection over Union); similar to Dice but penalizes false positives more. Higher is better <code>accu</code> Accuracy; proportion of correctly classified voxels across all classes. Higher is better <code>prec</code> Precision; proportion of predicted positives that are true positives (i.e., few false positives). Higher is better <code>sens</code> Sensitivity (Recall); proportion of true positives detected (i.e., few false negatives). Higher is better <code>spec</code> Specificity; proportion of true negatives correctly identified. Higher is better <code>haus</code> Hausdorff distance (95% percentile); measures the worst-case boundary error. Lower is better <code>lesion_size</code> Total volume of the predicted lesion, in mm\u00b3. Application-dependent <p>Each metric offers insight into different aspects of model performance:</p> <ul> <li>Overlap metrics like Dice and Jaccard assess spatial agreement.</li> <li>Threshold metrics like precision and sensitivity are useful for imbalance analysis.</li> <li>Distance metrics (e.g., Hausdorff) reflect boundary accuracy.</li> <li>Lesion size serves as a complementary indicator to understand under- or over-segmentation.</li> </ul> <p>Warning</p> <p>Some metrics (especially <code>dice</code>, <code>haus</code>, or <code>lesion_size</code>) may behave differently across regions with different sizes. Small structures are more sensitive to minor errors, so always interpret values in context.</p>"},{"location":"analysis_modes/multi_model/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The Multi-model performance comparison mode provides two complementary dashboards:</p>"},{"location":"analysis_modes/multi_model/#dashboard-1-summary-table","title":"Dashboard 1: Summary table","text":"<p>At the top of the interface, a summary table displays the mean \u00b1 standard deviation of each selected metric for  all selected models. These values are computed across all subjects, and by default, aggregated over all tumor regions.</p> <p>This compact overview enables users to quickly identify which models perform best on average, and which ones exhibit  higher variability.</p> <p> Figure 1: Summary table showing model performance aggregated over all tumor regions.</p> <p>The \"Aggregated\" checkbox above the table allows switching between showing average scores across all  regions or displaying each region individually.</p> <p> Figure 2: Summary table with the \"Aggregated\" option disabled. Each region is shown separately for more granular analysis.</p>"},{"location":"analysis_modes/multi_model/#dashboard-2-boxplot-visualization","title":"Dashboard 2: Boxplot visualization","text":"<p>Additionally to the table, an interactive boxplot provides a detailed view of the distribution of metric values. This plot visualizes the variability, median, and outliers for each selected metric, grouped by model.</p> <p>By default, the plot shows aggregated values \u2014 i.e., one box per model and metric, summarizing performance across  all tumor regions.</p> <ul> <li>X-axis: Selected metrics (e.g., Dice, Jaccard, Sensitivity)  </li> <li>Y-axis: Metric value  </li> <li>Color: Each model is assigned a distinct color  </li> <li>Boxes: One per model and metric, showing median, quartiles, and outliers</li> </ul> <p> Figure 3: Boxplots showing the distribution of aggregated model performance across metrics. Each color represents a different model.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/multivariate/","title":"Multivariate feature analysis","text":"<p>The Multivariate analysis mode in AUDIT enables users to investigate how multiple features interact with each other across different datasets. It provides a two-dimensional scatter plot where each data point represents a subject, and its position is determined by two selected features. This visualization is particularly useful for detecting complex patterns, identifying correlations, and revealing dataset-specific behaviors or outliers that may not be visible in univariate analyses.</p> <p>The goals of multivariate analysis are to:</p> <ul> <li>Explore relationships between pairs of features across multiple datasets.</li> <li>Detect hidden clusters or trends in the feature space.</li> <li>Identify dataset shifts or feature interactions that might affect model performance.</li> <li>Spot outliers that deviate from the main population.</li> <li>Visually compare cohorts in a shared 2D space.</li> </ul>"},{"location":"analysis_modes/multivariate/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the multivariate interface:</p> <p></p>"},{"location":"analysis_modes/multivariate/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/multivariate/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Users can load and compare multiple datasets simultaneously in the Multivariate analysis mode. This  enables side-by-side inspection of how a given feature varies across different cohorts \u2014 for example,  across different institutions, scanner protocols, or study years.</p> <p>Each dataset is automatically assigned a distinct color in all plots, ensuring an easy interpretation.</p> <p>Datasets can be:</p> <ul> <li>Selected or deselected using the dedicated selector panel on the left-hand side of the dashboard.</li> <li>Toggled directly from the interactive Plotly legend, which allows temporary hiding/showing of datasets with a single click.</li> </ul> <p>While the Plotly legend interaction is useful for quick comparisons, we recommend using the left-side dataset selector  for a more stable and consistent user experience, especially when exporting plots or applying filters.</p>"},{"location":"analysis_modes/multivariate/#2-feature-selection-x-and-y-axes","title":"2. Feature Selection (X and Y axes)","text":"<p>Users must select two features, one for the X-axis and one for the Y-axis, from the available categories:</p> Feature Description Example Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>The combination of different types of features (e.g., texture vs. spatial) can help uncover intricate data patterns or technical biases.</p> <p>Some features depend heavily on the imaging modality, and comparisons between datasets are only meaningful when  extracted from preprocessed MRIs. Texture features in particular are highly sensitive to variations in acquisition  protocols and preprocessing pipelines. This sensitivity arises because descriptors like entropy, contrast, and energy  are affected by voxel size and image spacing, intensity range and quantization strategy, and other technical aspects  related to the scanners. Such technical differences are especially relevant in multi-center studies, where spatial  resolution and image quality may vary substantially.</p> <p>Other features, such as tumor volume, center of mass, or spatial location, are typically derived from segmentation  masks that take into account the voxel spacing, and are independent of intensity information. These are more robust  to differences in acquisition and preprocessing, making them suitable for comparisons even across heterogeneous cohorts.</p> <p>Info</p> <p>More technical details can be found in our publication and within the API reference.</p>"},{"location":"analysis_modes/multivariate/#3-color-feature","title":"3. Color Feature","text":"<p>In addition to selecting the X and Y axes, users can define a third variable to control the color of each data point in  the scatter plot. This feature enables a richer multivariate exploration by adding a visual encoding of an additional  dimension, effectively transforming the 2D plot into a pseudo-3D representation.</p> <p>The available options include:</p> <ul> <li> <p>Dataset (default): Colors each point based on the dataset it belongs to. This is particularly useful when      comparing cohorts, such as data from different hospitals, protocols, or population groups. Patterns of separation,      overlap, or clustering between datasets can provide insight into dataset shifts or cohort-specific trends.</p> </li> <li> <p>Any other available feature: Users may also choose to color points based on the value of a specific feature, such      as tumor volume, center of mass, texture entropy, or any other scalar variable present in the dataset. This allows      researchers to visually explore how a third variable distributes over the 2D feature space and whether it aligns      with certain regions, clusters, or gradients.</p> </li> </ul> <p>Colorbars are dynamically generated when using feature-based coloring, providing a reference scale for  interpretation. Continuous variables use a sequential or diverging colormap, while categorical features are  represented using distinct colors.</p> <p>Tip</p> <p>When coloring by a continuous feature, watch for visual patterns such as localized extremes. These often indicate a  non-trivial interaction between the color feature and the selected X/Y pair, which may warrant deeper investigation  or stratified modeling approaches.</p>"},{"location":"analysis_modes/multivariate/#4-highlight-subject","title":"4. Highlight Subject","text":"<p>The Highlight subject option allows users to trace the behavior of an individual patient (or subject) across  plots. By entering a specific subject ID and specifying the corresponding dataset, the selected subject will be  visually highlighted in the dashboard, where the subject appears as a clearly marked point (e.g., with a  different color or shape).</p> <p>This feature is particularly valuable in contexts such as detecting whether a subject is an outlier or aligns with cohort  expectations or verifying how individuals from different cohorts are distributed in shared feature space.</p> <p>Tip</p> <p>When analyzing extreme values, this feature can help determine whether a subject truly deviates from the cohort or  lies within natural variability.</p>"},{"location":"analysis_modes/multivariate/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The Multivariate analysis mode displays an interactive 2D scatter plot:</p> <p> Figure 1: A scatter plot showing the relationship between two selected features across datasets. Each point represents a subject and is colored by dataset.</p> <p> Figure 2: A scatter plot showing the relationship between three selected features across datasets. X-axis and Y-axis represent Kurtosis and     Skewness fo FLAIR sequence intensity pixels. Color represents tumor location for enhancing region.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/pairwise_model/","title":"Pairwise model performance comparison","text":"<p>The Pairwise model performance comparison analysis mode in AUDIT enables users to directly compare two segmentation  models by quantifying the improvement in performance between them. This mode provides intuitive visualizations that  highlight where one model outperforms another, helping researchers understand relative model strengths, identify  systematic differences, and make evidence-based decisions about model selection and deployment.</p> <p>It provides a set of region-based barplot plots where each one represents a subject, placed according to:</p> <ul> <li>X-axis: A difference in performance between models (e.g., relative Dice, absolute Hausdorff distance, etc.)</li> <li>Y-axis: Regions of interest (e.g. necrotic, edema, etc.)</li> </ul> <p>The purpose of pairwise model performance comparison is to:</p> <ul> <li>Compare two segmentation models head-to-head on the same dataset.</li> <li>Quantify performance differences using multiple comparison metrics (absolute, relative, ratio).</li> <li>Identify which regions show the greatest improvement between models.</li> <li>Visualize subject-level variability in model performance differences.</li> <li>Support statistical validation of performance differences through hypothesis testing.</li> <li>Guide model selection by revealing systematic strengths and weaknesses.</li> </ul>"},{"location":"analysis_modes/pairwise_model/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the pairwise model performance comparison mode interface:</p> <p></p>"},{"location":"analysis_modes/pairwise_model/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/pairwise_model/#1-dataset-selection","title":"1. Dataset selection","text":"<p>In this mode, users can select only one dataset. This ensures a consistent and fair comparison, as all models are  evaluated on the exact same subjects.</p> <p>In this mode, users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the pair-wise model performance analysis is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how two segmentation models perform on  a specific dataset, ensuring a consistent and fair comparison, as all models are evaluated on the exact same subjects.</p> <p>It is important that the segmentation labels and the ground truth labels must be aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it  is explained how to modify the dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>Ensure that all selected models were evaluated using the same dataset version, and that predictions were computed using consistent preprocessing and alignment protocols. Otherwise, performance comparisons may be biased.</p>"},{"location":"analysis_modes/pairwise_model/#2-model-selection","title":"2. Model selection","text":"<p>Users must select two models for comparison:</p> <ul> <li>Baseline model: The reference model against which improvements are measured</li> <li>Benchmark model: The comparison model being evaluated for improvement</li> </ul> <p>The choice of which model serves as baseline versus benchmark affects the interpretation of results:</p> <ul> <li>Positive values indicate the benchmark model performs better than the baseline. </li> <li>Negative values indicate the baseline model performs better than the benchmark</li> </ul> <p>Green bars indicate improvement (benchmark &gt; baseline), orange bars indicate decline (benchmark &lt; baseline)</p> <p>Tip</p> <p>When comparing an established model against a new architecture, typically the established model serves as the  baseline and the new model as the benchmark. This framing makes positive improvements easy to identify.</p>"},{"location":"analysis_modes/pairwise_model/#3-metric-selection","title":"3. Metric selection","text":"<p>Users must select a single evaluation metric that quantifies the segmentation quality of the chosen model. As mentioned,  these metrics are precomputed by the AUDIT backend and loaded from disk when the analysis is launched. The selected  metric is plotted on the X-axis.</p> <p>AUDIT natively supports the following metrics:</p> Metric Description Interpretation <code>dice</code> Dice coefficient; measures the overlap between predicted and ground truth masks. Higher is better <code>jacc</code> Jaccard Index (Intersection over Union); similar to Dice but penalizes false positives more. Higher is better <code>accu</code> Accuracy; proportion of correctly classified voxels across all classes. Higher is better <code>prec</code> Precision; proportion of predicted positives that are true positives (i.e., few false positives). Higher is better <code>sens</code> Sensitivity (Recall); proportion of true positives detected (i.e., few false negatives). Higher is better <code>spec</code> Specificity; proportion of true negatives correctly identified. Higher is better <code>haus</code> Hausdorff distance (95% percentile); measures the worst-case boundary error. Lower is better <code>lesion_size</code> Total volume of the predicted lesion, in mm\u00b3. Application-dependent <p>Each metric offers insight into different aspects of model performance:</p> <ul> <li>Overlap metrics like Dice and Jaccard assess spatial agreement.</li> <li>Threshold metrics like precision and sensitivity are useful for imbalance analysis.</li> <li>Distance metrics (e.g., Hausdorff) reflect boundary accuracy.</li> <li>Lesion size serves as a complementary indicator to understand under- or over-segmentation.</li> </ul> <p>Warning</p> <p>Some metrics (especially <code>dice</code>, <code>haus</code>, or <code>lesion_size</code>) may behave differently across regions with different sizes. Small structures are more sensitive to minor errors, so always interpret values in context.</p>"},{"location":"analysis_modes/pairwise_model/#4-type-of-comparison","title":"4. Type of comparison","text":"<p>AUDIT provides three different ways to quantify the improvement between models, each offering different perspectives on performance differences:</p>"},{"location":"analysis_modes/pairwise_model/#1-absolute-improvement","title":"1. Absolute improvement","text":"\\[\\text{Absolute} = M_{\\text{benchmark}} - M_{\\text{baseline}}\\] <p>Measures the raw difference in metric values. For example, if the baseline Dice is 0.75 and the benchmark is 0.80,  the absolute improvement is +0.05.</p> <p>It is best for understanding the magnitude of improvement in the original metric units. Simple and intuitive since it directly  shows how much the metric changed.</p>"},{"location":"analysis_modes/pairwise_model/#2-relative-improvement","title":"2. Relative improvement","text":"\\[\\text{Relative} = \\frac{M_{\\text{benchmark}} - M_{\\text{baseline}}}{M_{\\text{baseline}}} \\times 100\\] <p>Measures improvement as a percentage of the baseline performance. For example, if the baseline Dice is 0.75 and the  benchmark is 0.80, the relative improvement is +6.67%.</p> <p>It is best for comparing improvements across different baseline performance levels. Accounts for the difficulty of  improvement (harder to improve from 0.90 than from 0.50).</p>"},{"location":"analysis_modes/pairwise_model/#3-ratio","title":"3. Ratio","text":"\\[\\text{Ratio} = \\frac{M_{\\text{benchmark}}}{M_{\\text{baseline}}}\\] <p>Measures the ratio of benchmark to baseline performance. Values greater than 1 indicate improvement.</p> <p>It is best for understanding multiplicative relationships between model performances. A ratio of 1.10 means the  benchmark is 10% better than the baseline</p> <p>Tip</p> <p>Relative improvement is often most informative for comparing models across different datasets or regions, as it accounts for varying difficulty levels. Use absolute improvement when you need to report changes in clinically meaningful units.</p>"},{"location":"analysis_modes/pairwise_model/#5-aggregation-mode","title":"5. Aggregation mode","text":"<p>Users can toggle between two views using the Aggregated checkbox:</p>"},{"location":"analysis_modes/pairwise_model/#1-disaggregated-view","title":"1. Disaggregated view","text":"<p>Shows performance differences for each subject individually, with bars representing different anatomical regions. This  subject-level view reveals which subjects benefit most from the benchmark model,whether improvements are consistent  across subjects, and help discovering subject-specific patterns that may be hidden in averages</p>"},{"location":"analysis_modes/pairwise_model/#2-aggregated-view","title":"2. Aggregated view","text":"<p>Shows average performance differences across all subjects, summarized by anatomical region. This cohort-level view  reveals hich regions show the greatest overall improvement, whether improvements are statistically significant, and the general trend across the entire dataset</p> <p>Info</p> <p>The disaggregated view is particularly useful for identifying outlier subjects or understanding variability, while  the aggregated view provides a clearer picture of overall model performance.</p>"},{"location":"analysis_modes/pairwise_model/#visualizations","title":"\ud83d\udcca Visualizations","text":""},{"location":"analysis_modes/pairwise_model/#1-subject-level-comparison-disaggregated-view","title":"1. Subject-level comparison (disaggregated view)","text":"<p>In the disaggregated view, each subject is displayed with a horizontal bar chart showing performance differences across  anatomical regions.</p> <p> Figure 1: Subject-level comparison showing performance differences across anatomical regions for individual subjects. Green bars indicate improvement by the benchmark model, while orange bars indicate decline.</p> <p>Each bar chart includes:</p> <ul> <li>Subject ID and key metadata (lesion location, lesion size)</li> <li>Average baseline and benchmark performance for context</li> <li>Color coding: Green bars indicate improvement (benchmark &gt; baseline), orange bars indicate decline (benchmark &lt; baseline)</li> <li>Multiple regions per subject, enabling region-specific analysis</li> </ul> <p>Sorting options: Users can sort subjects in ascending or descending order by:</p> <ul> <li>Performance of baseline model</li> <li>Performance of benchmark model</li> <li>Subject ID</li> </ul> <p>Maximum subjects: Users can limit the number of subjects displayed to focus on the most relevant cases  (e.g., top 10 subjects with greatest improvement).</p> <p>Clipping: Users can clip extreme values to focus on the typical range of improvements, preventing outliers from  dominating the visualization.</p> <p>Tip</p> <p>Sort by improvement magnitude in descending order to quickly identify subjects where the benchmark model provides  the greatest benefit. This can guide case-specific analysis or help identify what characteristics make certain  subjects easier or harder for each model.</p>"},{"location":"analysis_modes/pairwise_model/#cohort-level-comparison-aggregated-view","title":"Cohort-level comparison (aggregated view)","text":"<p>In the aggregated view, performance differences are averaged across all subjects and displayed as a single bar chart  with one bar per anatomical region.</p> <p> Figure 2: Aggregated comparison showing average performance differences across anatomical regions for the entire cohort. Each bar represents the mean improvement or decline for that region.</p> <p>The aggregated bar chart shows: - Average improvement for each anatomical region - Color coding: Green bars indicate overall improvement, orange bars indicate overall decline - Region-level insights: Which tumor subregions benefit most from the benchmark model</p> <p>Download option: The aggregated plot can be downloaded as a high-resolution image for reports or publications.</p> <p>Statistical testing: Users can enable statistical hypothesis testing to determine whether observed differences are  statistically significant.</p> <p>Warning</p> <p>Aggregated results can hide important subject-level variability. Always inspect the disaggregated view to ensure  that apparent improvements are consistent across subjects and not driven by a small subset of cases.</p>"},{"location":"analysis_modes/pairwise_model/#statistical-testing","title":"\ud83e\uddea Statistical testing","text":"<p>AUDIT provides built-in statistical hypothesis testing to validate whether observed performance differences between  models are statistically significant or could have occurred by chance.</p> <p>When in aggregated view, users can check the \"Perform statistical test\" option. This triggers an automated analysis  pipeline that:</p> <ol> <li>Tests parametric assumptions: Normality and homoscedasticity. Independence is assumed.</li> <li>Selects appropriate test: Paired t-test or Wilcoxon signed-rank test</li> <li>Reports results: p-value and interpretation</li> </ol>"},{"location":"analysis_modes/pairwise_model/#1-parametric-assumptions","title":"1. Parametric assumptions","text":"<p>Before performing the comparison test, AUDIT automatically checks whether the data meet the assumptions required for  parametric testing:</p> <p>Normality test: Tests whether the performance distributions for both models follow a normal distribution using the Shapiro-Wilk or Lilliefors test.</p> <ul> <li>Null hypothesis: The data are normally distributed</li> <li>Result: Displays p-value and whether normality assumption is met</li> </ul> <p>For each model, AUDIT displays the normality test results in a table and a histogram showing the distribution of performance values</p> <p> Figure 3: Normality test performed for baseline and benchmark models. </p> <p>Homoscedasticity test: Tests whether the two models have equal variances using Levene's test.</p> <ul> <li>Null hypothesis: The variances are equal</li> <li>Result: Displays whether homoscedasticity assumption is met</li> </ul>"},{"location":"analysis_modes/pairwise_model/#2-hypothesis-testing","title":"2. Hypothesis testing","text":"<p>Based on the results of the parametric assumptions tests, AUDIT automatically selects the appropriate statistical test:</p> <p>Paired Student's t-test (Parametric): Used when both samples are normally distributed and have equal variances. It  is appropriate for data meeting the parametric assumptions. </p> <p>Wilcoxon signed-rank test (Non-parametric): Used when the data violate normality or homoscedasticity assumptions. </p>"},{"location":"analysis_modes/pairwise_model/#3-report-results","title":"3. Report results","text":"<p>The statistical test produces:</p> <ul> <li>p-value: The probability of observing the data if there were truly no difference between models</li> <li>Interpretation: A plain-language summary of the result</li> </ul> <p> Figure 4: Statistical significance test performed on baseline and benchmark models. </p> <p>Warning</p> <p>By default, AUDIT uses a significance threshold of \u03b1 = 0.05. P-values below this threshold indicate statistically  significant differences. Users can download the complete dataset used for statistical testing, so we encourage them to perform their own analysis.</p> <p>All plots are interactive, powered by Plotly, allowing users to: - Toggle regions on and off - Zoom and pan within plots - Export high-resolution images - Hover to inspect exact values</p> <p>Whether you're validating a new model architecture, debugging unexpected results, or preparing results for publication,  the pairwise model performance comparison mode provides the statistical rigor and visual clarity needed to make  evidence-based decisions about model performance.</p>"},{"location":"analysis_modes/segmentation_error/","title":"Segmentation error matrix","text":"<p>The Segmentation Error Matrix analysis in AUDIT provides an intuitive way to assess model performance at the pixel level. It visualizes the confusion matrix of predicted vs. ground truth segmentations for each  subject or aggregated across a dataset. This helps identify common misclassifications between tumor  subregions and assess overall segmentation quality.</p> <p>The goals of segmentation error matrix analysis are to:</p> <ul> <li>Visualize systematic errors across a dataset or individual subject.</li> <li>Assess prediction quality for individual tumor subregions, identifying which classes are most accurately segmented and which are often confused.</li> <li>Spot model failure modes not captured by summary metrics like Dice.</li> </ul>"},{"location":"analysis_modes/segmentation_error/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the segmentation error matrix mode interface:</p> <p></p>"},{"location":"analysis_modes/segmentation_error/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/segmentation_error/#1-dataset-selection","title":"1. Dataset selection","text":"<p>In this mode, users can select a single dataset from the dropdown in the sidebar. Unlike other modes in AUDIT (such as  the Multivariate analysis mode), the segmentation error matrix is not designed for multi-dataset comparison within the  same view. Instead, its purpose is to provide a focused and detailed evaluation of how a segmentation model performs on  a specific dataset by analyzing which tumor subregions are well segmented and which are commonly misclassified.</p> <p>It is important that the segmentation labels and the ground truth labels must be aligned properly; otherwise, the  analysis mode will not work correctly. You can follow this tutorial where it is explained how to modify the  dataset labels to ensure proper alignment.</p> <p>Warning</p> <p>This analysis mode compares the predicted segmentations directly against the ground truth labels. Therefore, it can only be used when the original ground truth segmentations are available in the dataset. If the dataset only contains predictions and lacks reference segmentations, the confusion matrix cannot be computed.</p>"},{"location":"analysis_modes/segmentation_error/#2-model-selection","title":"2. Model selection","text":"<p>In this step, users must select the segmentation model whose predicted outputs will be compared against the ground  truth segmentations of the chosen dataset. This is crucial because the confusion matrix is computed by analyzing the  pixel-wise agreement (or disagreement) between the model\u2019s predictions and the ground truth labels.</p> <p>The dropdown menu lists all available models that have generated predictions for the selected dataset. Selecting a  model will load its prediction masks, which are then aligned with the ground truth segmentations to compute the  pseudo-confusion matrix. Users can compare multiple models by changing this selection and observing differences in the  matrices, which helps identify strengths and weaknesses in segmentation performance at the class level.</p> <p>Models might differ in terms of architecture, training data, or preprocessing pipelines, so this step allows for  flexible evaluation of any medical image segmentation model whose outputs are available in AUDIT.</p>"},{"location":"analysis_modes/segmentation_error/#3-select-the-subject-id-to-visualize","title":"3. Select the subject ID to visualize","text":"<p>Users have the option to select which subject(s) they want to analyze through the confusion matrix:</p> <ul> <li> <p>Specific subject: Selecting an individual subject ID will display the confusion matrix for that single case. This      allows detailed inspection of how the model performed on that particular subject, helping to identify      subject-specific errors or unique segmentation challenges.</p> </li> <li> <p>All: Choosing All aggregates the confusion matrices of every subject in the dataset into a single summary     matrix. This aggregated view provides an overview of the model\u2019s overall common error patterns across the entire     cohort, smoothing out subject-level variability.</p> </li> </ul> <p>This flexibility enables both granular, case-by-case evaluation and broad, cohort-level assessment within the same  interface.</p>"},{"location":"analysis_modes/segmentation_error/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The pseudo-confusion matrix shown in this mode is structured as follows:</p> <ul> <li>Rows represent the ground truth (true) labels, indicating the actual pixel classifications.  </li> <li>Columns represent the predicted labels output by the segmentation model, indicating the model\u2019s pixel-wise classification.   </li> <li>Diagonal cells correspond to correctly classified pixels, where the predicted label matches the true label. Not shown, the matrix highlights only errors.</li> <li>Off-diagonal cells highlight misclassifications, where the predicted label differs from the true label.  </li> </ul> <p>Info</p> <p>Darker colors in the matrix represent larger errors or higher misclassification rates by the model, while lighter cells indicate fewer errors or better agreement between prediction and ground truth.  </p> <p>The confusion matrix is normalized row-wise, meaning each row sums to 100%, which helps interpret the prediction  distribution per true label. Since this is a pseudo-confusion matrix, pixels correctly classified are not indicated  within the matrix. It is expected that the model classifies most of the regions correctly, so this analysis mode focuses  on systematic errors instead.</p> <p>Example</p> <p>For example, if the true label is \"edema\", the cells might be distributed as follows: - A value of 70% in the \"bkg\" column means 70% of edema pixels were wrongly predicted as background. - 25% in the \"enh\" column means 25% of edema pixels were wrongly predicted as enhancing tumor. - 5% in the \"nec\" column means 5% of edema pixels were wrongly predicted as necrosis.</p> <p> Figure 1: Single subject confusion matrix for the nnUNet model on the BraTS2024_SSA dataset. The rows represent the true pixel labels. The columns represent the model's predicted labels. Darker cells indicate a higher percentage of misclassified pixels between true and predicted classes.</p>"},{"location":"analysis_modes/segmentation_error/#additional-options","title":"\ud83e\uddf0 Additional options","text":""},{"location":"analysis_modes/segmentation_error/#normalization-and-averaging","title":"Normalization and averaging","text":"<ul> <li> <p>Normalized per ground truth label (enabled by default): normalizes each row so percentages sum to 100%, which     makes it easier to interpret class-wise prediction behavior. When this button is disabled cells show the total     number of misclassified pixels.</p> </li> <li> <p>Averaged per number of subjects (only available when \"All\" subjects are selected): computes the average     pseudo-confusion matrix across all subjects.</p> </li> </ul> <p>Warning</p> <p>Be aware that the absolute pixel count of misclassified regions can be influenced by the voxel spacing of each MRI scan. For this reason, it is highly recommended to register all datasets to a common reference space, so that voxel  sizes and orientations are standardized. Otherwise, cross-dataset comparisons can become difficult to interpret.</p>"},{"location":"analysis_modes/segmentation_error/#itk-snap-integration","title":"ITK-SNAP integration","text":"<p>When a single subject is selected, users can launch ITK-SNAP to directly inspect the segmentation results in 3D. This launches the ground truth and predicted segmentations side by side.</p> <p>Tip</p> <p>This feature is ideal for verifying segmentation quality when unusual confusion patterns appear in the matrix. It  can reveal issues like label swapping, partial segmentations, or preprocessing errors.</p> <p>Warning</p> <p>ITK-SNAP must be installed and correctly configured in your system for this option to work.</p> <p>All confusion matrices in AUDIT are fully interactive and support:</p> <ul> <li>Hover to reveal exact percentages</li> <li>Dynamic updates when selecting different datasets, models, or subjects</li> </ul> <p>Whether you're debugging segmentation behavior or reporting evaluation results, this tool provides a clear and  quantitative view into model errors and class-wise performance.</p>"},{"location":"analysis_modes/single_model/","title":"Single model performance","text":"<p>The Single model performance analysis mode in AUDIT allows users to evaluate how a given segmentation model performs across multiple datasets based on a chosen feature and evaluation metric.</p> <p>It provides a univariate scatter plot where each point represents a subject, placed according to: - X-axis: A selected feature (e.g., maximum intensity in T1, tumor volume, etc.) - Y-axis: A performance metric (e.g., Dice, Hausdorff distance, etc.)</p> <p>This enables intuitive exploration of how specific image characteristics relate to model performance, helping identify  trends, outliers, or systematic failures.</p> <p>The purpose of single model performance analysis is to:</p> <ul> <li>Assess model performance in relation to a single image-derived feature.</li> <li>Compare how the same model behaves across multiple datasets or cohorts.</li> <li>Identify systematic failure patterns linked to specific feature ranges (e.g., intensity, volume).</li> <li>Detect outlier cases where the model performs unexpectedly well or poorly.</li> <li>Understand how specific features relate to performance, to guide model improvement and debugging.</li> </ul>"},{"location":"analysis_modes/single_model/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the segmentation error matrix mode interface:</p> <p></p>"},{"location":"analysis_modes/single_model/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/single_model/#1-dataset-selection","title":"1. Dataset selection","text":"<p>Users can load and compare multiple datasets simultaneously in the Single model performance analysis mode. This  enables side-by-side inspection of how models perform across different cohorts \u2014 for example,  across different institutions, scanner protocols, or study years.</p> <p>Each dataset is automatically assigned a distinct color in all plots, ensuring an easy interpretation.</p> <p>Datasets can be:</p> <ul> <li>Selected or deselected using the dedicated selector panel on the left-hand side of the dashboard.</li> <li>Toggled directly from the interactive Plotly legend, which allows temporary hiding/showing of datasets with a single click.</li> </ul> <p>While the Plotly legend interaction is useful for quick comparisons, we recommend using the left-side dataset selector  for a more stable and consistent user experience, especially when exporting plots or applying filters.</p> <p>Warning</p> <p>To ensure meaningful comparisons, all datasets should ideally be registered to the same reference space and preprocessed in a consistent way. Otherwise, differences in voxel spacing, intensity scaling, or orientation may introduce bias in both features and performance metrics.</p>"},{"location":"analysis_modes/single_model/#2-model-selection","title":"2. Model selection","text":"<p>Users must choose one of the available segmentation models to evaluate in this mode. This model must have associated  prediction files, from which performance metrics were precomputed by the AUDIT backend. These precalculated  metrics are then loaded and visualized in the scatter plot.</p> <p>The selected model determines which performance values are shown on the Y-axis, one point per subject. This allows  users to inspect how the model performs across subjects, in relation to a selected feature on the X-axis.</p> <p>Tip</p> <p>If you have multiple models available for the same dataset, switching between them is a powerful way to  compare their behavior under the same data conditions and identify strengths or weaknesses specific to each  architecture.</p>"},{"location":"analysis_modes/single_model/#3-metric-selection","title":"3. Metric selection","text":"<p>Users must select a single evaluation metric that quantifies the segmentation quality of the chosen model. As mentioned,  these metrics are precomputed by the AUDIT backend and loaded from disk when the analysis is launched. The selected  metric is plotted on the Y-axis, allowing users to analyze performance relative to the selected feature (X-axis).</p> <p>AUDIT natively supports the following metrics:</p> Metric Description Interpretation <code>dice</code> Dice coefficient; measures the overlap between predicted and ground truth masks. Higher is better <code>jacc</code> Jaccard Index (Intersection over Union); similar to Dice but penalizes false positives more. Higher is better <code>accu</code> Accuracy; proportion of correctly classified voxels across all classes. Higher is better <code>prec</code> Precision; proportion of predicted positives that are true positives (i.e., few false positives). Higher is better <code>sens</code> Sensitivity (Recall); proportion of true positives detected (i.e., few false negatives). Higher is better <code>spec</code> Specificity; proportion of true negatives correctly identified. Higher is better <code>haus</code> Hausdorff distance (95% percentile); measures the worst-case boundary error. Lower is better <code>lesion_size</code> Total volume of the predicted lesion, in mm\u00b3. Application-dependent <p>Each metric offers insight into different aspects of model performance:</p> <ul> <li>Overlap metrics like Dice and Jaccard assess spatial agreement.</li> <li>Threshold metrics like precision and sensitivity are useful for imbalance analysis.</li> <li>Distance metrics (e.g., Hausdorff) reflect boundary accuracy.</li> <li>Lesion size serves as a complementary indicator to understand under- or over-segmentation.</li> </ul> <p>Warning</p> <p>Some metrics (especially <code>dice</code>, <code>haus</code>, or <code>lesion_size</code>) may behave differently across regions with different sizes. Small structures are more sensitive to minor errors, so always interpret values in context.</p>"},{"location":"analysis_modes/single_model/#4-feature-selection","title":"4. Feature selection","text":"<p>Users must choose a feature to use for the X-axis. Features are extracted from the images and segmentations and  can belong to various categories:</p> Feature Description Example Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>Some features depend heavily on the imaging modality, and comparisons between datasets are only meaningful when  extracted from preprocessed MRIs. Texture features in particular are highly sensitive to variations in acquisition  protocols and preprocessing pipelines. This sensitivity arises because descriptors like entropy, contrast, and energy  are affected by voxel size and image spacing, intensity range and quantization strategy, and other technical aspects  related to the scanners. Such technical differences are especially relevant in multi-center studies, where spatial  resolution and image quality may vary substantially.</p> <p>Other features, such as tumor volume, center of mass, or spatial location, are typically derived from segmentation  masks that take into account the voxel spacing, and are independent of intensity information. These are more robust  to differences in acquisition and preprocessing, making them suitable for comparisons even across heterogeneous cohorts.</p> <p>Info</p> <p>More technical details can be found in our publication and within the API reference.</p>"},{"location":"analysis_modes/single_model/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The single model performance dashboard displays an interactive 2D scatter plot in which each point represent a subject. Each dot corresponds to a subject, positioned by its feature value (X-axis) and performance  metric (Y-axis), and colored by dataset.</p> <p> Figure 1: A scatter plot showing the relationship between a selected features and metric across datasets. Each point represents a subject and is colored by dataset.</p> <p>Info</p> <p>The \u201cAggregated\u201d checkbox allows toggling between: - Aggregated mode: Each point summarizes the subject across all regions. - Disaggregated mode: The plot is faceted by region, showing one subplot per tumor region.</p>"},{"location":"analysis_modes/single_model/#additional-options","title":"\ud83e\uddf0 Additional options","text":""},{"location":"analysis_modes/single_model/#highlighting-points","title":"Highlighting points","text":"<p>Users can double-click on a point to highlight a specific subject in red. Highlighted points are preserved across views and can be reset by clicking the Reset highlighted cases button.</p> <p>This is useful for inspecting specific subjects with particularly low or high performance. Understanding why some cases  are easier (or harder) to segment can be crucial to improving model generalization capabilities.</p> <p> Figure 2: A scatter plot showing the relationship between a selected features and metric across datasets. Each point represents a subject and is colored by dataset. Notice how there are two specific subjects highlighted in red to monitor their performance.</p>"},{"location":"analysis_modes/single_model/#aggregating","title":"Aggregating","text":"<p>Users can switch between visualizing the subjects in an aggregated or disaggregated view. This is particularly useful to understand which subregions are more challenging to segment.</p> <p>Additionally, it helps understand how different tumor regions contribute to the overall  performance distribution. For example, a model might appear robust in aggregated view but show poor performance on  specific subregions when disaggregated.</p> <p>In aggregated view, performance metrics are computed across the entire segmentation mask (all regions combined). In disaggregated view, the plot is faceted by region, and each subplot shows region-specific performance.</p> <p>Tip</p> <p>Use the highlighting option to mark the patients you are interested in, and then switch to the disaggregated view. This will show how your selected cases perform across different tumor subregions, helping reveal region-specific issues or inconsistencies.</p> <p> Figure 3: A disaggregated view scatter plot showing the relationship between a selected features and metric across datasets. Each point represents a subject and is colored by dataset. Notice how there are two specific subjects highlighted in red to monitor their performance.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/subjects_exploration/","title":"Subject exploration","text":"<p>The Subject exploration analysis mode in AUDIT provides a detailed view of individual subjects, allowing users to inspect  their feature profiles and determine whether they represent outliers within their cohort. This mode supports targeted  investigation of specific cases, helping researchers understand subject-level variability and identify atypical  patterns that may warrant further investigation or quality control.</p> <p>The purpose of subject exploration is to:</p> <ul> <li>Inspect individual subject characteristics across all extracted features.</li> <li>Identify outlier subjects using statistical detection methods.</li> <li>Compare subject values against cohort statistics (median, mean, standard deviation).</li> <li>Support quality control by flagging subjects with unusual feature profiles.</li> <li>Enable targeted case review for subjects with unexpected model performance or clinical presentation.</li> </ul>"},{"location":"analysis_modes/subjects_exploration/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the subject exploration mode interface:</p> <p></p>"},{"location":"analysis_modes/subjects_exploration/#user-configuration","title":"\u2699\ufe0f User Configuration","text":""},{"location":"analysis_modes/subjects_exploration/#1-dataset-selection","title":"1. Dataset selection","text":"<p>Users must first select a single dataset from which to explore individual subjects. Unlike other analysis modes that  support multi-dataset comparison, subject exploration focuses on understanding individuals within a specific  cohort context.</p> <p>The selected dataset determines: - Which subjects are available for inspection - The reference population used for outlier detection - The statistical benchmarks (median, mean, standard deviation) displayed for comparison</p> <p>Warning</p> <p>To ensure meaningful comparisons, all datasets should ideally be registered to the same reference space and preprocessed in a consistent way. Otherwise, differences in voxel spacing, intensity scaling, or orientation may introduce bias in both features and performance metrics.</p>"},{"location":"analysis_modes/subjects_exploration/#2-subject-selection","title":"2. Subject selection","text":"<p>After selecting a dataset, users choose a specific subject ID to inspect. The interface displays all subjects  available within the selected dataset, and users can select one from a dropdown menu.</p> <p>Once selected, the dashboard displays a complete feature profile for that subject across all feature categories,  including statistical comparison against the rest of the cohort and an outlier status for each feature (determined  using the IQR method).</p> <p>This mode is particularly useful when: - Investigating subjects flagged in other analysis modes (e.g., poor model performance) - Performing quality control on newly acquired data - Understanding why certain subjects behave differently than expected</p>"},{"location":"analysis_modes/subjects_exploration/#visualizations","title":"\ud83d\udcca Visualizations","text":""},{"location":"analysis_modes/subjects_exploration/#subject-information","title":"Subject information","text":"<p>This section provides a comprehensive overview of the selected subject's feature values, organized by feature category for clarity.</p> <p>Features are displayed in separate tables, one for each category:</p> Feature Category Description Example Features Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>Each table shows:</p> <ul> <li>Feature name: The specific feature being measured</li> <li>Value: The subject's value for that feature</li> </ul> <p>This organization makes it easy to quickly scan for unusual values or patterns within specific feature domains.</p> <p>Info</p> <p>More technical details about feature extraction can be found in our publication  and within the API reference.</p>"},{"location":"analysis_modes/subjects_exploration/#iqr-outlier-detection","title":"IQR outlier detection","text":"<p>The IQR (Interquartile Range) outlier detection section automatically identifies whether the selected subject exhibits  outlier values for any extracted feature, relative to the rest of the cohort.</p>"},{"location":"analysis_modes/subjects_exploration/#1-detection-method","title":"1. Detection method","text":"<p>The IQR method defines outliers based on the spread of the middle 50% of the data:</p> <ol> <li>Calculate Q1 (25th percentile) and Q3 (75th percentile) for each feature across the cohort</li> <li>Compute IQR = Q3 - Q1</li> <li>Define outlier bounds:</li> <li>Lower bound = Q1 - (deviation \u00d7 IQR)</li> <li>Upper bound = Q3 + (deviation \u00d7 IQR)</li> <li>Flag the subject if its value falls outside these bounds</li> </ol> <p>By default, the deviation multiplier is set to 1.5 (standard outliers). Users can enable extreme outlier detection by checking the \"Extreme outlier\" option, which uses a deviation multiplier of 3.0.</p>"},{"location":"analysis_modes/subjects_exploration/#2-interpretation","title":"2. Interpretation","text":"<p>The outlier table displays only features for which the subject is flagged as an outlier. For each flagged feature, the table shows:</p> Column Description Feature Name of the feature Median (Dataset) Median value across the cohort Mean \u00b1 Std (Dataset) Mean and standard deviation across the cohort Subject The subject's value for this feature <p>If the subject is not an outlier for any feature, a message is displayed confirming this.</p> <p>Tip</p> <p>Use the extreme outlier setting to focus only on the most severe deviations. Standard outliers (1.5 \u00d7 IQR) capture moderate deviations, while extreme outliers (3.0 \u00d7 IQR) highlight only the most unusual cases.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"analysis_modes/univariate/","title":"Univariate feature analysis","text":"<p>The Univariate analysis mode in AUDIT provides a comprehensive and intuitive way to explore how a single variable  relates to the behavior of a segmentation model, or to differences across datasets. This mode supports descriptive  analytics through visual exploration, enabling researchers to detect outliers, identify dataset shifts, and understand  variability in imaging or metadata features that may influence downstream model performance.</p> <p>The purpose of univariate analysis is to:</p> <ul> <li>Compare datasets or cohorts based on a single feature of interest.</li> <li>Explore the distribution of quantitative relevant features (statistical, texture, tumor, etc.).</li> <li>Detect outliers or atypical cases that may affect downstream modeling.</li> <li>Identify dataset shifts (e.g., site or scanner variability).</li> <li>Visualize central tendencies and variability in the selected feature.</li> </ul>"},{"location":"analysis_modes/univariate/#demo-video","title":"\ud83c\udfa5 Demo video","text":"<p>Below is a short video that walks you through the homepage interface:</p> <p></p>"},{"location":"analysis_modes/univariate/#user-configuration","title":"\u2699\ufe0f User configuration","text":""},{"location":"analysis_modes/univariate/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Users can load and compare multiple datasets simultaneously in the Univariate analysis mode. This  enables side-by-side inspection of how a given feature varies across different cohorts \u2014 for example,  across different institutions, scanner protocols, or study years.</p> <p>Each dataset is automatically assigned a distinct color in all plots, ensuring an easy interpretation.</p> <p>Datasets can be:</p> <ul> <li>Selected or deselected using the dedicated selector panel on the left-hand side of the dashboard.</li> <li>Toggled directly from the interactive Plotly legend, which allows temporary hiding/showing of datasets with a single click.</li> </ul> <p>While the Plotly legend interaction is useful for quick comparisons, we recommend using the left-side dataset selector  for a more stable and consistent user experience, especially when exporting plots or applying filters.</p>"},{"location":"analysis_modes/univariate/#2-feature-selection","title":"2. Feature Selection","text":"<p>Users can choose from different feature categories such as:</p> Feature Description Example Statistical Descriptive measures derived from intensity distributions. Mean, standard deviation, maximum Texture Quantitative descriptors of local intensity patterns (GLCM-based). Entropy, contrast, homogeneity Spatial Anatomical or geometric positioning of segmented regions. Center of mass (x, y, z) Tumor Morphological properties extracted from segmentation masks. Lesion volume, tumor location <p>Some features depend heavily on the imaging modality, and comparisons between datasets are only meaningful when  extracted from preprocessed MRIs. Texture features in particular are highly sensitive to variations in acquisition  protocols and preprocessing pipelines. This sensitivity arises because descriptors like entropy, contrast, and energy  are affected by voxel size and image spacing, intensity range and quantization strategy, and other technical aspects  related to the scanners. Such technical differences are especially relevant in multi-center studies, where spatial  resolution and image quality may vary substantially.</p> <p>Other features, such as tumor volume, center of mass, or spatial location, are typically derived from segmentation  masks that take into account the voxel spacing, and are independent of intensity information. These are more robust  to differences in acquisition and preprocessing, making them suitable for comparisons even across heterogeneous cohorts.</p> <p>Info</p> <p>More technical details can be found in our publication and within the API reference.</p>"},{"location":"analysis_modes/univariate/#3-filtering","title":"3. Filtering","text":"<p>Several filtering strategies are available to clean or constrain the data prior to visualization:</p> <ul> <li>No filter: Show all data as-is.</li> <li>Removing outliers: Exclude points out of a range of values provided by the users.</li> <li>Clipping outliers: Truncate values beyond a user-defined threshold.</li> <li>Standard deviation-based filtering: Retain only values within a specified number of standard deviations from the mean.</li> </ul> <p>This helps in focusing on the core structure of the data, minimizing the impact of extreme values.</p> <p>Warning</p> <p>Some filters or parameter settings may be overly restrictive, reducing the number of visible data points and affecting the interpretability of the visualizations. We encourage users to carefully adjust filtering thresholds depending on the context and data distribution.</p>"},{"location":"analysis_modes/univariate/#4-highlight-subject","title":"4. Highlight subject","text":"<p>The Highlight subject option allows users to trace the behavior of an individual patient (or subject) across  plots. By entering a specific subject ID and specifying the corresponding dataset, the selected subject will be  visually highlighted in Boxplots and Violinplots, where the subject appears as a clearly marked point (e.g., with a  different color or shape).</p> <p>This feature is particularly valuable in contexts such as detecting whether a subject is an outlier or aligns with cohort  expectations or verifying how individuals from different cohorts are distributed in shared feature space.</p> <p>The subject remains highlighted even when toggling between plot types, making it easier to interpret its relative  position under different visual perspectives.</p> <p>Tip</p> <p>When analyzing extreme values, this feature can help determine whether a subject truly deviates from the cohort or lies within natural variability.</p>"},{"location":"analysis_modes/univariate/#visualizations","title":"\ud83d\udcca Visualizations","text":"<p>The Univariate analysis mode provides two dedicated dashboards, each tailored to a specific type of visual  exploration:</p>"},{"location":"analysis_modes/univariate/#dashboard-1-distribution-by-dataset","title":"Dashboard 1: Distribution by dataset","text":"<p>This view allows users to compare feature distributions across datasets or groups using statistical summaries and  individual data points.</p>"},{"location":"analysis_modes/univariate/#box","title":"Box","text":"<p>Displays the median, interquartile range, and potential outliers for each group. Useful for comparing central tendency and variability between datasets.</p> <p> Figure 1: This box plot summarizes the subject-level distribution of a selected feature across multiple datasets, showing median, quartiles, and outliers.</p>"},{"location":"analysis_modes/univariate/#violin-plot","title":"Violin Plot","text":"<p>Extends the boxplot with a mirrored kernel density estimation, offering insight into the shape of the distribution  (e.g., multimodality or skewness). Great for identifying subtle differences between groups.</p> <p> Figure 2: Violin plots illustrate the full shape of feature distributions across datasets, useful for spotting multimodal or skewed distributions.</p>"},{"location":"analysis_modes/univariate/#box-points","title":"Box + Points","text":"<p>Combines a boxplot with individual data points overlaid.  </p> <p> Figure 3: Box + Points visualization combines summary statistics with subject-level markers, making it easier to spot trends or anomalies within groups.</p>"},{"location":"analysis_modes/univariate/#dashboard-2-global-distribution-shape","title":"Dashboard 2: Global Distribution Shape","text":"<p>This view focuses on the overall shape and structure of the feature distribution across datasets.</p>"},{"location":"analysis_modes/univariate/#probability-density-smoothed-distribution","title":"Probability Density (Smoothed Distribution)","text":"<p>Displays a smoothed estimate of the probability density function (PDF) for each dataset, often using kernel density estimation.</p> <p> Figure 4: Probability density functions estimate the underlying shape of feature distributions, revealing subtle differences in central tendency and spread.</p>"},{"location":"analysis_modes/univariate/#histogram","title":"Histogram","text":"<p>Shows the frequency of observations grouped into bins.</p> <p> Figure 5: Histograms display the frequency distribution of a selected feature across datasets, allowing users to tune bin width for detailed or coarse analysis.</p> <p>When using the Histogram plot, users can configure the number of bins used to partition the feature range. This  setting controls the granularity of the histogram. Typically, a smaller number of bins results in broader, coarser  groupings, which can help reveal overall distribution trends. In contrast, a larger number of bins produces a more  detailed view, potentially exposing finer structure or noise in the data.</p> <p>This setting is especially useful when exploring skewed, sparse, or multimodal distributions, where default binning may  obscure relevant patterns.</p> <p>Warning</p> <p>Despite the number defined in the settings, Plotly may automatically adjust the bin size and number of bins to optimize the visualization. As a result, the final number of bins rendered may not exactly match the user\u2019s input.</p> <p>All plots are interactive, powered by Plotly, allowing users to:</p> <ul> <li>Toggle datasets on and off</li> <li>Zoom and pan within plots</li> <li>Export high-resolution images</li> <li>Hover to inspect exact values</li> </ul> <p>Whether you're debugging unexpected results or preparing figures for publication, this analysis mode is a core part of  the AUDIT toolkit.</p>"},{"location":"getting_started/additional_notes/","title":"Additional Tips &amp; Best Practices","text":"<ul> <li>For ITK-Snap integration, ensure it is installed and configured correctly.</li> <li>Use the <code>logs/</code> directory to monitor execution details and debug issues.</li> <li>Always start from the example configuration files and project structure provided in the repository.</li> <li>Keep your Python environment isolated to avoid dependency conflicts.</li> </ul>"},{"location":"getting_started/additional_notes/#next-steps","title":"Next Steps","text":"<p>You're all set to start using AUDIT! Dive into your MRI data, evaluate AI models, and gain deeper insights with AUDIT\u2019s powerful tools.</p> <p>For further details, check out the other sections of the documentation and the AUDIT GitHub repository.</p> <p>Tip</p> <p>Following best practices and recommended workflows ensures reproducible and reliable results.</p>"},{"location":"getting_started/configuration/","title":"Configuration Guide","text":"<p>AUDIT uses configuration files to define paths, settings, and parameters for feature extraction, metric evaluation, and  app customization. We recommend starting from the example files included in the repository and adapting them to your  needs.</p>"},{"location":"getting_started/configuration/#configuration-files","title":"Configuration files","text":"<ul> <li>Feature extraction: feature_extraction.yaml</li> <li>Metric evaluation: metric_extraction.yaml</li> <li>App settings: app.yaml</li> </ul> <p>All configuration files are stored in the config/ directory.</p>"},{"location":"getting_started/configuration/#example-feature-extraction-config","title":"Example: Feature Extraction Config","text":"<p>Defines dataset paths, label mappings, features to extract, and longitudinal study settings.</p> <pre><code># Paths to datasets\ndata_paths:\n  BraTS: '/home/user/AUDIT/datasets/BraTS/BraTS_images'\n  UCSF: '/home/user/AUDIT/datasets/UCSF/UCSF_images'\n\n# Label mapping\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# Features to extract\nfeatures:\n  statistical: true\n  texture: false\n  spatial: true\n  tumor: true\n\n# Longitudinal settings\nlongitudinal:\n  UCSF:\n    pattern: \"_\"\n    longitudinal_id: 1\n    time_point: 2\n\n# Output path\noutput_path: '/home/user/AUDIT/outputs/features'\n</code></pre>"},{"location":"getting_started/configuration/#example-metric-extraction-config","title":"Example: Metric Extraction Config","text":"<p>Defines dataset and prediction paths, label mappings, metrics to compute, and output settings.</p> <pre><code># Dataset path\ndata_path: '/home/user/AUDIT/datasets/BraTS/BraTS_images'\n\n# Model predictions\nmodel_predictions_paths:\n  nnUnet: '/home/user/AUDIT/datasets/BraTS/BraTS_seg/nnUnet'\n\n# Label mapping\nlabels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\n# Metrics to compute\nmetrics:\n  dice: true\n  jacc: true\n  accu: true\n  prec: true\n  sens: true\n  spec: true\n  haus: true\n  size: true\n\n# Library and output\npackage: custom\ncalculate_stats: false\noutput_path: '/home/user/AUDIT/outputs/metrics'\nfilename: 'BraTS'\n</code></pre>"},{"location":"getting_started/configuration/#example-app-config","title":"Example: App Config","text":"<p>Defines dataset, feature, and metric paths for the web app.</p> <pre><code>labels:\n  BKG: 0\n  EDE: 3\n  ENH: 1\n  NEC: 2\n\ndatasets_path: '/home/user/AUDIT/datasets'\nfeatures_path: '/home/user/AUDIT/outputs/features'\nmetrics_path: '/home/user/AUDIT/outputs/metrics'\n\nraw_datasets:\n  BraTS: \"${datasets_path}/BraTS/BraTS_images\"\n  UCSF: \"${datasets_path}/UCSF/UCSF_images\"\n\nfeatures:\n  BraTS: \"${features_path}/extracted_information_BraTS.csv\"\n  UCSF: \"${features_path}/extracted_information_UCSF.csv\"\n\nmetrics:\n  UCSF: \"${metrics_path}/extracted_information_UCSF.csv\"\n\npredictions:\n  UCSF:\n    SegResNet: \"${datasets_path}/UCSF/UCSF_seg/SegResNet\"\n</code></pre>"},{"location":"getting_started/configuration/#getting-started","title":"Getting Started","text":"<ul> <li>Start from the example configuration files in the repository.</li> <li>Adjust paths and settings to match your environment and project needs.</li> <li>For more details, see the API reference and other documentation sections.</li> </ul> <p>Tip</p> <p>Consistent configuration ensures reproducible and reliable results.</p>"},{"location":"getting_started/installation/","title":"Installation Guide","text":"<p>Welcome to AUDIT! Before installing on your local computer, we recommend exploring the publicly deployed app on Streamlit Cloud:</p> <p>https://auditapp.streamlit.app/</p> <p>This allows you to try AUDIT without any setup. When you're ready to analyze your own datasets or use advanced features, follow the steps below to install AUDIT locally.</p>"},{"location":"getting_started/installation/#installation-via-repository-recommended","title":"Installation via Repository (Recommended)","text":"<p>For full flexibility, access to the latest updates, and example files, we recommend installing AUDIT by cloning the official repository. This method is suitable for most users, including those who want to customize or contribute to the project.</p> <p>Choose one of the following options:</p>"},{"location":"getting_started/installation/#option-1-using-conda","title":"Option 1: Using Conda","text":"<ol> <li> <p>Create an isolated environment (recommended to avoid dependency conflicts):</p> <pre><code>conda create -n audit_env python=3.10\nconda activate audit_env\n</code></pre> </li> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/caumente/AUDIT.git\ncd AUDIT\n</code></pre> </li> <li> <p>Install dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> </ol>"},{"location":"getting_started/installation/#option-2-using-poetry","title":"Option 2: Using Poetry","text":"<p>Poetry is a modern dependency manager that simplifies library management and environment creation.</p> <ol> <li> <p>Install Poetry (if not already installed):</p> <p>Poetry installation guide</p> </li> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/caumente/AUDIT.git\ncd AUDIT\n</code></pre> </li> <li> <p>Install dependencies:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Activate the virtual environment:</p> <pre><code>poetry shell\n</code></pre> </li> </ol>"},{"location":"getting_started/installation/#installation-via-pypi","title":"Installation via PyPI","text":"<p>If you want a quick way to use AUDIT for data analysis and evaluation, you can install the latest stable version from PyPI. AUDIT (<code>auditapp</code>) requires Python 3.10 or higher. If you try to install it with an older Python version, you will see an error like:</p> <p>Failure</p> <p>ERROR: Could not find a version that satisfies the requirement auditapp (from versions: none) ERROR: No matching distribution found for auditapp</p> <pre><code>pip install auditapp\n</code></pre> <p>This method is ideal for users who do not need to modify the source code. You will still need to set up configuration files and the recommended project structure.</p> <p>Tip</p> <p>If you encounter issues with permissions, try running <code>pip install --user auditapp</code>.</p>"},{"location":"getting_started/installation/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>The repository installation provides greater flexibility and access to example cases, project structure templates, and outputs.</li> <li>Example configuration files and datasets are included in the repository to help you get started quickly.</li> <li>For details on project structure, configuration, and running AUDIT, see the other sections in the documentation.</li> </ul> <p>Info</p> <p>The recommended workflow is to install AUDIT via the repository for full functionality and easier customization.</p>"},{"location":"getting_started/installation/#troubleshooting-tips","title":"Troubleshooting &amp; Tips","text":"<ul> <li>If you have issues with Python versions, ensure you are using Python 3.10 or higher.</li> <li>For help with dependencies, check the requirements.txt or pyproject.toml files.</li> <li>For more information, visit the AUDIT GitHub repository.</li> </ul> <p>You're ready to start using AUDIT! For further guidance, explore the rest of the documentation.</p>"},{"location":"getting_started/project_structure/","title":"Project Structure Guide","text":"<p>AUDIT supports flexible project organization, but we recommend following the default structure for clarity, reproducibility, and ease of use. This structure is modular and designed to help you manage datasets, configurations, outputs, and logs efficiently.</p>"},{"location":"getting_started/project_structure/#recommended-project-structure","title":"Recommended project structure","text":"<pre><code>your_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u251c\u2500\u2500 logs/\n</code></pre>"},{"location":"getting_started/project_structure/#datasets-directory-datasets","title":"Datasets Directory (<code>datasets/</code>)","text":"<p>Central location for all datasets used in training and testing. Organize each dataset in its own subdirectory, with images, ground truth segmentations, and model predictions clearly separated.</p> <p>Info</p> <p>The reserved word <code>__pred_</code> is used in AUDIT for model predictions.</p> <p>Example: <pre><code>datasets/\n\u251c\u2500\u2500 dataset_1/\n\u2502   \u251c\u2500\u2500 dataset_1_images/\n\u2502   \u251c\u2500\u2500 dataset_1_seg/\n\u2502   \u2502   \u251c\u2500\u2500 model_1/\n\u2502   \u2502   \u251c\u2500\u2500 model_2/\n\u251c\u2500\u2500 dataset_2/\n</code></pre></p>"},{"location":"getting_started/project_structure/#configuration-directory-config","title":"Configuration Directory (<code>config/</code>)","text":"<p>Store all configuration files needed for feature extraction, metric evaluation, and app settings.</p> <p>Example: <pre><code>config/\n\u251c\u2500\u2500 feature_extraction.yaml\n\u251c\u2500\u2500 metric_extraction.yaml\n\u251c\u2500\u2500 app.yaml\n</code></pre></p>"},{"location":"getting_started/project_structure/#outputs-directory-outputs","title":"Outputs Directory (<code>outputs/</code>)","text":"<p>All results generated by AUDIT are saved here, including extracted features and evaluation metrics.</p> <p>Example: <pre><code>outputs/\n\u251c\u2500\u2500 features/\n\u251c\u2500\u2500 metrics/\n</code></pre></p>"},{"location":"getting_started/project_structure/#logs-directory-logs","title":"Logs Directory (<code>logs/</code>)","text":"<p>Contains logs for debugging, monitoring, and record-keeping.</p> <p>Example: <pre><code>logs/\n\u251c\u2500\u2500 features/\n\u251c\u2500\u2500 metric/\n</code></pre></p>"},{"location":"getting_started/project_structure/#getting-started","title":"Getting Started","text":"<ul> <li>The recommended structure is automatically created when you run AUDIT for the first time.</li> <li>Example datasets and configuration files are available in the repository.</li> <li>For details on configuration and running AUDIT, see the other sections in the documentation.</li> </ul> <p>Tip</p> <p>Using the default structure ensures seamless integration and reproducibility.</p>"},{"location":"getting_started/run_app/","title":"Running AUDIT Web App","text":"<p>AUDIT provides an interactive web app for data exploration and visualization. We recommend running the app from the cloned repository for full access to features and customization.</p>"},{"location":"getting_started/run_app/#run-from-repository-recommended","title":"Run from Repository (Recommended)","text":"<p>Start the app using:</p> <pre><code>python src/audit/app/launcher.py --config path/to/your/app.yaml\n</code></pre> <p>Info</p> <p>The <code>--config</code> parameter is optional if you edit the default configuration file in <code>src/audit/configs/app.yaml</code>.</p>"},{"location":"getting_started/run_app/#run-from-pypi-installation","title":"Run from PyPI Installation","text":"<p>If installed via PyPI, use:</p> <pre><code>auditapp run-app --config path/to/your/app.yaml\n</code></pre>"},{"location":"getting_started/run_app/#accessing-the-app","title":"Accessing the App","text":"<ul> <li>The app will open in your default web browser at http://localhost:8501/.</li> <li>Use the dashboards to explore data distributions, compare model performance, and analyze trends.</li> </ul>"},{"location":"getting_started/run_app/#getting-started","title":"Getting Started","text":"<ul> <li>Example configuration files are available in the repository.</li> <li>For more details, see the configuration and project structure guides.</li> </ul> <p>Tip</p> <p>For full functionality, use the repository installation and default configuration files.</p>"},{"location":"getting_started/run_backend/","title":"Running AUDIT Backend","text":"<p>AUDIT's backend is responsible for extracting features from medical images and evaluating segmentation model performance using metrics. These steps are essential for understanding your data, benchmarking models, and generating results for further analysis and visualization.</p> <p>Feature extraction computes quantitative descriptors (statistical, texture, spatial, tumor features) from your datasets, enabling detailed cohort analysis and model comparison. Metric extraction evaluates segmentation predictions against ground truth, providing objective measures of model accuracy and reliability.</p> <p>All extraction and evaluation methods are documented in the API reference.</p>"},{"location":"getting_started/run_backend/#run-from-repository-recommended","title":"Run from Repository (Recommended)","text":"<p>Use the following commands to run feature extraction and metric evaluation modules:</p> <pre><code>python src/audit/feature_extraction.py --config path/to/your/feature_extraction.yaml\npython src/audit/metric_extraction.py --config path/to/your/metric_extraction.yaml\n</code></pre> <p>Info</p> <p>The <code>--config</code> parameter is optional if you edit the default configuration files in <code>src/audit/configs/</code>.</p>"},{"location":"getting_started/run_backend/#run-from-pypi-installation","title":"Run from PyPI Installation","text":"<p>If installed via PyPI, use the command-line interface:</p> <pre><code>auditapp feature-extraction --config full/path/to/your/feature_extraction.yaml\nauditapp metric-extraction --config full/path/to/your/metric_extraction.yaml\n</code></pre> <p>Full path to your config files is mandatory. Otherwise, AUDIT might look for internal config files contained in the library by default.</p>"},{"location":"getting_started/run_backend/#getting-started","title":"Getting Started","text":"<ul> <li>Output and log files are saved in the directories specified in your configuration files.</li> <li>Example configuration files are available in the repository.</li> <li>For more details, see the configuration and project structure guides.</li> <li>For technical details, see the API reference documentation.</li> </ul> <p>Tip</p> <p>For best results, use the repository installation and default configuration files.</p>"},{"location":"tutorials/BraTS_2024/","title":"BraTS 2024 Tutorial","text":"<p>This tutorial guides you step by step through preparing the BraTS 2024 dataset, organizing it, configuring feature  extraction, and launching the interactive dashboard to explore your data using the AUDIT framework.</p> <p>Unlike BraTS 2025, this dataset includes a demographic mapping file containing valuable metadata such as glioma type,  MRI scanner specifications, magnetic field strength, and patient demographics. This enables a deeper analysis not only  exploring dataset structure and image-derived features but also uncovering potential biases or acquisition-related  differences that may affect model performance.</p> <p>AUDIT provides a unified environment to analyze, compare, and visualize these variations across patient cohorts,  acquisition sites, and demographic groups.</p> <p>References:</p> <ul> <li> <p>The 2024 Brain Tumor Segmentation (BraTS) Challenge: Glioma Segmentation on Post-treatment MRI</p> </li> <li> <p>The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification</p> </li> <li> <p>BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis</p> </li> </ul> <p>Note</p> <p>The annotation protocol followed in this tutorial was:  Label 0: Background, Label 1: Non-enhancing tumor core, Label 2: surrounding non-enhancing FLAIR hyperintensity,  Label 3: Enhancing tumor, Label 4: resection cavity. However, we encourage users to check the protocol used in their datasets.</p>"},{"location":"tutorials/BraTS_2024/#1-prerequisites","title":"1. Prerequisites","text":"<p>Before starting, make sure you are familiar with the BraTS 2025 tutorial, as this guide assumes you already understand  the fundamentals of AUDIT, such as environment setup, project structure, and configuration files.</p> <p>We recommend using an isolated Anaconda environment with Jupyter Notebook and following the same project organization  used in the previous tutorial.</p> <p>The BraTS 2024 dataset is hosted on Synapse and requires prior registration. Please follow the official  instructions from the challenge and to BraTS 2024 Data Access section to download the data. </p> <p>Once downloaded and unzipped, store the training and additional training sets in a directory called  <code>.datasets/BraTS_2024</code>. The metadata file must be stored in the project root. The project structure should look like  this before starting:</p> <pre><code>brats2024_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500\u2500\u2500 BraTS_2024/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 BraTS-GLI-00005-100/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 BraTS-GLI-00005-101/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 BraTS-PTG supplementary demographic information and metadata.xlsx\n</code></pre>"},{"location":"tutorials/BraTS_2024/#2-exploring-the-metadata","title":"2. Exploring the metadata","text":"<p>Before working with the imaging data, it is useful to explore the metadata. The BraTS 2024 demographic spreadsheet  includes variables such as acquisition site, magnetic field strength, scanner manufacturer, patient age, sex, and  glioma type - all of which can later be used to stratify analyses and investigate dataset heterogeneity.</p> <p>Let\u2019s begin by loading the file in a pandas DataFrame and take a first look:</p> <pre><code>import pandas as pd\nmapping = pd.read_excel(\"./BraTS-PTG supplementary demographic information and metadata.xlsx\")\nmapping.head(4)\n</code></pre> <p>This will give you a table like this:</p> BraTS Subject ID Site Site Subject ID Annotator 1 Approver 1 Train/Test/Validation Magnetic Field Strength Manufacturer Patient's Age Patient's Sex Glioma type BraTS-GLI-00005-100 UCSF 1002251 NaN AMR Train 3.0 GE 50.0 M Oligodendroglioma BraTS-GLI-00005-101 UCSF 1002252 NaN AMR Train 3.0 GE 50.0 M Oligodendroglioma BraTS-GLI-00006-100 UCSF 1001751 BKKF JDR Train 3.0 GE 39.0 F Glioblastoma BraTS-GLI-00006-101 UCSF 1001752 BKKF JDR Train 3.0 GE 39.0 F Glioblastoma <p>Each patient record is linked to a specific site and scanner configuration. Later, you can use this information to  partition the dataset and perform site-wise or scanner-wise analyses.</p> <p>For this tutorial, we will focus on the training and additional training subsets, excluding validation data.</p> <pre><code>df = mapping[mapping[\"Train/Test/Validation \"].isin([\"Train\", \"Train-additional\"])]\n</code></pre>"},{"location":"tutorials/BraTS_2024/#3-organizing-the-dataset-by-site","title":"3. Organizing the dataset by site","text":"<p>To facilitate comparisons between different acquisition centers, we will reorganize the dataset into site-specific  folders.  The function below automates the process by reading the metadata and copying each subject folder into the  appropriate subdirectory. However, we  encourage users to perform their own analyses, exploring splitting by glioma  type, magnetic field strength, manufacturer, and other variables.</p> <p>The goal is to show how AUDIT can help you better understand your dataset and perform a more accurate evaluation of segmentation models and MRI cohorts.</p> <p>Let's create a function to move each patient into their corresponding folder, instead of keeping all of them grouped  in a single directory.</p> <pre><code>import os\nimport shutil\n\ndef organize_patients(df, column, input_dir, output_dir):\n    \"\"\"\n    Organize patient folders into subfolders based on a DataFrame column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Metadata DataFrame (must contain 'BraTS Subject ID').\n    column : str\n        Column name to organize by (e.g., 'Site', 'Manufacturer', 'Glioma type').\n    input_dir : str\n        Directory containing all patient folders (e.g., 'BraTS_2024').\n    output_dir : str\n        Directory where organized folders will be created (e.g., 'BraTS_2024_by_site').\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    for _, row in df.iterrows():\n        subject = row[\"BraTS Subject ID\"]\n        category = str(row[column])  # subfolder name\n\n        # Create category subfolder if it doesn\u2019t exist\n        category_folder = os.path.join(output_dir, category)\n        os.makedirs(category_folder, exist_ok=True)\n\n        # Source patient folder/file\n        subject_path = os.path.join(input_dir, subject)\n        dest_path = os.path.join(category_folder, subject)\n\n        if os.path.exists(subject_path):\n            if os.path.isdir(subject_path):\n                shutil.copytree(subject_path, dest_path, dirs_exist_ok=True)\n            else:\n                shutil.copy2(subject_path, dest_path)\n        else:\n            print(f\"Subject {subject} not found in {input_dir}\")\n\n    print(f\"Organization completed in {output_dir}\")\n</code></pre> <p>This function allows you to organize patients into separate directories and can be reused for any other analysis you  want to perform with the BraTS 2024 dataset. Run the following code to organize your data by acquisition site:</p> <pre><code>input_dir = \"./datasets/BraTS_2024\"\noutput_dir = \"./datasets/BraTS_2024_by_site\"\n\norganize_patients(df, column=\"Site\", input_dir=input_dir, output_dir=output_dir)\n</code></pre> <p>Ready! After running this, your project structure will look like:</p> <pre><code>brats2024_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500\u2500\u2500 BraTS_2024_by_site/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Duke/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-101/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Indiana/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Missouri/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 UCSD/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 UCSF/\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 BraTS-PTG supplementary demographic information and metadata.xlsx\n</code></pre> <p>To be able to take advantage of all functionalities of AUDIT, for instance the connectivity with ITK-Snap toolkit, we recommend locating the sequences and masks under the same folder, and predictions for each model under another. Then, The project structure should be the following.</p> <pre><code>brats2024_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500\u2500\u2500 BraTS_2024_by_site/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Duke/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke_images/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-101/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke_segs/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 model_1/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_pred.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-101_pred.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 model_2/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_pred.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-101_pred.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Indiana/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Missouri/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 UCSD/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 UCSF/\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 BraTS-PTG supplementary demographic information and metadata.xlsx\n</code></pre>"},{"location":"tutorials/BraTS_2024/#4-standardize-folder-and-file-naming","title":"4. Standardize folder and file naming","text":"<p>As in the BraTS 2025 tutorial, we need to standardize the folder and file names to ensure AUDIT compatibility. This  includes replacing prefixes, updating sequence identifiers, and ensuring consistent use of underscores. Check BraTS 2025 tutorial for more details.</p>"},{"location":"tutorials/BraTS_2024/#41-rename-folders-and-files","title":"4.1 Rename folders and files","text":"<p>We\u2019ll first replace the <code>BraTS-GLI-</code> prefix with the site name for clarity.</p> <pre><code>sites = [\"Duke\", \"Indiana\", \"Missouri\", \"UCSD\", \"UCSF\"]\nfor s in sites:\n    rename_dirs(\n        root_dir=os.path.join(output_dir, s),\n        old_name=\"BraTS-GLI\",\n        new_name=s,\n        safe_mode=False\n    )\n\n    rename_files(\n        root_dir=os.path.join(output_dir, s),\n        old_name=\"BraTS-GLI\",\n        new_name=s,\n        safe_mode=False\n    )\n</code></pre>"},{"location":"tutorials/BraTS_2024/#42-standardize-sequence","title":"4.2 Standardize sequence","text":"<p>Additionally, to make the sequence names clearer and consistent, we will replace suffixes:</p> <pre><code>old_names = ['-seg.nii.gz', '-t1c.nii.gz', '-t1n.nii.gz', '-t2f.nii.gz', '-t2w.nii.gz']\nnew_names = ['_seg.nii.gz', '_t1ce.nii.gz', '_t1.nii.gz', '_flair.nii.gz', '_t2.nii.gz']\nsites = [\"Duke\", \"Indiana\", \"Missouri\", \"UCSD\", \"UCSF\"]\n\nfor s in sites:\n    for o, n in zip(old_names, new_names):\n        rename_files(\n            root_dir=os.path.join(output_dir, s),\n            old_name=o,\n            new_name=n,\n            safe_mode=False\n        )\n</code></pre>"},{"location":"tutorials/BraTS_2024/#43-replace-dashes-with-underscores-optional","title":"4.3 Replace dashes with underscores (optional)","text":"<p>Finally, to ensure consistent naming conventions, we replace all <code>-</code> characters with <code>_</code>:</p> <pre><code>for s in sites:\n    rename_files(root_dir=root_dir=os.path.join(output_dir, s), old_name=\"-\", new_name=\"_\", safe_mode=False)\n    rename_dirs(root_dir=root_dir=os.path.join(output_dir, s), old_name=\"-\", new_name=\"_\", safe_mode=False)\n</code></pre>"},{"location":"tutorials/BraTS_2024/#5-run-feature-extraction","title":"5. Run feature extraction","text":"<p>Now that the datasets are ready, let\u2019s configure the feature extraction. AUDIT takes advantage of YAML configuration  files for extracting features and metrics and running the app. All of them have been created automatically inside  the <code>configs/</code> at the time of generating the project structure. The one we need to edit to run the feature extraction  is named <code>feature_extraction.yaml</code>. </p> <p>Here we provide the config file needed for this project. In it, paths to datasets, sequences names, labels mapping,  features to extract, and output paths need to be defined.:</p> <pre><code># Paths to all the datasets\ndata_paths:\n  Duke: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Duke/Duke_images/'\n  Indiana: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Indiana/Indiana_images/'\n  Missouri: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Missouri/Missouri_images/'\n  UCSD: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/UCSD/UCSD_images/'\n  UCSF: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/UCSF/UCSF_images/'\n\n# Sequences available\nsequences:\n  - '_t1'\n  - '_t2'\n  - '_t1ce'\n  - '_flair'\n\n# Mapping of labels to their numeric values\nlabels:\n  BKG: 0\n  NETC: 1\n  SNFH: 2\n  ET: 3\n  RC: 4\n\n# List of features to extract\nfeatures:\n  statistical: true\n  texture: true\n  spatial: true\n  tumor: true\n\n# Output paths\noutput_path: '/home/usr/brats2024_project/outputs/features'\nlogs_path: '/home/usr/brats2024_project/logs/features'\n\n# Resources\ncpu_cores: 4\n</code></pre> <p>Run feature extraction by using the following command:</p> <pre><code>auditapp feature-extraction --config /home/usr/projects/configs/feature_extraction.yaml\n</code></pre> <p>After execution, <code>/home/usr/brats2024_project/outputs/features</code> will contain the extracted features for all the datasets.</p> <p>Important</p> <p>All paths must be absolute. Otherwise, AUDIT may fall back to its internal default config files.</p>"},{"location":"tutorials/BraTS_2024/#6-launch-the-dashboard-without-predictions","title":"6. Launch the dashboard (without predictions)","text":"<p>Once features are extracted, we can explore them using the web app. We should edit the config file named <code>app.yaml</code> and  adapt it as follows:</p> <pre><code># Sequences available\nsequences:\n  - '_t1'\n  - '_t2'\n  - '_t1ce'\n  - '_flair'\n\n# Mapping of labels to their numeric values\nlabels:\n  BKG: 0\n  NETC: 1\n  SNFH: 2\n  ET: 3\n  RC: 4\n\n# Root paths\ndatasets_path: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site'\nfeatures_path: '/home/usr/brats2024_project/outputs/features'\nmetrics_path: '/home/usr/brats2024_project/outputs/metrics'\n\n# Raw datasets\nraw_datasets:\n  Duke: \"${datasets_path}/Duke/Duke_images\"\n  Indiana: \"${datasets_path}/Indiana/Indiana_images\"\n  Missouri: \"${datasets_path}/Missouri/Missouri_images\"\n  UCSD: \"${datasets_path}/UCSD/UCSD_images\"\n  UCSF: \"${datasets_path}/UCSF/UCSF_images\"\n\n# Feature extraction CSVs\nfeatures:\n  Duke: \"${features_path}/extracted_information_Duke.csv\"\n  Indiana: \"${features_path}/extracted_information_Indiana.csv\"\n  Missouri: \"${features_path}/extracted_information_Missouri.csv\"\n  UCSD: \"${features_path}/extracted_information_UCSD.csv\"\n  UCSF: \"${features_path}/extracted_information_UCSF.csv\"\n\n\n# Metric extraction CSVs\nmetrics:\n\n# Model predictions\npredictions:\n</code></pre> <p>Launch the app with:</p> <pre><code>auditapp run-app --config /home/usr/projects/configs/app.yaml\n</code></pre> <p>Now it\u2019s time to explore the datasets!</p> <p>Keep in mind that some feature types may influence generalization across cohorts more than others. For example, while  the statistical features appear quite similar between the datasets, we encourage you to dive deeper into the data,  experiment with different feature sets, and challenge your models to reach the next level of performance.</p> <p> Figure 1: Univariate feature analysis mode. Maximum pixel intensity distribution for T1 sequence.</p> <p>If everything has been set up correctly \u2014 especially making sure to include the patients (original sequences and masks) within the dataset_images directory (e.g., datasets/Duke/Duke_images/), and ITK has been installed properly as suggested in the documentation, then it is possible to click on any of the points of interest and explore them directly in ITK-SNAP. In our case, we clicked on the patient Indiana-03062-100.</p> <p> Figure 2: Subject Indiana-03062-100 exploration directly on ITK-SNAP by clicking on a point from the boxplot shown in Figure 1.</p>"},{"location":"tutorials/BraTS_2024/#7-perform-inference","title":"7. Perform inference","text":"<p>In addition to analyzing data distributions, studying correlations, identifying anomalous subjects, and many other  analyses, one of AUDIT's main functionalities is the evaluation of AI medical segmentation models. To perform inference,  we used the models presented at MICCAI 2025, specifically the top 4 ranked models in the Adult Glioma Segmentation  (Pre &amp; Post-Treatment) challenge.</p> <p>For this purpose, we leveraged the brats library, which contains the Docker  containers for each of the models:</p> Year Rank Authors Paper Available Model ID 2025 1st Ishika Jain, et al. N/A \u274c BraTS25_1 2025 2nd Qu Lin, et al. N/A \u2705 BraTS25_2 2025 3rd Liwei Jin, et al. N/A \u2705 BraTS25_3A 2025 3rd Adrian Celaya, et al. N/A \u274c BraTS25_3B <p>After performing inference on the 5 subdatasets (Duke, Indiana, Missouri, UCSD, and UCSF) with each of the 4 models,  your project structure should look like the following, for example for subject Duke-02060-100:</p> <pre><code>brats2024_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500\u2500\u2500 BraTS_2024_by_site/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Duke/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Duke_images/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_flair.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_t1.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_t2.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_t1ce.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_seg.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500 Duke_seg/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 BraTS25_1/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Duke-02060-100_pred.nii.gz\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 .....\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 BraTS25_2/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 BraTS25_3A/\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 BraTS25_3B/\n</code></pre> <p>Important</p> <p>The suffixes _seg and _pred are reserved keywords within AUDIT and are necessary for metric calculation.</p>"},{"location":"tutorials/BraTS_2024/#8-run-metric-extraction","title":"8. Run metric extraction","text":"<p>Now that the datasets are ready, let's configure metric extraction. Just as we did previously, we now need to configure  the file named <code>metric_extraction.yaml</code>.</p> <p>Here we provide the config file needed to extract metrics for the Duke dataset. Paths to datasets, sequence names,  label mappings, metrics to compute, and output paths need to be defined. The same configuration file should be prepared  for each of the other datasets.</p> <pre><code># Path to the raw dataset\ndata_path: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Duke/Duke_images'\n\n# Paths to model predictions\nmodel_predictions_paths:\n  BraTS25_1: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Duke/Duke_seg/BraTS25_1'\n  BraTS25_2: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Duke/Duke_seg/BraTS25_2'\n  BraTS25_3A: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Duke/Duke_seg/BraTS25_3A'\n  BraTS25_3B: '/home/usr/brats2024_project/datasets/BraTS_2024_by_site/Duke/Duke_seg/BraTS25_3B'\n\n# Mapping of labels to their numeric values\nlabels:\n  BKG: 0\n  NETC: 1\n  SNFH: 2\n  ET: 3\n  RC: 4\n\n# List of metrics to compute\nmetrics:\n  dice: true\n  jacc: true\n  accu: true\n  prec: true\n  sens: true\n  spec: true\n  haus: true\n  size: true\n\n# Library used for computing all the metrics\npackage: audit\n\n# Path where output metrics will be saved\noutput_path: '/home/usr/brats2024_project/outputs/metrics'\nfilename: 'Duke'\nlogs_path: '/home/usr/brats2024_project/logs/metric'\n\n# Resources\ncpu_cores: 4\n</code></pre> <p>Run metric extraction by using the following command:</p> <pre><code>auditapp metric-extraction --config /home/usr/projects/configs/metric_extraction.yaml\n</code></pre> <p>After execution, <code>/home/usr/brats2024_project/outputs/metrics</code> will contain the extracted metrics for all the datasets.</p> <p>Important</p> <p>All paths must be absolute. Otherwise, AUDIT may fall back to its internal default config files.</p>"},{"location":"tutorials/BraTS_2024/#9-launch-the-dashboard-with-predictions","title":"9. Launch the dashboard (with predictions)","text":"<p>Now that we have generated all predictions and extracted both features and metrics, we can relaunch the dashboard as  we did in step 6. However, in this case we must include new lines in our configuration file to enable reading of the predictions.</p> <p>Here we provide the remaining portion of the configuration file started in section 6. Users should concatenate this  information with the previous configuration.</p> <pre><code># Paths for metric extraction CSV files\nmetrics:\n  Duke: \"${metrics_path}/extracted_information_Duke.csv\"\n  Indiana: \"${metrics_path}/extracted_information_Indiana.csv\"\n  Missouri: \"${metrics_path}/extracted_information_Missouri.csv\"\n  UCSD: \"${metrics_path}/extracted_information_UCSD.csv\"\n  UCSF: \"${metrics_path}/extracted_information_UCSF.csv\"\n\n# Paths for models predictions\npredictions:\n  Duke:\n    BraTS2025_1: \"${datasets_path}/Duke/Duke_seg/BraTS25_1\"\n    BraTS2025_2: \"${datasets_path}/Duke/Duke_seg/BraTS25_2\"\n    BraTS2025_3A: \"${datasets_path}/Duke/Duke_seg/BraTS25_3A\"\n    BraTS2025_3B: \"${datasets_path}/Duke/Duke_seg/BraTS25_3B\"\n  Indiana:\n    BraTS2025_1: \"${datasets_path}/Indiana/Indiana_seg/BraTS25_1\"\n    BraTS2025_2: \"${datasets_path}/Indiana/Indiana_seg/BraTS25_2\"\n    BraTS2025_3A: \"${datasets_path}/Indiana/Indiana_seg/BraTS25_3A\"\n    BraTS2025_3B: \"${datasets_path}/Indiana/Indiana_seg/BraTS25_3B\"\n  Missouri:\n    BraTS2025_1: \"${datasets_path}/Missouri/Missouri_seg/BraTS25_1\"\n    BraTS2025_2: \"${datasets_path}/Missouri/Missouri_seg/BraTS25_2\"\n    BraTS2025_3A: \"${datasets_path}/Missouri/Missouri_seg/BraTS25_3A\"\n    BraTS2025_3B: \"${datasets_path}/Missouri/Missouri_seg/BraTS25_3B\"\n  UCSD:\n    BraTS2025_1: \"${datasets_path}/UCSD/UCSD_seg/BraTS25_1\"\n    BraTS2025_2: \"${datasets_path}/UCSD/UCSD_seg/BraTS25_2\"\n    BraTS2025_3A: \"${datasets_path}/UCSD/UCSD_seg/BraTS25_3A\"\n    BraTS2025_3B: \"${datasets_path}/UCSD/UCSD_seg/BraTS25_3B\"\n  UCSF:\n    BraTS2025_1: \"${datasets_path}/UCSF/UCSF_seg/BraTS25_1\"\n    BraTS2025_2: \"${datasets_path}/UCSF/UCSF_seg/BraTS25_2\"\n    BraTS2025_3A: \"${datasets_path}/UCSF/UCSF_seg/BraTS25_3A\"\n    BraTS2025_3B: \"${datasets_path}/UCSF/UCSF_seg/BraTS25_3B\"\n</code></pre> <p>Launch the app again with:</p> <pre><code>auditapp run-app --config /home/usr/projects/configs/app.yaml\n</code></pre>"},{"location":"tutorials/BraTS_2024/#10-model-evaluation","title":"10. Model evaluation","text":"<p>AUDIT presents different analysis modes to evaluate model behavior, from high-level granularity down to fine-grained  details at the patient and region level. Some of the analyses we can perform include the following:</p>"},{"location":"tutorials/BraTS_2024/#101-segmentation-error-matrix","title":"10.1 Segmentation error matrix","text":"<p>The segmentation error matrix allows you to analyze in detail which regions your model confuses. It is a pseudo-confusion  matrix normalized at the ground truth level. For more details about how this analysis mode works, check out the  documentation.</p> <p> Figure 3: Segmentation error matrix for the Duke dataset and BraTS2025_1 model.</p> <p>The matrix is analyzed at a global level, encompassing all errors made by the model at a high level throughout the  entire dataset. The main problem of the model is confusing Background with the SNFH region. Therefore, developers  should focus on finding strategies to minimize this weakness. The next most frequent confusion is predicting Background  when the actual label is RC. However, it's important to note that this evaluation is done as a percentage normalized  at the actual region level, so it must be interpreted carefully.</p> <p>If we focus on a specific subject, for example patient Duke-02060-100, we can analyze whether this same pattern holds  in absolute terms:</p> <p> Figure 4: Segmentation error matrix for patient Duke-02060-100 from the Duke dataset and BraTS2025_1 model.</p> <p>The behavior of confusing Background and SNFH that we observed at the dataset level also holds in absolute terms for  this specific patient. A total of 2,905 voxels were labeled as Background when they were actually voxels labeled as  tumoral by the medical experts (SNFH). However, for this specific patient, there are a total of 2,106 voxels that were  labeled as Background again, but actually belonged to the ET region.</p> <p>This is why it's crucial to understand not only the general behavior of the model, but also the specific cases. Now  that we know that for this specific patient the ET region was incorrectly labeled, we should analyze in detail the MRI  characteristics of this patient to see if there is any peculiarity that makes that region especially difficult to segment.</p>"},{"location":"tutorials/BraTS_2024/#102-single-model-performance","title":"10.2 Single model performance","text":"<p>Analyzing model performance as a function of different characteristics allows us to understand whether our model has  any bias. For example, we can answer questions such as: does my model perform better when predicting tumors that are  far from the brain's center of mass, or does our model predict better for tumors belonging to a specific pathology?  For more details about how this analysis mode works, check out the documentation.</p> <p>Ideally, what we would expect to see is a plot where we don't visualize any specific trend, indicating that there is  no bias. That is precisely what we observe in the following figure:</p> <p> Figure 5: Performance analysis of the BraTS2025_1 model as a function of tumor location for each subset.</p> <p>As we can see, the model has been trained with data that is robust enough in terms of tumor location not to have  problems with different locations.</p> <p>Note</p> <p>We recommend that users conduct their own analyses and draw their own conclusions, as certain correlations may not  be linear, and transformations such as logarithmic could reveal hidden trends.</p>"},{"location":"tutorials/BraTS_2024/#103-pair-wise-model-performance","title":"10.3 Pair-wise model performance","text":"<p>The BraTS competition awarded third place to two different models, here called BraTS25_3A and BraTS25_3B on the private  test set. We don't have access to it and are evaluating the models on the training set itself. However, this is  sufficient for our demonstrative purposes.</p>"},{"location":"tutorials/BraTS_2024/#1031-pair-wise-model-performance-aggregated-view","title":"10.3.1 Pair-wise model performance - Aggregated view","text":"<p>Let's make a comparison taking the BraTS25_3A model as the baseline model and BraTS25_3B as the benchmark. In our case, having the dataset divided by Site, we can analyze the differences individually. We will use the Dice metric as it is  the most widely reported in medical segmentation problems.</p> <p> Figure 6: Pairwise model performance comparison between BraTS25_3A and BraTS25_3B models for the Duke subset.</p> <p>On average, the BraTS 3B model improved the performance of the BraTS 3A model by up to 1.42% for the Dice metric. The  improvement came mainly from better segmentation of the RC region, and to a lesser extent from the NETC and ET regions.  However, the model worsened its performance for the SNFH region by around 1%.</p> <p>AUDIT also provides an estimation of whether the differences are statistically significant. For example, if we select  the Missouri subset, we can verify that the average differences between both models are 0.09%, apparently a minimal  difference. When performing the Wilcoxon signed-rank statistical test due to the violation of parametric assumptions,  it is verified that there are no significant differences to reject the null hypothesis. Therefore, on this specific  subset, the behavior of both models is similar.</p> <p> Figure 7: Statistical test results between models BraTS25_3A and BraTS25_3B to verify if the differences are statistically significant.</p>"},{"location":"tutorials/BraTS_2024/#1034-pair-wise-model-performance-disaggregated-view","title":"10.3.4 Pair-wise model performance - Disaggregated view","text":"<p>In general, artificial intelligence models in the field of medical imaging are analyzed at a high level, comparing  general metrics such as mean or median, and sometimes not even providing a measure of variance. This is why it's  essential to understand how the model behaves at the patient level, why the model fails in some specific cases,  and to ensure that the new version of the model we develop not only achieves better average results but also  generalizes well and doesn't make significant errors on certain patients that we were already able to segment correctly.</p> <p>With AUDIT we can analyze model performance at the patient level. We can obtain a Top-K patients to analyze either by  name, by baseline model performance, benchmark model performance, sort ascending or descending, etc. Let's compare the  performance of models BraTS_3A and BraTS_3B on the UCSF subset.</p> <p> Figure 8: Comparison results between models BraTS25_3A and BraTS25_3B at the subject level.</p> <p>As can be observed, BraTS25_3B, our benchmark, has clearly improved the results obtained by baseline BraTS25_3A in cases  UCSF-00005-100 and UCSF-00005-101, going from values around 0.70 to values of 0.94. This same exercise should be done  for those patients for which performance was poor, to understand what the weaknesses of our model are.</p>"},{"location":"tutorials/BraTS_2024/#11-conclusions","title":"11. Conclusions","text":"<p>Throughout this tutorial we have presented some of the features that AUDIT provides. It not only allows analyzing  differences in distributions, but its strength lies in model evaluation and low-level comparison\u2014analyses that often  go unnoticed when presenting results in the field of medical imaging.</p>"},{"location":"tutorials/BraTS_2025/","title":"BraTS 2025 Tutorial","text":"<p>This tutorial guides you step-by-step through preparing the BraTS 2025 dataset, organizing it, extracting features,  and launching the interactive dashboard to explore your data using AUDIT framework.</p> <p>Working with limited resources?</p> <p>If you have computational or storage constraints, you can skip the feature extraction entirely! Pre-extracted features  for both datasets are publicly available in the AUDIT GitHub repository. Simply  download the CSV files and jump directly to Section 6.</p> <p>References:</p> <ul> <li> <p>The 2024 Brain Tumor Segmentation (BraTS) Challenge: Glioma Segmentation on Post-treatment MRI</p> </li> <li> <p>The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification</p> </li> </ul> <p>Warning</p> <p>The annotation protocol followed in this tutorial was:  Label 0: Background, Label 1: Necrotic core, Label 2: Edema, Label 3: Enhancing tumor. However, we encourage users  to check the protocol used in their datasets.</p>"},{"location":"tutorials/BraTS_2025/#1-prerequisites-installation","title":"1. Prerequisites &amp; Installation","text":"<p>Before working with the dataset, we need to set up a clean environment. This ensures that all dependencies are properly managed and won't conflict with other Python packages you may have installed on your system.</p>"},{"location":"tutorials/BraTS_2025/#11-create-a-new-environment","title":"1.1 Create a new environment","text":"<p>We recommend creating a new conda environment to avoid conflicts with other packages. Conda provides excellent package  management and environment isolation:</p> <pre><code>conda create -n audit_env python=3.10\nconda activate audit_env\n</code></pre> <p>This creates an isolated Python 3.10 environment named audit_env. Python 3.10 is the recommended version for AUDIT, as  it provides the best compatibility with all dependencies.</p>"},{"location":"tutorials/BraTS_2025/#12-install-required-packages","title":"1.2 Install required packages","text":"<p>Install the core AUDIT package and Jupyter-related dependencies. The AUDIT package includes all necessary tools for  feature extraction, metric computation, and visualization:</p> <pre><code>pip install auditapp\npip install ipykernel jupyter\n</code></pre>"},{"location":"tutorials/BraTS_2025/#13-register-kernel-for-jupyter","title":"1.3 Register kernel for Jupyter","text":"<p>Register the environment so it appears as an option in Jupyter Notebook. This step is crucial for ensuring that Jupyter  can access the packages installed in your conda environment:</p> <pre><code>python -m ipykernel install --user --name=audit_env --display-name \"Python (audit_env)\"\n</code></pre> <p>This command registers your environment with Jupyter, making it available in the kernel selection menu. The  <code>--display-name</code> parameter determines what you'll see in the Jupyter interface.</p> <p>Now, launch Jupyter Notebook:</p> <pre><code>jupyter notebook\n</code></pre> <p>When opening a notebook, select the kernel \"Python (audit_env)\" from the Kernel menu  (Kernel \u2192 Change kernel \u2192 Python (audit_env)). This ensures that all code cells execute within your AUDIT environment  with access to all installed packages.</p> <p>Tip</p> <p>Check out our installation guide if you need more detailed instructions or troubleshooting tips.</p>"},{"location":"tutorials/BraTS_2025/#2-download-the-brats-2025-data","title":"2. Download the BraTS 2025 data","text":"<p>The BraTS 2025 dataset is one of the most comprehensive brain tumor segmentation datasets available, containing  multimodal MRI scans with expert annotations. Understanding how to properly access and organize this data is essential  for working with AUDIT.</p> <p>The BraTS 2025 dataset is hosted on Synapse and requires prior registration. Please follow the official  instructions from the challenge and to BraTS 2025 Data Access section to download the data. For this tutorial, we will take advantage of both the training and  validation datasets. </p> <p>After downloading, you'll receive compressed archives (typically .zip or .tar.gz files). For consistency with this  tutorial and to ensure that all subsequent commands work without modification, rename the extracted folders as follows:</p> <ul> <li>Training dataset \u2192 BraTS2025_train</li> <li>Validation dataset \u2192 BraTS2025_val</li> </ul>"},{"location":"tutorials/BraTS_2025/#3-project-structure","title":"3. Project structure","text":"<p>A well-organized project structure is fundamental to working efficiently with AUDIT. The framework expects specific  directories for datasets, configurations, outputs, and logs. Let's set this up properly.</p>"},{"location":"tutorials/BraTS_2025/#31-create-base-project-directory","title":"3.1 Create base project directory","text":"<p>Open your terminal and create a new folder for your project. This will serve as the root directory for all  AUDIT-related work:</p> <pre><code>mkdir brats2025_project\ncd brats2025_project\n</code></pre> <p>You can name this folder anything you prefer, but for consistency with this tutorial, we'll use brats2025_project.  All subsequent paths will be relative to this directory.</p>"},{"location":"tutorials/BraTS_2025/#32-initialize-audit-project-structure","title":"3.2 Initialize AUDIT project structure","text":"<p>AUDIT library requires a specific base structure for the project to function correctly. This structure separates  raw data, configurations, outputs, and logs into organized directories. To create this structure, let's start by  creating a new Jupyter notebook in your project directory and importing some necessary functions from  the file_manager module.</p> <p>Open Jupyter (if not already open), create a new notebook and import the required utilities:</p> <pre><code>from audit.utils.commons.file_manager import (\n    create_project_structure,\n    list_dirs,\n    rename_files,\n    rename_dirs\n)\n</code></pre> <p>Now, let's create the project folder hierarchy by running the following command. Define the base path where you want  to create the project structure (in this case, the current directory):</p> <pre><code>create_project_structure(base_path=\"./\")\n</code></pre> <p>After executing this command, you should see the following structure in your project directory:</p> <pre><code>brats2025_project/\n\u251c\u2500\u2500 datasets/\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u251c\u2500\u2500 logs/\n</code></pre>"},{"location":"tutorials/BraTS_2025/#33-organize-your-datasets","title":"3.3 Organize your datasets","text":"<p>Now that we have the proper directory structure, it's time to move the downloaded BraTS 2025 datasets into the project. Unzip the folders and move both datasets, training and validation, into your project\u2019s <code>./datasets/</code> folder.</p> <p>After this step, your project structure should look like:</p> <pre><code>brats2025_project/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 BraTS2025_train/\n\u2502   \u2502   \u251c\u2500\u2500 BraTS-GLI-00000-000/\n\u2502   \u2502   \u251c\u2500\u2500 BraTS-GLI-00002-000/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 BraTS2025_val/\n\u2502       \u251c\u2500\u2500 BraTS-GLI-00000-000/\n\u2502       \u251c\u2500\u2500 BraTS-GLI-00002-000/\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 outputs/\n\u2514\u2500\u2500 logs/\n</code></pre> <p>Each subject folder contains multiple NIfTI files (.nii.gz format) representing different MRI sequences  (T1, T1CE, T2, FLAIR) and the segmentation mask.</p> <p>In your Jupyter notebook, define Python variables pointing to your datasets. Using variables makes it easier to  reference these paths throughout your workflow:</p> <pre><code>train_data_path = \"./datasets/BraTS2025_train/\"\nval_data_path = \"./datasets/BraTS2025_val/\"\n</code></pre> <p>Verify that the datasets are accessible and contain the expected data:</p> <pre><code># Preview first subfolders\nprint(list_dirs(train_data_path)[:10])\n\n['BraTS-GLI-00000-000', 'BraTS-GLI-00002-000', ...]\n</code></pre>"},{"location":"tutorials/BraTS_2025/#4-standardize-folder-and-file-naming","title":"4. Standardize folder and file naming","text":"<p>To ensure compatibility with AUDIT and maintain consistency across your project, we need to standardize the naming  conventions for both folders and files. The BraTS dataset uses specific naming patterns that we'll modify to be more  explicit and aligned with AUDIT's expectations.</p>"},{"location":"tutorials/BraTS_2025/#41-rename-folders","title":"4.1 Rename folders","text":"<p>We will replace the generic prefix BraTS-GLI- with dataset-specific names (BraTS2025_train or BraTS2025_val). This  makes it immediately clear which dataset each subject belongs to, which is especially important when analyzing  results or comparing features across datasets.</p> <p>The renaming functions from AUDIT contain several parameters that provide fine-grained control over the renaming process:</p> <ul> <li>safe_mode: When set to True, performs a dry run that shows what would be renamed without actually changing anything. This is highly recommended for first-time users or when working with new datasets.</li> <li>verbose: When set to True, prints detailed information about each rename operation, allowing you to see exactly what's being changed.</li> <li>recursive: Controls whether to search subdirectories (useful for complex dataset structures).</li> </ul> <p>You can check out the complete documentation for these functions in the file manager module from  the API reference.</p> <p>Once you've verified that the preview looks correct and you're ready to proceed, run the actual renaming commands:</p> <pre><code>rename_dirs(\n    root_dir=train_data_path,\n    old_name=\"BraTS-GLI\",\n    new_name=\"BraTS2025_train\",\n    safe_mode=False\n)\n\nrename_dirs(\n    root_dir=val_data_path,\n    old_name=\"BraTS-GLI\",\n    new_name=\"BraTS2025_val\",\n    safe_mode=False\n)\n</code></pre>"},{"location":"tutorials/BraTS_2025/#42-rename-files","title":"4.2 Rename files","text":"<p>AUDIT also provides analogous functions for file manipulation. The file renaming process is similar to folder renaming,  but it operates on the individual files within each subject directory (the NIfTI image files and segmentation masks).</p> <p>Just as with folders, we'll replace the BraTS-GLI prefix with dataset-specific identifiers:</p> <pre><code>rename_files(\n    root_dir=train_data_path,\n    old_name=\"BraTS-GLI\",\n    new_name=\"BraTS2025_train\",\n    safe_mode=False\n)\n\nrename_files(\n    root_dir=val_data_path,\n    old_name=\"BraTS-GLI\",\n    new_name=\"BraTS2025_val\",\n    safe_mode=False\n)\n</code></pre> <p>This operation recursively processes all files within the dataset directories, ensuring consistent naming throughout  your project.</p>"},{"location":"tutorials/BraTS_2025/#43-standardize-sequences","title":"4.3 Standardize sequences","text":"<p>Beyond the dataset prefixes, we also need to standardize the MRI sequence naming conventions. The original BraTS data  uses abbreviations that may not be immediately clear or consistent with other datasets you might work with. Let's make  the sequence names clearer and more explicit.</p> <p>Original naming \u2192 Standardized naming:</p> <ul> <li>-seg.nii.gz \u2192 _seg.nii.gz (segmentation mask)</li> <li>-t1c.nii.gz \u2192 _t1ce.nii.gz (T1 contrast-enhanced)</li> <li>-t1n.nii.gz \u2192 _t1.nii.gz (T1 native/non-contrast)</li> <li>-t2f.nii.gz \u2192 _flair.nii.gz (FLAIR sequence)</li> <li>-t2w.nii.gz \u2192 _t2.nii.gz (T2-weighted)</li> </ul> <pre><code>old_names = ['-seg.nii.gz', '-t1c.nii.gz', '-t1n.nii.gz', '-t2f.nii.gz', '-t2w.nii.gz']\nnew_names = ['_seg.nii.gz', '_t1ce.nii.gz', '_t1.nii.gz', '_flair.nii.gz', '_t2.nii.gz']\n\nfor root_path in [train_data_path, val_data_path]:\n    for o, n in zip(old_names, new_names):\n        rename_files(\n            root_dir=root_path,\n            old_name=o,\n            new_name=n,\n            safe_mode=False\n        )\n</code></pre>"},{"location":"tutorials/BraTS_2025/#44-replace-dashes-with-underscores-optional","title":"4.4 Replace dashes with underscores (optional)","text":"<p>Finally, for consistency, we replace all <code>-</code> characters with <code>_</code>:</p> <pre><code>for root_path in [train_data_path, val_data_path]:\n    rename_files(root_dir=root_path, old_name=\"-\", new_name=\"_\", safe_mode=False)\n    rename_dirs(root_dir=root_path, old_name=\"-\", new_name=\"_\", safe_mode=False)\n</code></pre> <p>After this step, a typical subject folder will contain:</p> <pre><code>BraTS2025_train_00000_000/\n\u251c\u2500\u2500 BraTS2025_train_00000_000_t1.nii.gz       # T1-weighted\n\u251c\u2500\u2500 BraTS2025_train_00000_000_t1ce.nii.gz     # T1 contrast-enhanced\n\u251c\u2500\u2500 BraTS2025_train_00000_000_t2.nii.gz       # T2-weighted\n\u251c\u2500\u2500 BraTS2025_train_00000_000_flair.nii.gz    # FLAIR\n\u2514\u2500\u2500 BraTS2025_train_00000_000_seg.nii.gz      # Segmentation mask\n</code></pre> <p>If your output matches this format, congratulations! Your dataset is now properly standardized and ready for  feature extraction.</p>"},{"location":"tutorials/BraTS_2025/#5-run-feature-extraction","title":"5. Run feature extraction","text":"<p>Now that the datasets are properly organized and standardized, we can proceed to the feature extraction phase. This is where AUDIT demonstrates its power by automatically computing a comprehensive set of radiomic and morphological  features from your medical images.</p>"},{"location":"tutorials/BraTS_2025/#51-understanding-audit-configuration-files","title":"5.1 Understanding AUDIT configuration files","text":"<p>AUDIT takes advantage of YAML configuration files for all major operations, including feature extraction, metric  computation, and running the web application. </p> <p>When you created the project structure in Section 3, AUDIT automatically generated template configuration files in the  <code>config/</code> directory. These templates contain sensible defaults, but we need to customize them for the BraTS 2025 dataset. The configuration file we need to edit for feature extraction is named <code>feature_extraction.yaml.</code></p> <p>Open <code>config/feature_extraction.yaml</code> in your preferred text editor. Here's the complete configuration needed for this  project, with detailed explanations for each section:</p> <pre><code># Paths to all the datasets\n# IMPORTANT: These MUST be absolute paths, not relative paths\ndata_paths:\n  BraTS2025_train: '/home/usr/brats2025_project/datasets/BraTS2025_train/'\n  BraTS2025_val: '/home/usr/brats2025_project/datasets/BraTS2025_val/'\n\n# Sequences available in your dataset\n# These identifiers must match the suffixes in your standardized filenames\nsequences:\n  - '_t1'\n  - '_t2'\n  - '_t1ce'\n  - '_flair'\n\n# Mapping of labels to their numeric values in the segmentation masks\n# These values come from the BraTS challenge specification\nlabels:\n  BKG: 0\n  EDE: 2\n  ENH: 3\n  NEC: 1\n\n# List of features to extract\n# Set to 'true' to enable each feature category\nfeatures:\n  statistical: true\n  texture: true\n  spatial: true\n  tumor: true\n\n# Output paths for extracted features and logs\n# Again, these should be absolute paths\noutput_path: '/home/usr/brats2025_project/outputs/features'\nlogs_path: '/home/usr/brats2025_project/logs/features'\n\n# Resource allocation\n# Adjust cpu_cores based on your system (don't exceed your CPU count)\ncpu_cores: 8\n</code></pre> <p>Once you've configured the YAML file with your absolute paths, you're ready to run the extraction. Open a terminal  (not your Jupyter notebook) and execute:</p> <pre><code>auditapp feature-extraction --config /home/usr/brats2025_project/configs/feature_extraction.yaml\n</code></pre> <p>After execution, <code>/home/usr/brats2025_project/outputs/features</code> will contain the extracted features for both training and  validation datasets. The resulting CSV file will contain: Subject identifiers, features from each MRI sequence  (T1, T2, T1CE, FLAIR), features for each label/region (background, edema, enhancing tumor, necrotic core), etc.</p> <p>Important</p> <p>All paths must be absolute. Otherwise, AUDIT may fall back to its internal default config files.</p>"},{"location":"tutorials/BraTS_2025/#6-launch-the-dashboard","title":"6. Launch the dashboard","text":"<p>Now comes the exciting part, exploring your data! Once features are extracted, AUDIT's interactive dashboard allows you  to visualize feature distributions, identify outliers, compare datasets, and gain insights into your data quality and  characteristics.</p> <p>Just like feature extraction, the dashboard is configured using a YAML file. We need to edit <code>config/app.yaml</code> to tell  AUDIT where to find your datasets, extracted features, and any metrics or predictions you might have.</p> <p>Open <code>config/app.yaml</code> in your text editor and configure it as follows:</p> <pre><code># Sequences available in your dataset\n# Must match the sequences defined in feature_extraction.yaml\nsequences:\n  - '_t1'\n  - '_t2'\n  - '_t1ce'\n  - '_flair'\n\n# Mapping of labels to their numeric values\n# Must match the labels defined in feature_extraction.yaml\nlabels:\n  BKG: 0\n  EDE: 2\n  ENH: 3\n  NEC: 1\n\n# Root paths\ndatasets_path: '/home/usr/brats2025_project/datasets'\nfeatures_path: '/home/usr/brats2025_project/outputs/features'\nmetrics_path: '/home/usr/brats2025_project/outputs/metrics'\n\n# Raw datasets - Point to the directories containing your NIfTI files\n# The dashboard uses these to load and display the actual MRI images\nraw_datasets:\n  BraTS2025_train: \"${datasets_path}/BraTS2025_train\"\n  BraTS2025_val: \"${datasets_path}/BraTS2025_val\"\n\n# Feature extraction CSVs - These contain all the extracted radiomic features for each subject\nfeatures:\n  BraTS2025_train: \"${features_path}/extracted_information_BraTS2025_train.csv\"\n  BraTS2025_val: \"${features_path}/extracted_information_BraTS2025_val.csv\"\n\n# Metric extraction CSVs (optional)\nmetrics:\n\n# Model predictions (optional)\npredictions:\n</code></pre> <p>With your configuration file properly set up, you're ready to launch the dashboard. Open a terminal and execute:</p> <pre><code>auditapp run-app --config /home/usr/brats2025_project/configs/app.yaml\n</code></pre> <p>Now it\u2019s time to explore the datasets!</p> <p>Keep in mind that some feature types may influence generalization across cohorts more than others. For example, while  the statistical features appear quite similar between the datasets, we encourage you to dive deeper into the data,  experiment with different feature sets, and challenge your models to reach the next level of performance.</p> <p> Figure 1: Univariate feature analysis mode. Median pixel intensity distribution for FLAIR sequence.</p>"},{"location":"tutorials/Lumiere_postprocessing/","title":"Post-processing model predictions with AUDIT","text":"<p>In the `LUMIERE preprocessing tutorial, we prepared and adapted the dataset LUMIERE to the file structure required to  work with AUDIT. In this tutorial, we will see how, in addition to standardizing the organization of your project's  files, it will be beneficial for your datasets to be standardized and follow the same naming conventions. This way,  it'll be much easier to compare them with each other, evaluate models, and more.</p> <p>Let's assume we've selected a pre-trained model from an open-source repository to generate predictions on your  dataset. It is likely that the model wasn\u2019t trained to predict the same labels that our segmentations use. In such  cases, we\u2019ll need to apply some post-processing to properly evaluate these predictions using AUDIT.</p> <p>Fortunately, AUDIT provides users with tools to perform this post-processing and adapt the predictions as needed. </p> <p>By the end of this tutorial, you will: - Organize predictions into a structured directory. - Rename files and labels to align with AUDIT's requirements. - Standardize labels for both ground truth and predictions, ensuring compatibility.</p> <p>Let\u2019s get started!</p>"},{"location":"tutorials/Lumiere_postprocessing/#1-prerequisites","title":"1. Prerequisites","text":"<p>This tutorial uses some utility functions to manipulate files and sequences.</p> <pre><code>from audit.utils.sequences.sequences import (\n    load_nii_by_subject_id,\n    iterative_labels_replacement,\n    count_labels\n)\n\nfrom audit.utils.commons.file_manager import (\n    list_dirs,\n    list_files,\n    organize_files_into_dirs,\n    add_suffix_to_files\n)\n</code></pre>"},{"location":"tutorials/Lumiere_postprocessing/#2-data-understanding","title":"2. Data understanding","text":"<p>Now, let's suppose we have a model that was pretrained on a brain MRI dataset. We will use this model to generate a  series of predictions on our own dataset. In our case, we want to run inference on LUMIERE.</p> <p>After running the inference, we store the predictions in the following directory:</p> <pre><code>root_path_sequences = \"./datasets/LUMIERE/LUMIERE_images/\"\nroot_path_predictions = \"./datasets/LUMIERE/LUMIERE_seg/nnUnet\"\n</code></pre> <p>Let\u2019s check the contents of this directory:</p> <pre><code>print(list_dirs(root_path_predictions))\nprint(list_files(root_path_predictions)[:6])\n</code></pre> <pre><code>[]\n['Patient-001-week-044.nii.gz', 'Patient-001-week-056.nii.gz', 'Patient-002-week-000.nii.gz', 'Patient-002-week-003.nii.gz', 'Patient-002-week-021.nii.gz', 'Patient-002-week-037.nii.gz']\n</code></pre>"},{"location":"tutorials/Lumiere_postprocessing/#3-organize-folder","title":"3. Organize folder","text":"<p>AUDIT is designed to work with multiple models and datasets, but it requires a specific organization of the data for  processing. Currently, we have a single directory \".datasets/LUMIERE/LUMIERE_seg/\". Have a look at the  documentation to learn more about AUDIT project structure.</p> <p>This directory contains the predictions generated by our model for the LUMIERE dataset. However, AUDIT requires that  each segmentation be contained within a folder named after the subject ID. Since users may not structure their code to  produce output in this specific format\u2014like the pre-trained model we are using\u2014AUDIT provides functions to facilitate  the organization of files.</p> <p>In this case, we can use the organize_files_into_folders function to create the necessary data structure without  needing to perform complex manipulations.</p> <pre><code>organize_files_into_dirs(\n    root_dir=root_path_predictions,\n    extension='.nii.gz',\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would move: ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-058-week-002.nii.gz -&gt; ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-058-week-002/Patient-058-week-002.nii.gz\n[SAFE MODE] Would move: ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-018-week-063.nii.gz -&gt; ./datasets/LUMIERE/LUMIERE_seg/nnUnet/Patient-018-week-063/Patient-018-week-063.nii.gz\n...\n</code></pre> <p>Let's turn safe_mode into False to apply the changes.</p> <pre><code>organize_files_into_dirs(\n    root_dir=root_path_predictions,\n    extension='.nii.gz',\n    safe_mode=False\n)\n</code></pre> <p>The structure of the directory containing the predictions has changed. Instead of having all the files  scattered within the directory, a separate folder has been created for each subject, containing their respective  predictions. </p> <p>This organized structure makes it much easier to manage and process the predictions for each subject in the dataset.  Let\u2019s take a moment to verify this new organization:</p> <pre><code>|--Patient-000-week-000\n|---- Patient-000-week-000.nii.gz\n|--Patient-000-week-001\n|---- Patient-000-week-001.nii.gz\n...\n</code></pre>"},{"location":"tutorials/Lumiere_postprocessing/#4-rename-files","title":"4. Rename files","text":"<p>Another important aspect of AUDIT is that it uses file extensions to distinguish between sequences (e.g., _t1, _t2,  _t1ce, _flair), segmentations (_seg), and predictions (_pred). However, if we check the names of the files  generated after inference, they did not contain any specific nomenclature.</p> <p>To simplify the task for users and eliminate the need to modify their pipelines to ensure compatibility with AUDIT,  the library provides methods to quickly and easily adapt the file names. </p> <p>Let\u2019s explore how to rename the files so they align with AUDIT\u2019s requirements. Taking advantage of the function  add_suffix_to_files, we can add the extension required by AUDIT.</p> <pre><code>add_suffix_to_files(\n    root_dir=root_path_predictions,\n    suffix='_pred', \n    ext='.nii.gz', \n    safe_mode=False\n)\n</code></pre> <p>Now, all the files within the root_path contain the _pred extension.</p> <pre><code>list_files(os.path.join(root_path_preditions, 'Patient-012-week-016'))\n</code></pre> <pre><code>[Patient-012-week-016_pred.nii.gz]\n</code></pre>"},{"location":"tutorials/Lumiere_postprocessing/#5-label-replacement-ground-truth","title":"5. Label replacement (Ground Truth)","text":"<p>We need to verify which labels were generated after the inference. The model used in this tutorial was pre-trained on the BraTS2020 dataset to predict the labels ENH, NEC, and EDE. However, these may not align with the  labels used in the LUMIERE dataset. In fact, they are different and will need to be adjusted accordingly.</p> <pre><code>subject = \"Patient-006-week-000\"\n\nseg = load_nii_by_subject_id(\n    root_dir=root_path_sequences,\n    subject_id=subject,\n    seq=\"_seg\",\n    as_array=True\n)\n\ncount_labels(seg)\n</code></pre> <pre><code>{0: 6968420, 1: 31427, 2: 44342, 3: 176843}\n</code></pre> <p>The annotation protocol followed for BraTS 2020 dataset was the following:</p> <ul> <li>BKG: 0</li> <li>EDE: 3</li> <li>ENH: 1</li> <li>NEC: 2</li> </ul> <p>In contrast, LUMIERE uses:</p> <ul> <li>BKG: 0</li> <li>EDE: 3</li> <li>ENH: 2</li> <li>NEC: 1</li> </ul> <p>To resolve this mismatch, we will use the iterative_labels_replacement function. This function takes the old mapping  and the new mapping as parameters and replaces the labels accordingly.</p> <pre><code>original_labels = [0, 1, 2, 3]  # current mapping used by LUMIERE (BKG: 0 NEC: 1 ENH: 2 EDE: 3)\nnew_labels = [0, 2, 1, 3]  # new mapping we want LUMIERE to use (BKG: 0 ENH: 1 NEC: 2 EDE: 3)\n\niterative_labels_replacement(\n    root_dir=root_path_sequences,\n    original_labels=original_labels,\n    new_labels=new_labels,\n    ext=\"_seg\"  # only files whose extension is '_seg' will be relabeled\n)\n</code></pre> <p>Once we apply this function, we can confirm that the labels are now correct.</p> <pre><code>2025-01-20 13:52:47.302 | INFO     | src.audit.utils.sequences.sequences:iterative_labels_replacement:216 - Iterative label replacement completed: 513 files processed, 2052 files skipped.\n</code></pre>"},{"location":"tutorials/Lumiere_postprocessing/#6-label-replacement-predictions","title":"6. Label replacement (Predictions)","text":"<p>The model we used to generate the predictions was trained with the intention that tumor regions be labeled as follows:</p> <ul> <li>BKG: 0</li> <li>EDE: 2</li> <li>ENH: 4</li> <li>NEC: 1</li> </ul> <p>As can be observed in the prediction generated for the same subject we have been working with.</p> <pre><code>pred = load_nii_by_subject_id(\n    root_dir=root_path_predictions,\n    subject_id=subject,\n    seq=\"_pred\",\n    as_array=True\n)\n\ncount_labels(pred)\n</code></pre> <pre><code>{0.0: 6959023, 1.0: 33811, 2.0: 192371, 4.0: 35827}\n</code></pre> <p>Therefore, we will once again need to rename the labels to match the ground truth.</p> <pre><code>original_labels = [0, 1, 2, 4]  # current mapping used in our predictions  (BKG: 0 NEC: 1 EDE: 2 ENH: 4)\nnew_labels = [0, 2, 3, 1]  # new mapping we want LUMIERE to use (BKG: 0 ENH: 1 NEC: 2 EDE: 3)\n\n\niterative_labels_replacement(\n    root_dir=root_path_predictions,\n    original_labels=original_labels,\n    new_labels=new_labels,\n    ext=\"_pred\"  # only files whose extension is '_seg' will be relabeled\n)\n</code></pre> <pre><code>2025-01-20 14:34:52.473 | INFO     | src.audit.utils.sequences.sequences:iterative_labels_replacement:216 - Iterative label replacement completed: 507 files processed, 0 files skipped.\n</code></pre> <p>Finally, we have prepared the dataset to run the metric extraction module and start using AUDIT effectively.</p> <pre><code>seg = load_nii_by_subject_id(\n    root_dir=root_path_sequences,\n    subject_id=subject,\n    seq=\"_seg\",\n    as_array=True\n)\npred = load_nii_by_subject_id(\n    root_dir=root_path_predictions,\n    subject_id=subject,\n    seq=\"_pred\",\n    as_array=True\n)\n\n# now they use the same labeling system\nprint(count_labels(seg))\nprint(count_labels(pred))\n</code></pre> <pre><code>{0: 6968420, 1: 31427, 2: 44342, 3: 176843}\n{0.0: 6959023, 1.0: 35827, 2.0: 33811, 3.0: 192371}\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/","title":"LUMIERE dataset preprocessing","text":"<p>In this tutorial, we will walk you through the preprocessing steps of the LUMIERE dataset using the AUDIT library. Our goal is to demonstrate how to clean, organize, and standardize the dataset to prepare it for further  analysis. We'll cover key operations such as file and folder cleaning, reorganization, and renaming, all while  explaining each step in detail.</p> <p>References:</p> <ul> <li>The LUMIERE dataset: Longitudinal Glioblastoma MRI with expert RANO evaluation</li> </ul> <p>Let's get started!</p>"},{"location":"tutorials/Lumiere_preprocessing/#1-prerequisites","title":"1. Prerequisites","text":"<p>Before we dive into the dataset preprocessing, we first need to import the necessary functions from the AUDIT library. These utility functions will help us clean and reorganize the dataset. Importing these functions at the  start will give us all the tools we need for the upcoming steps.</p> <p>If you have already installed AUDIT a library, then you'll be able to import the main functions.</p> <pre><code>import os\nfrom audit.utils.commons.file_manager import (\n    list_dirs,\n    list_files,\n    delete_files_by_extension,\n    delete_dirs_by_pattern,\n    move_files_to_parent,\n    organize_subdirs_into_named_dirs,\n    rename_files,\n    add_string_files,\n    rename_dirs\n)\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#2-data-understanding","title":"2. Data understanding","text":"<p>Before diving into the preprocessing tasks, it is essential to gain an understanding of the dataset's structure. This  helps us identify the key elements we will be working with, such as the available sequences and segmentation data.  LUMIERE dataset contains images captured over multiple time points, so we'll need to identify and remove the unwanted data, keeping only what\u2019s relevant for analysis.</p> <p>In this section, we will explore the overall structure of the dataset, focusing on the folders and files that need our attention.</p> <p>To follow along, download the LUMIERE dataset beforehand.</p> <pre><code>root_data_path = \"./datasets/LUMIERE/\"\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#exploring-the-dataset","title":"Exploring the Dataset","text":"<p>We begin by checking the main directory of the dataset to understand its organization. We observe that there are 91  directories, each corresponding to a patient.</p> <pre><code>print(list_dirs(root_data_path)[:5])\nprint(list_dirs(root_data_path)[-5:])\n</code></pre> <pre><code>['Patient-001', 'Patient-002', 'Patient-003', 'Patient-004', 'Patient-005']\n['Patient-087', 'Patient-088', 'Patient-089', 'Patient-090', 'Patient-091']\n</code></pre> <p>Each patient folder contains subdirectories for different timepoints, typically named in the format \"week-XXX\", where  XXX corresponds to the week the image was taken. We may also encounter directories with additional suffixes like \"-N\"  for specific timepoints.</p> <p>To streamline our tutorial, we will focus only on the core timepoints and exclude those with the \"-N\" suffix.</p> <pre><code>print(list_dirs(f\"{root_data_path}Patient-001\"))\nprint(list_dirs(f\"{root_data_path}Patient-091\"))\n</code></pre> <pre><code>['week-000-1', 'week-000-2', 'week-044', 'week-056']\n['week-000', 'week-001', 'week-014', 'week-026', 'week-036', 'week-043']\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#reviewing-the-data-at-each-timepoint","title":"Reviewing the data at each timepoint","text":"<p>Each timepoint contains a variety of sequences along with segmentation data. For example, in the case of patient 091, we examine the week-0000 folder to find different sequence data along with segmentation predictions generated by two  models: \"DeepBraTumIA-segmentation\" and \"HD-GLIO-AUTO-segmentation\".</p> <p>For this tutorial, we will consider the segmentation provided by \"DeepBraTumIA-segmentation\" as the ground truth.</p> <pre><code>print(list_dirs(f\"{root_data_path}Patient-091/week-000\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000\"))\n</code></pre> <pre><code>['DeepBraTumIA-segmentation', 'HD-GLIO-AUTO-segmentation']\n['CT1.nii.gz', 'FLAIR.nii.gz', 'T1.nii.gz', 'T2.nii.gz']\n</code></pre> <p>The sequences that are of primary interest to us are stored in the \"atlas/skull_strip\" directory, and the corresponding segmentation is found in the \"atlas/segmentation\" directory.</p> <pre><code>print(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/skull_strip\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/segmentation\"))\n</code></pre> <pre><code>['brain_mask.nii.gz', 'ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n['measured_volumes_in_mm3.json', 'seg_mask.nii.gz']\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#3-files-cleaning","title":"3. Files cleaning","text":"<p>Now that we understand the structure, the next step is to clean the dataset by removing unnecessary files. This  involves eliminating sequences and files that we won\u2019t use in our analysis. The files we want to remove are mainly  the raw image sequences ('CT1.nii.gz', 'FLAIR.nii.gz', 'T1.nii.gz', 'T2.nii.gz'), as well as other irrelevant files  like 'brain_mask.nii.gz' and 'measured_volumes_in_mm3.json'.</p> <p>In this section, we will walk you through the process of cleaning these files using AUDIT methods.</p> <pre><code>delete_files_by_extension(\n    root_dir=root_data_path,\n    ext='CT1.nii.gz',\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-001/week-000-1/CT1.nii.gz\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-001/week-000-2/CT1.nii.gz\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-001/week-044/CT1.nii.gz\n....\n....\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-091/week-036/CT1.nii.gz\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-091/week-043/CT1.nii.gz\n</code></pre> <p>First, we use the <code>delete_files_by_extension</code> function in safe mode to preview the files that will be deleted. After  confirming that these are indeed the unnecessary files, we proceed to remove them.</p> <pre><code>sequences_to_delete = ['CT1.nii.gz', 'FLAIR.nii.gz', 'T1.nii.gz', 'T2.nii.gz']\nfor seq in sequences_to_delete:\n    delete_files_by_extension(\n        root_dir=root_data_path,\n        ext=seq,\n        safe_mode=False\n    )\n</code></pre> <p>After deletion, we can verify that the unnecessary files have been removed from the timepoint folders.</p> <pre><code>print(list_dirs(f\"{root_data_path}Patient-091/week-000\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000\"))\n</code></pre> <p>Additionally, we clean up other unnecessary files like 'brain_mask.nii.gz' and 'measured_volumes_in_mm3.json'.</p> <pre><code>files_to_delete = ['.json', 'brain_mask.nii.gz']\nfor file in files_to_delete:\n    delete_files_by_extension(\n        root_dir=root_data_path,\n        ext=file,\n        safe_mode=False\n    )\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#4-folders-cleaning","title":"4. Folders cleaning","text":"<p>In this step, we remove unnecessary folders that contain irrelevant data, such as \"HD-GLIO-AUTO-segmentation\" and  \"DeepBraTumIA-segmentation/atlas/\". This helps us further simplify the structure, keeping only the essential files  and folders for our analysis.</p> <pre><code>delete_dirs_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"HD-GLIO\",\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-088/week-000-2/HD-GLIO-AUTO-segmentation\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-088/week-000-1/HD-GLIO-AUTO-segmentation\n....\n[SAFE MODE] Would delete: ./datasets/LUMIERE/Patient-078/week-119/HD-GLIO-AUTO-segmentation\n</code></pre> <pre><code>delete_dirs_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"HD-GLIO\",\n    safe_mode=False\n)\n\ndelete_dirs_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"native\",\n    safe_mode=False\n)\n</code></pre> <p>After cleaning the folders, we verify that the unnecessary directories have been removed.</p> <pre><code># No unnecessary files in the folders.\nprint(list_files(f\"{root_data_path}Patient-091/week-000/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas\"))\n\n# No unnecessary directories in the folders.\nprint(list_dirs(f\"{root_data_path}Patient-091/week-000/\"))\nprint(list_dirs(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/\"))\nprint(list_dirs(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas\"))\n</code></pre> <pre><code>[]\n[]\n[]\n['DeepBraTumIA-segmentation']\n['atlas']\n['segmentation', 'skull_strip']\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#5-files-organization","title":"5. Files organization","text":"<p>Now that we have cleaned up the unnecessary files and folders, it\u2019s time to organize the remaining files. Currently,  they are nested in deep directory structures, and we need to move them to the parent folders for easier access.</p> <p>In this section, we will use the move_files_to_parent function to simplify the file structure by moving the  necessary files to their corresponding parent directories.</p> <pre><code>move_files_to_parent(\n    root_dir=root_data_path,\n    levels_up=3,\n    ext=None,\n    safe_mode=False\n)\n</code></pre> <p>This will move the files up from their current deep folder structure to the appropriate parent folders. Let's verify  that everything has been moved correctly.</p> <pre><code>print(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/skull_strip\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/segmentation\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/atlas/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/DeepBraTumIA-segmentation/\"))\nprint(list_files(f\"{root_data_path}Patient-091/week-000/\"))\nprint(list_files(f\"{root_data_path}Patient-002/week-047/\"))\n</code></pre> <pre><code>[]\n[]\n[]\n[]\n['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n</code></pre> <p>We also remove any remaining directories we no longer need.</p> <pre><code>delete_dirs_by_pattern(\n    root_dir=root_data_path,\n    pattern=\"DeepBraTumIA-segmentation\",\n    safe_mode=False\n)\n</code></pre> <pre><code>[]\n['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#6-folders-organization","title":"6. Folders organization","text":"<p>At this point, we need to organize the folders further. To align with the expected structure for AUDIT, we will  move the timepoint folders to the root level. This way, each subject and their respective timepoints will be placed  in a well-organized directory.</p> <p>The function organize_subdirs_into_named_dirs will help us organize subdirectories by joining their parent and child folder names. The \"join\" argument defines the string used to concatenate the parent folder and the child folder.  Check the documentation for more detailed information.</p> <pre><code>organize_subdirs_into_named_dirs(\n    root_dir=root_data_path,\n    join_char=\"-\",\n    safe_mode=True\n)\n</code></pre> <pre><code>[SAFE MODE] Would move: ./datasets/LUMIERE/Patient-001/week-000-2/t2_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-001-week-000-2/t2_skull_strip.nii.gz\n[SAFE MODE] Would move: ./datasets/LUMIERE/Patient-001/week-000-2/ct1_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-001-week-000-2/ct1_skull_strip.nii.gz\n....\n[SAFE MODE] Would move: ./datasets/LUMIERE/Patient-091/week-014/flair_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-091-week-014/flair_skull_strip.nii.gz\n</code></pre> <p>Now, we will move the subfolders into the root directory and rename them as necessary.</p> <pre><code>organize_subdirs_into_named_dirs(\n    root_dir=root_data_path,\n    join_char=\"-\",\n    safe_mode=False\n)\n</code></pre> <pre><code>print(list_dirs(root_data_path)[:6])\n</code></pre> <pre><code>['Patient-001-week-000-1', 'Patient-001-week-000-2', 'Patient-001-week-044', 'Patient-001-week-056', 'Patient-002-week-000', 'Patient-002-week-003']\n</code></pre> <p>As mentioned earlier, for simplicity, we will only keep the timepoints that do not have a \"-N\" suffix at the end of  the corresponding week.</p> <p><pre><code>pattern_to_delete = r\"^Patient-\\d{3}-week-\\d{3}-\\d\"\n\ndelete_dirs_by_pattern(\n    root_dir=root_data_path,\n    pattern=pattern_to_delete,\n    safe_mode=False\n)\n\nprint(list_dirs(root_data_path)[:6])\n</code></pre> <pre><code>['Patient-001-week-044', 'Patient-001-week-056', 'Patient-002-week-000', 'Patient-002-week-003', 'Patient-002-week-021', 'Patient-002-week-037']\n</code></pre></p>"},{"location":"tutorials/Lumiere_preprocessing/#7-sequences-name-standardization","title":"7. Sequences name standardization","text":"<p>Finally, to follow a more standardized naming convention, such as the one used in the BraTS dataset, we will rename  the sequences and the segmentation to follow a similar pattern. Typically, MRI sequences are named t1, t2, t1ce,  and flair, and the segmentation is named seg. However, the names we currently have do not follow this convention.  Let's use rename_files to modify them.</p> <pre><code>print(list_files(f\"{root_data_path}Patient-001-week-044\"))\n</code></pre> <pre><code>['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\n</code></pre> <pre><code>old_names = ['ct1_skull_strip.nii.gz', 'flair_skull_strip.nii.gz', 'seg_mask.nii.gz', 't1_skull_strip.nii.gz', 't2_skull_strip.nii.gz']\nnew_names = ['t1ce.nii.gz', 'flair.nii.gz', 'seg.nii.gz', 't1.nii.gz', 't2.nii.gz']\n\nfor o, n in zip(old_names, new_names):\n    rename_files(\n        root_dir=root_data_path,\n        old_name=o,\n        new_name=n,\n        safe_mode=True\n    )\n</code></pre> <pre><code>[SAFE MODE] Would rename: ./datasets/LUMIERE/Patient-012-week-016/ct1_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-012-week-016/t1ce.nii.gz\n[SAFE MODE] Would rename: ./datasets/LUMIERE/Patient-023-week-001/ct1_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-023-week-001/t1ce.nii.gz\n....\n[SAFE MODE] Would rename: ./datasets/LUMIERE/Patient-077-week-083/t2_skull_strip.nii.gz -&gt; ./datasets/LUMIERE/Patient-077-week-083/t2.nii.gz\n</code></pre> <pre><code>for o, n in zip(old_names, new_names):\n    rename_files(\n        root_dir=root_data_path,\n        old_name=o,\n        new_name=n,\n        safe_mode=False\n    )\n</code></pre> <p>Additionally, to allow AUDIT to locate each image simply by the subject ID, we will name each image with the  corresponding subject identifier along with the sequence name. To do this, we will use the add_string_files  function, which allows us to add both suffixes and prefixes to specific files.</p> <pre><code>for subject in list_dirs(root_data_path):\n    add_string_files(\n        root_dir=os.path.join(root_data_path, subject),\n        prefix=f\"{subject}_\",\n        ext=None,\n        safe_mode=False\n    )\n</code></pre> <p>With this, we would have organized the project as required to work with AUDIT. Additionally, we recommend that the  images (sequences and segmentations provided by the medical experts) be placed in a directory called DATASET_images,  so that the segmentations from each model are contained in the DATASET_seg directory. Therefore, to conclude, we'll  rename the LUMIERE directory to LUMIERE_images.</p> <pre><code>rename_dirs(\n    root_dir=\"./datasets/\",\n    old_name=\"LUMIERE\",\n    new_name=\"LUMIERE_images\",\n    safe_mode=False\n)\n</code></pre>"},{"location":"tutorials/Lumiere_preprocessing/#conclusion","title":"Conclusion","text":"<p>By following these steps, we have successfully cleaned, organized, and standardized the LUMIERE dataset, making it  ready for further analysis. The AUDIT library has provided a powerful toolkit for efficiently preprocessing and  structuring the data.</p>"}]}